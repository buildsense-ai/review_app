#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ®æ”¯æŒåº¦è¯„ä¼°æœåŠ¡è·¯ç”±å™¨
åŸºäºweb_agent_appçš„FastAPIè·¯ç”±å™¨å®ç°
"""

import os
import sys
import json
import time
import tempfile
import shutil
import logging
import traceback
from typing import Optional, Dict, Any, List
from datetime import datetime
from pathlib import Path

from fastapi import APIRouter, File, UploadFile, HTTPException, BackgroundTasks, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, Field

# æ·»åŠ web_agent_appåˆ°Pythonè·¯å¾„
web_agent_path = Path(__file__).parent.parent.parent / "web_agent_app"
sys.path.insert(0, str(web_agent_path))

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å…¼å®¹web_agent_appçš„é…ç½®
import os
if not os.getenv('LOG_LEVEL'):
    os.environ['LOG_LEVEL'] = 'INFO'
if not os.getenv('MAX_WORKERS'):
    os.environ['MAX_WORKERS'] = '5'
if not os.getenv('ENABLE_PARALLEL_SEARCH'):
    os.environ['ENABLE_PARALLEL_SEARCH'] = 'true'
if not os.getenv('ENABLE_PARALLEL_ENHANCEMENT'):
    os.environ['ENABLE_PARALLEL_ENHANCEMENT'] = 'true'

try:
    from whole_document_pipeline import WholeDocumentPipeline
    from evidence_detector import UnsupportedClaim, EvidenceResult
except ImportError as e:
    logging.error(f"æ— æ³•å¯¼å…¥web_agent_appæ¨¡å—: {e}")
    # è®¾ç½®ä¸ºNoneï¼Œè¡¨ç¤ºå¯¼å…¥å¤±è´¥
    WholeDocumentPipeline = None
    
    # å®šä¹‰åŸºç¡€æ¨¡å‹ä½œä¸ºåå¤‡
    class UnsupportedClaim:
        def __init__(self, claim_id="", claim_text="", section_title="", claim_type="factual", 
                     confidence_level=0.8, context="", search_keywords=None, original_position=1):
            self.claim_id = claim_id
            self.claim_text = claim_text
            self.section_title = section_title
            self.claim_type = claim_type
            self.confidence_level = confidence_level
            self.context = context
            self.search_keywords = search_keywords or []
            self.original_position = original_position
    
    class EvidenceResult:
        def __init__(self, claim_id="", claim_text="", section_title="", search_query="", 
                     evidence_sources=None, enhanced_text="", confidence_score=0.0, processing_status="success"):
            self.claim_id = claim_id
            self.claim_text = claim_text
            self.section_title = section_title
            self.search_query = search_query
            self.evidence_sources = evidence_sources or []
            self.enhanced_text = enhanced_text
            self.confidence_score = confidence_score
            self.processing_status = processing_status
    
    WholeDocumentPipeline = None

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="", tags=["è®ºæ®æ”¯æŒåº¦è¯„ä¼°"])

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
pipeline = None
processing_tasks = {}

# ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å†…å­˜å­˜å‚¨æ¥è·Ÿè¸ªä»»åŠ¡çŠ¶æ€
_task_storage = {}

# ä»»åŠ¡çŠ¶æ€ç®¡ç†å‡½æ•°
def update_task_status(task_id: str, status: str, progress: float, message: str, result: Any = None, error: str = None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    _task_storage[task_id] = {
        "task_id": task_id,
        "status": status,
        "progress": progress,
        "message": message,
        "result": result,
        "error": error,
        "updated_at": datetime.now().isoformat()
    }

def extract_document_sections(document_content: str) -> Dict[str, Dict[str, str]]:
    """æå–æ–‡æ¡£ä¸­çš„ç« èŠ‚å†…å®¹ï¼ŒæŒ‰ä¸€çº§æ ‡é¢˜å’ŒäºŒçº§æ ‡é¢˜åµŒå¥—ç»„ç»‡"""
    sections = {}
    lines = document_content.split('\n')
    current_h1 = None
    current_h2 = None
    current_content = []
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸€çº§æ ‡é¢˜
        if line.strip().startswith('# ') and not line.strip().startswith('## '):
            # ä¿å­˜å‰ä¸€ä¸ªäºŒçº§ç« èŠ‚
            if current_h1 and current_h2 and current_content:
                if current_h1 not in sections:
                    sections[current_h1] = {}
                sections[current_h1][current_h2] = '\n'.join(current_content).strip()
            
            # å¼€å§‹æ–°çš„ä¸€çº§æ ‡é¢˜
            current_h1 = line.strip().replace('# ', '').strip()
            current_h2 = None
            current_content = []
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯äºŒçº§æ ‡é¢˜
        elif line.strip().startswith('## '):
            # ä¿å­˜å‰ä¸€ä¸ªäºŒçº§ç« èŠ‚
            if current_h1 and current_h2 and current_content:
                if current_h1 not in sections:
                    sections[current_h1] = {}
                sections[current_h1][current_h2] = '\n'.join(current_content).strip()
            
            # å¼€å§‹æ–°çš„äºŒçº§æ ‡é¢˜
            current_h2 = line.strip().replace('## ', '').strip()
            current_content = []
            
        elif current_h2:
            # æ·»åŠ åˆ°å½“å‰äºŒçº§ç« èŠ‚å†…å®¹
            current_content.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
    if current_h1 and current_h2 and current_content:
        if current_h1 not in sections:
            sections[current_h1] = {}
        sections[current_h1][current_h2] = '\n'.join(current_content).strip()
    
    return sections

def _generate_markdown_from_claims(unified_claims: Dict[str, Any], original_document: str) -> str:
    """
    ä»ç»Ÿä¸€è®ºæ–­ç»“æœç”Ÿæˆå®Œæ•´çš„markdownæ–‡æ¡£
    
    Args:
        unified_claims: ç»Ÿä¸€è®ºæ–­ç»“æœï¼ˆåµŒå¥—ç»“æ„ï¼‰
        original_document: åŸå§‹æ–‡æ¡£å†…å®¹
        
    Returns:
        str: ç”Ÿæˆçš„markdownæ–‡æ¡£
    """
    lines = []
    
    # æå–æ–‡æ¡£å¼€å¤´çš„éç« èŠ‚å†…å®¹ï¼ˆå¦‚æ ‡é¢˜ã€æ‘˜è¦ç­‰ï¼‰
    doc_lines = original_document.split('\n')
    header_lines = []
    
    for line in doc_lines:
        if line.strip().startswith('# ') and not line.strip().startswith('## '):
            break
        header_lines.append(line)
    
    # æ·»åŠ æ–‡æ¡£å¤´éƒ¨
    if header_lines:
        lines.extend(header_lines)
        lines.append('')
    
    # æ·»åŠ è¯æ®å¢å¼ºæ ‡è®°
    lines.append("## ğŸ“‹ è¯æ®å¢å¼ºè¯´æ˜")
    lines.append("**æœ¬æ–‡æ¡£å·²é€šè¿‡AIè¯æ®æœç´¢å’Œåˆ†æè¿›è¡Œå¢å¼ºï¼Œæ‰€æœ‰è®ºæ–­éƒ½å·²è¡¥å……ç›¸å…³è¯æ®æ”¯æ’‘ã€‚**")
    lines.append("")
    lines.append("*ä»¥ä¸‹å†…å®¹ä¸­çš„è®ºæ–­å‡å·²é€šè¿‡ç½‘ç»œæœç´¢éªŒè¯å¹¶å¢å¼ºã€‚*")
    lines.append("")
    
    # æŒ‰åŸå§‹æ–‡æ¡£ä¸­çš„ç« èŠ‚é¡ºåºç”Ÿæˆå†…å®¹
    original_sections = extract_document_sections(original_document)
    
    for h1_title, h2_sections in original_sections.items():
        if h1_title in unified_claims:
            # æ·»åŠ ä¸€çº§æ ‡é¢˜
            lines.append(f'# {h1_title}')
            lines.append('')
            
            # éå†äºŒçº§æ ‡é¢˜
            for h2_title in h2_sections.keys():
                if h2_title in unified_claims[h1_title]:
                    # æ·»åŠ äºŒçº§æ ‡é¢˜
                    lines.append(f'## {h2_title}')
                    lines.append('')
                    
                    # æ·»åŠ è¯æ®å¢å¼ºæ ‡è®°
                    lines.append("*[æœ¬ç« èŠ‚çš„è®ºæ–­å·²é€šè¿‡è¯æ®æœç´¢è¿›è¡Œå¢å¼º]*")
                    lines.append("")
                    
                    # è·å–å¢å¼ºåçš„å†…å®¹
                    section_claims = unified_claims[h1_title][h2_title]
                    enhanced_content = h2_sections[h2_title]  # é»˜è®¤ä½¿ç”¨åŸå†…å®¹
                    
                    # å¦‚æœæœ‰å¢å¼ºçš„è®ºæ–­ï¼Œæ›¿æ¢ç›¸åº”å†…å®¹
                    for claim_id, claim_data in section_claims.items():
                        if claim_data.get('status') == 'enhanced':
                            # åœ¨åŸå†…å®¹ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢åŸå§‹è®ºæ–­
                            original_claim = claim_data.get('original_content', '')
                            regenerated_claim = claim_data.get('regenerated_content', '')
                            if original_claim and regenerated_claim:
                                enhanced_content = enhanced_content.replace(original_claim, regenerated_claim)
                    
                    # æ·»åŠ å†…å®¹
                    if enhanced_content:
                        lines.append(enhanced_content)
                        lines.append('')
    
    return '\n'.join(lines).strip()

def _enhance_claim_with_evidence(claim_text: str, evidence_sources: List[Dict[str, Any]]) -> str:
    """
    ä½¿ç”¨è¯æ®æ¥å¢å¼ºè®ºæ–­
    
    Args:
        claim_text: åŸå§‹è®ºæ–­æ–‡æœ¬
        evidence_sources: è¯æ®æ¥æºåˆ—è¡¨
        
    Returns:
        str: å¢å¼ºåçš„è®ºæ–­æ–‡æœ¬
    """
    try:
        if not evidence_sources or not isinstance(evidence_sources, list):
            return claim_text
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å‰3ä¸ªè¯æ®
        top_evidence = evidence_sources[:3]
        
        # æ„å»ºè¯æ®æ‘˜è¦
        evidence_snippets = []
        for source in top_evidence:
            try:
                if not isinstance(source, dict):
                    continue
                    
                snippet = source.get('snippet', '').strip()
                domain = source.get('source_domain', '').strip()
                
                # æ¸…ç†snippetä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…JSONé—®é¢˜
                if snippet:
                    # ç§»é™¤å¯èƒ½å¯¼è‡´JSONé—®é¢˜çš„å­—ç¬¦
                    snippet = snippet.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
                    snippet = snippet[:100]  # é™åˆ¶é•¿åº¦
                    
                if snippet and domain:
                    evidence_snippets.append(f"{snippet} (æ¥æº: {domain})")
                elif snippet:
                    evidence_snippets.append(snippet)
                    
            except Exception as e:
                print(f"    âš ï¸ å¤„ç†è¯æ®æºæ—¶å‡ºé”™: {str(e)}")
                continue
        
        if not evidence_snippets:
            return claim_text
        
        # ç®€å•çš„å¢å¼ºé€»è¾‘ï¼šåœ¨åŸè®ºæ–­åæ·»åŠ è¯æ®æ”¯æ’‘
        evidence_text = "ï¼›".join(evidence_snippets[:2])  # åªä½¿ç”¨å‰2ä¸ªæœ€ç›¸å…³çš„è¯æ®
        
        # æ¸…ç†evidence_textï¼Œç¡®ä¿JSONå®‰å…¨
        evidence_text = evidence_text.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
        
        # å¦‚æœåŸè®ºæ–­ä»¥å¥å·ç»“å°¾ï¼Œæ›¿æ¢ä¸ºåˆ†å·ï¼›å¦åˆ™æ·»åŠ åˆ†å·
        if claim_text.endswith('ã€‚'):
            enhanced_claim = claim_text[:-1] + f"ã€‚æ ¹æ®ç›¸å…³èµ„æ–™æ˜¾ç¤ºï¼Œ{evidence_text}ã€‚"
        else:
            enhanced_claim = claim_text + f"ã€‚ç›¸å…³ç ”ç©¶è¡¨æ˜ï¼Œ{evidence_text}ã€‚"
        
        # ç¡®ä¿è¿”å›çš„æ–‡æœ¬æ˜¯JSONå®‰å…¨çš„
        enhanced_claim = enhanced_claim.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
        
        return enhanced_claim
        
    except Exception as e:
        print(f"    âŒ å¢å¼ºè®ºæ–­æ—¶å‡ºé”™: {str(e)}")
        return claim_text

# =============================================================================
# Pydanticæ¨¡å‹å®šä¹‰
# =============================================================================

class SystemInfoResponse(BaseModel):
    system_name: str = "è®ºæ®æ”¯æŒåº¦è¯„ä¼°ç³»ç»Ÿ"
    version: str = "1.0.0"
    description: str = "åŸºäºAIçš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œç”¨äºéªŒè¯å­¦æœ¯æ–‡æ¡£ä¸­è®ºç‚¹çš„äº‹å®æ”¯æ’‘"
    supported_formats: List[str] = ["markdown", "json", "txt"]
    max_file_size_mb: int = 50
    features: List[str] = [
        "æ£€æµ‹ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­",
        "å¹¶è¡Œç½‘ç»œæœç´¢è¯æ®æ”¯æ’‘", 
        "ç« èŠ‚å¹¶è¡Œå¤„ç†æ¨¡å¼",
        "AIç›´æ¥ç”Ÿæˆä¿®æ”¹æ–‡æ¡£",
        "ä¸‰æ­¥éª¤å®Œæ•´æµæ°´çº¿å¤„ç†",
        "å¼‚æ­¥å¤„ç†æ”¯æŒ"
    ]

class ExtractClaimsRequest(BaseModel):
    content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    max_claims: int = Field(default=15, ge=1, le=50, description="æœ€å¤§è®ºæ–­æå–æ•°é‡")

class ExtractClaimsResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    claims: List[Dict[str, Any]] = Field(default=[], description="æå–çš„è®ºæ–­åˆ—è¡¨")
    total_claims: int = Field(default=0, description="è®ºæ–­æ€»æ•°")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class SearchEvidenceRequest(BaseModel):
    claims: List[Dict[str, Any]] = Field(..., description="éœ€è¦æœç´¢è¯æ®çš„è®ºæ–­åˆ—è¡¨")
    max_results_per_claim: int = Field(default=10, ge=1, le=20, description="æ¯ä¸ªè®ºæ–­çš„æœ€å¤§æœç´¢ç»“æœæ•°")

class SearchEvidenceResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    evidence_collections: List[Dict[str, Any]] = Field(default=[], description="è¯æ®æ”¶é›†ç»“æœ")
    total_evidence_found: int = Field(default=0, description="æ‰¾åˆ°çš„è¯æ®æ€»æ•°")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class AnalyzeEvidenceRequest(BaseModel):
    claims: List[Dict[str, Any]] = Field(..., description="è®ºæ–­åˆ—è¡¨")
    evidence_collections: List[Dict[str, Any]] = Field(..., description="è¯æ®æ”¶é›†ç»“æœ")
    original_content: Optional[str] = Field(default=None, description="åŸå§‹æ–‡æ¡£å†…å®¹")

class AnalyzeEvidenceResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    analysis_results: List[Dict[str, Any]] = Field(default=[], description="è¯æ®åˆ†æç»“æœ")
    summary: Dict[str, Any] = Field(default={}, description="åˆ†ææ‘˜è¦")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class WebSearchRequest(BaseModel):
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    max_results: int = Field(default=10, ge=1, le=20, description="æœ€å¤§æœç´¢ç»“æœæ•°")

class WebSearchResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    query: str = Field(description="æœç´¢æŸ¥è¯¢")
    search_results: List[Dict[str, Any]] = Field(default=[], description="æœç´¢ç»“æœ")
    total_results: int = Field(default=0, description="ç»“æœæ€»æ•°")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class PipelineRequest(BaseModel):
    content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    max_claims: int = Field(default=15, ge=1, le=50, description="æœ€å¤§è®ºæ–­å¤„ç†æ•°é‡")
    max_search_results: int = Field(default=10, ge=1, le=20, description="æ¯ä¸ªè®ºæ–­çš„æœ€å¤§æœç´¢ç»“æœæ•°")
    use_section_based_processing: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨ç« èŠ‚å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼ˆæ¨èï¼‰")

class PipelineResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    enhanced_document: Optional[str] = Field(default=None, description="å¢å¼ºåçš„æ–‡æ¡£")
    evidence_analysis: Optional[Dict[str, Any]] = Field(default=None, description="è¯æ®åˆ†æç»“æœ")
    statistics: Optional[Dict[str, Any]] = Field(default=None, description="å¤„ç†ç»Ÿè®¡")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class AsyncTaskResponse(BaseModel):
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="ä»»åŠ¡çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")

class TaskStatusResponse(BaseModel):
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="ä»»åŠ¡çŠ¶æ€")
    progress: Optional[str] = Field(default=None, description="è¿›åº¦ä¿¡æ¯")
    result: Optional[Dict[str, Any]] = Field(default=None, description="å¤„ç†ç»“æœ")
    created_at: Optional[str] = Field(default=None, description="åˆ›å»ºæ—¶é—´")
    completed_at: Optional[str] = Field(default=None, description="å®Œæˆæ—¶é—´")

class DocumentProcessResponse(BaseModel):
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    statistics: Optional[Dict[str, Any]] = Field(default=None, description="å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
    output_files: Optional[Dict[str, str]] = Field(default=None, description="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class ClaimResult(BaseModel):
    claim_id: str = Field(description="è®ºæ–­ID")
    original_content: str = Field(description="åŸå§‹è®ºæ–­å†…å®¹")
    regenerated_content: str = Field(description="è¯æ®å¢å¼ºåçš„å†…å®¹")
    evidence_sources: List[Dict[str, Any]] = Field(default=[], description="è¯æ®æ¥æº")
    confidence_score: float = Field(default=0.0, description="ç½®ä¿¡åº¦åˆ†æ•°")
    word_count: int = Field(description="å­—æ•°ç»Ÿè®¡")
    status: str = Field(description="å¤„ç†çŠ¶æ€")

class WebAgentAnalysisResult(BaseModel):
    claims: Dict[str, ClaimResult] = Field(description="æŒ‰ç« èŠ‚ç»„ç»‡çš„è®ºæ–­ç»“æœ")
    processing_time: float = Field(description="å¤„ç†æ—¶é—´")
    statistics: Dict[str, Any] = Field(description="ç»Ÿè®¡ä¿¡æ¯")

# =============================================================================
# åˆå§‹åŒ–å‡½æ•°
# =============================================================================

async def initialize_pipeline():
    """åˆå§‹åŒ–æµæ°´çº¿"""
    global pipeline
    if not pipeline:
        if WholeDocumentPipeline is None:
            raise HTTPException(
                status_code=503, 
                detail="web_agent_appæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè®ºæ®æ”¯æŒåº¦è¯„ä¼°æœåŠ¡ä¸å¯ç”¨"
            )
        try:
            print("ğŸš€ åˆå§‹åŒ–è®ºæ®æ”¯æŒåº¦è¯„ä¼°ç³»ç»Ÿ...")
            pipeline = WholeDocumentPipeline()
            print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")

# =============================================================================
# API ç«¯ç‚¹
# =============================================================================

# @router.get("/test")
# async def test_route():
#     """æµ‹è¯•è·¯ç”±æ˜¯å¦å·¥ä½œ"""
#     return {"message": "è®ºæ®æ”¯æŒåº¦è¯„ä¼°æœåŠ¡è·¯ç”±å·¥ä½œæ­£å¸¸!"}

@router.get("/", response_model=SystemInfoResponse)
async def service_info():
    """è·å–è®ºæ®æ”¯æŒåº¦è¯„ä¼°æœåŠ¡ä¿¡æ¯"""
    return SystemInfoResponse()

# @router.get("/health")
# async def health_check():
#     """å¥åº·æ£€æŸ¥"""
#     await initialize_pipeline()
#     return {
#         "status": "healthy",
#         "timestamp": datetime.now().isoformat(),
#         "pipeline_ready": pipeline is not None,
#         "api_available": bool(os.getenv("OPENROUTER_API_KEY"))
#     }

@router.post("/v1/extract-claims", response_model=ExtractClaimsResponse)
async def extract_claims(request: ExtractClaimsRequest):
    """æå–æ ¸å¿ƒè®ºç‚¹"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ” å¼€å§‹æå–ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­ï¼Œæ–‡æ¡£é•¿åº¦: {len(request.content)} å­—ç¬¦")
        
        # ä½¿ç”¨æ–°çš„evidence_detectoræ£€æµ‹è®ºæ–­
        detector = pipeline.evidence_detector
        
        # ç›´æ¥å¤„ç†å†…å®¹ï¼Œä¸éœ€è¦ä¸´æ—¶æ–‡ä»¶
        unsupported_claims = detector._detect_unsupported_claims("å®Œæ•´æ–‡æ¡£", request.content)
        
        if len(unsupported_claims) > request.max_claims:
            unsupported_claims = sorted(unsupported_claims, key=lambda x: x.confidence_level, reverse=True)[:request.max_claims]
        
        claims_data = []
        for claim in unsupported_claims:
            claims_data.append({
                "claim_id": claim.claim_id,
                "claim_text": claim.claim_text,
                "claim_type": claim.claim_type,
                "confidence_level": claim.confidence_level,
                "section_title": claim.section_title,
                "search_keywords": claim.search_keywords,
                "context": claim.context
            })
        
        processing_time = time.time() - start_time
        print(f"âœ… ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­æå–å®Œæˆï¼Œå…±æå– {len(claims_data)} ä¸ªè®ºæ–­ï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        return ExtractClaimsResponse(
            status="success",
            message=f"æˆåŠŸæå– {len(claims_data)} ä¸ªç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­",
            claims=claims_data,
            total_claims=len(claims_data),
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"æå–å®¢è§‚æ€§è®ºæ–­æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return ExtractClaimsResponse(
            status="failed",
            message="å®¢è§‚æ€§è®ºæ–­æå–å¤±è´¥",
            processing_time=processing_time,
            error=error_msg
        )

@router.post("/v1/search-evidence", response_model=SearchEvidenceResponse)
async def search_evidence(request: SearchEvidenceRequest):
    """ä¸ºè®ºæ–­æœç´¢è¯æ®æ”¯æ’‘"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ” å¼€å§‹ä¸º {len(request.claims)} ä¸ªè®ºæ–­æœç´¢è¯æ®...")
        
        # è½¬æ¢è®ºæ–­æ•°æ®ä¸º UnsupportedClaim å¯¹è±¡
        claims = []
        for i, claim_data in enumerate(request.claims):
            claim = UnsupportedClaim(
                claim_id=claim_data.get('claim_id', f'claim_{i+1}'),
                claim_text=claim_data['claim_text'],
                section_title=claim_data.get('section_title', f'ä½ç½®_{i+1}'),
                claim_type=claim_data.get('claim_type', 'factual'),
                confidence_level=claim_data.get('confidence_level', 0.8),
                context=claim_data.get('context', ''),
                search_keywords=claim_data.get('search_keywords', []),
                original_position=i+1
            )
            claims.append(claim)
        
        # ä½¿ç”¨æ–°çš„evidence_detectoræ‰¹é‡æœç´¢è¯æ®
        evidence_results = pipeline.evidence_detector._batch_search_evidence(claims)
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        evidence_data = []
        total_evidence = 0
        for er in evidence_results:
            evidence_info = {
                'claim_id': er.claim_id,
                'search_query': er.search_query,
                'search_results': [
                    {
                        'title': source.get('title', ''),
                        'url': source.get('url', ''),
                        'snippet': source.get('snippet', ''),
                        'source_domain': source.get('source_domain', ''),
                        'relevance_score': source.get('relevance_score', 0.0),
                        'authority_score': source.get('authority_score', 0.0)
                    } for source in er.evidence_sources
                ],
                'total_results': len(er.evidence_sources),
                'processing_status': er.processing_status,
                'confidence_score': er.confidence_score
            }
            evidence_data.append(evidence_info)
            total_evidence += len(er.evidence_sources)
        
        processing_time = time.time() - start_time
        print(f"âœ… è¯æ®æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_evidence} æ¡è¯æ®ï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        return SearchEvidenceResponse(
            status="success",
            message=f"æˆåŠŸä¸º {len(request.claims)} ä¸ªè®ºæ–­æœç´¢åˆ° {total_evidence} æ¡è¯æ®",
            evidence_collections=evidence_data,
            total_evidence_found=total_evidence,
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"æœç´¢è¯æ®æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return SearchEvidenceResponse(
            status="failed",
            message="è¯æ®æœç´¢å¤±è´¥",
            processing_time=processing_time,
            error=error_msg
        )

@router.post("/v1/analyze-evidence", response_model=AnalyzeEvidenceResponse)
async def analyze_evidence(request: AnalyzeEvidenceRequest):
    """åˆ†æè¯æ®å¹¶ç”Ÿæˆå¢å¼ºå†…å®¹"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ¤– å¼€å§‹åˆ†æ {len(request.claims)} ä¸ªè®ºæ–­çš„è¯æ®...")
        
        # è½¬æ¢è®ºæ–­æ•°æ®
        claims = []
        for i, claim_data in enumerate(request.claims):
            claim = UnsupportedClaim(
                claim_id=claim_data.get('claim_id', f'claim_{i+1}'),
                claim_text=claim_data['claim_text'],
                section_title=claim_data.get('section_title', f'ä½ç½®_{i+1}'),
                claim_type=claim_data.get('claim_type', 'factual'),
                confidence_level=claim_data.get('confidence_level', 0.8),
                context=claim_data.get('context', ''),
                search_keywords=claim_data.get('search_keywords', []),
                original_position=i+1
            )
            claims.append(claim)
        
        # è½¬æ¢è¯æ®æ•°æ®ä¸ºEvidenceResultæ ¼å¼
        evidence_results = []
        for ec_data in request.evidence_collections:
            er = EvidenceResult(
                claim_id=ec_data['claim_id'],
                claim_text=next((c.claim_text for c in claims if c.claim_id == ec_data['claim_id']), ''),
                section_title=next((c.section_title for c in claims if c.claim_id == ec_data['claim_id']), ''),
                search_query=ec_data.get('search_query', ''),
                evidence_sources=ec_data.get('search_results', []),
                enhanced_text='',  # å°†ç”±æ–‡æ¡£ç”Ÿæˆå™¨å¡«å……
                confidence_score=ec_data.get('confidence_score', 0.0),
                processing_status=ec_data.get('processing_status', 'success')
            )
            evidence_results.append(er)
        
        # ä½¿ç”¨æ–‡æ¡£ç”Ÿæˆå™¨ç”Ÿæˆå¢å¼ºå†…å®¹
        if request.original_content:
            enhanced_content = pipeline.document_generator.generate_section_with_evidence(
                section_title="å®Œæ•´æ–‡æ¡£",
                original_content=request.original_content,
                evidence_results=evidence_results
            )
        else:
            enhanced_content = "æ— åŸå§‹å†…å®¹æä¾›"
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        analysis_data = []
        for er in evidence_results:
            analysis_info = {
                'claim_id': er.claim_id,
                'claim_text': er.claim_text,
                'section_title': er.section_title,
                'search_query': er.search_query,
                'evidence_sources': er.evidence_sources,
                'confidence_score': er.confidence_score,
                'processing_status': er.processing_status
            }
            analysis_data.append(analysis_info)
        
        processing_time = time.time() - start_time
        print(f"âœ… è¯æ®åˆ†æå’Œæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        summary = {
            'total_claims_analyzed': len(evidence_results),
            'successful_claims': sum(1 for er in evidence_results if er.processing_status == 'success'),
            'total_evidence_sources': sum(len(er.evidence_sources) for er in evidence_results),
            'enhanced_content_generated': bool(enhanced_content and enhanced_content != "æ— åŸå§‹å†…å®¹æä¾›")
        }
        
        return AnalyzeEvidenceResponse(
            status="success",
            message=f"æˆåŠŸåˆ†æ {len(evidence_results)} ä¸ªè®ºæ–­çš„è¯æ®å¹¶ç”Ÿæˆå¢å¼ºå†…å®¹",
            analysis_results=analysis_data,
            summary=summary,
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"åˆ†æè¯æ®æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return AnalyzeEvidenceResponse(
            status="failed",
            message="è¯æ®åˆ†æå¤±è´¥",
            processing_time=processing_time,
            error=error_msg
        )

@router.post("/v1/websearch", response_model=WebSearchResponse)
async def websearch(request: WebSearchRequest):
    """ç½‘ç»œæœç´¢æ¥å£"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ” æ‰§è¡Œç½‘ç»œæœç´¢: {request.query}")
        
        # ä½¿ç”¨webæœç´¢ä»£ç†æ‰§è¡Œæœç´¢
        evidence = pipeline.web_search_agent.search_evidence_for_claim(
            claim_id="websearch_query",
            search_keywords=[request.query],
            claim_text=request.query,
            max_results=request.max_results
        )
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        search_results = []
        for sr in evidence.search_results:
            result_info = {
                'title': sr.title,
                'url': sr.url,
                'snippet': sr.snippet,
                'source_domain': sr.source_domain,
                'relevance_score': sr.relevance_score,
                'authority_score': sr.authority_score
            }
            search_results.append(result_info)
        
        processing_time = time.time() - start_time
        print(f"âœ… ç½‘ç»œæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        return WebSearchResponse(
            status="success",
            message=f"æˆåŠŸæœç´¢åˆ° {len(search_results)} ä¸ªç»“æœ",
            query=request.query,
            search_results=search_results,
            total_results=len(search_results),
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"ç½‘ç»œæœç´¢æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return WebSearchResponse(
            status="failed",
            message="ç½‘ç»œæœç´¢å¤±è´¥",
            query=request.query,
            processing_time=processing_time,
            error=error_msg
        )

@router.post("/v1/pipeline", response_model=PipelineResponse)
async def pipeline_sync(request: PipelineRequest):
    """å®Œæ•´æµæ°´çº¿å¤„ç†ï¼ˆåŒæ­¥ï¼‰"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    max_content_length = 1024 * 1024  # 1MB
    if len(request.content.encode('utf-8')) > max_content_length:
        raise HTTPException(status_code=413, detail="æ–‡æ¡£å†…å®¹è¿‡å¤§ï¼Œè¶…è¿‡1MBé™åˆ¶")
    
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, "document.md")
    
    try:
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(request.content)
        
        print(f"ğŸ”„ å¼€å§‹å®Œæ•´æµæ°´çº¿å¤„ç†ï¼Œæ–‡æ¡£é•¿åº¦: {len(request.content)} å­—ç¬¦")
        
        result = pipeline.process_whole_document(
            document_path=temp_file_path,
            max_claims=request.max_claims,
            max_search_results=request.max_search_results,
            use_section_based_processing=request.use_section_based_processing
        )
        
        processing_time = time.time() - start_time
        
        if result.get('status') == 'success':
            output_files = result.get('output_files', {})
            
            enhanced_document = request.content
            if 'enhanced_document' in output_files and os.path.exists(output_files['enhanced_document']):
                try:
                    with open(output_files['enhanced_document'], 'r', encoding='utf-8') as f:
                        enhanced_document = f.read()
                except Exception as e:
                    print(f"âš ï¸ è¯»å–å¢å¼ºæ–‡æ¡£å¤±è´¥: {str(e)}")
            
            evidence_analysis = {}
            if 'evidence_analysis' in output_files and os.path.exists(output_files['evidence_analysis']):
                try:
                    with open(output_files['evidence_analysis'], 'r', encoding='utf-8') as f:
                        evidence_analysis = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–è¯æ®åˆ†æå¤±è´¥: {str(e)}")
            
            print(f"âœ… å®Œæ•´æµæ°´çº¿å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.1f}ç§’")
            
            return PipelineResponse(
                status="success",
                message="å®Œæ•´æµæ°´çº¿å¤„ç†æˆåŠŸå®Œæˆ",
                enhanced_document=enhanced_document,
                evidence_analysis=evidence_analysis,
                statistics=result.get('statistics', {}),
                processing_time=processing_time
            )
        else:
            error_msg = result.get('error', 'å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯')
            print(f"âŒ æµæ°´çº¿å¤„ç†å¤±è´¥: {error_msg}")
            
            return PipelineResponse(
                status="failed",
                message="æµæ°´çº¿å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸæ–‡æ¡£",
                enhanced_document=request.content,
                evidence_analysis={},
                processing_time=processing_time,
                error=error_msg
            )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"æµæ°´çº¿å¤„ç†æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return PipelineResponse(
            status="failed",
            message="æµæ°´çº¿å¤„ç†å¼‚å¸¸ï¼Œè¿”å›åŸæ–‡æ¡£",
            enhanced_document=request.content,
            evidence_analysis={},
            processing_time=processing_time,
            error=error_msg
        )
    
    finally:
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {str(e)}")

@router.post("/v1/upload", response_model=AsyncTaskResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    max_claims: int = Form(default=15),
    max_search_results: int = Form(default=10),
    use_section_based_processing: bool = Form(default=True)
):
    """æ–‡ä»¶ä¸Šä¼ å¤„ç†ï¼ˆå¼‚æ­¥ï¼‰"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    allowed_extensions = {'.md', '.json', '.txt'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(allowed_extensions)}"
        )
    
    max_size = 50 * 1024 * 1024  # 50MB
    content = await file.read()
    if len(content) > max_size:
        raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡50MBé™åˆ¶")
    
    task_id = f"task_{int(time.time())}_{hash(file.filename) % 10000}"
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, file.filename)
    
    try:
        with open(temp_file_path, 'wb') as f:
            f.write(content)
        
        processing_tasks[task_id] = {
            "status": "processing",
            "start_time": time.time(),
            "progress": "æ–‡ä»¶å·²ä¸Šä¼ ï¼Œå¼€å§‹å¤„ç†...",
            "temp_dir": temp_dir,
            "created_at": datetime.now().isoformat()
        }
        
        background_tasks.add_task(
            process_document_background,
            task_id,
            temp_file_path,
            max_claims,
            max_search_results,
            use_section_based_processing
        )
        
        return AsyncTaskResponse(
            task_id=task_id,
            status="processing",
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹å¤„ç†"
        )
        
    except Exception as e:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@router.post("/v1/pipeline-async", response_model=AsyncTaskResponse)
async def pipeline_async(
    background_tasks: BackgroundTasks,
    request: PipelineRequest
):
    """å¼‚æ­¥æµæ°´çº¿å¤„ç†"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    max_content_length = 1024 * 1024  # 1MB
    if len(request.content.encode('utf-8')) > max_content_length:
        raise HTTPException(status_code=413, detail="æ–‡æ¡£å†…å®¹è¿‡å¤§ï¼Œè¶…è¿‡1MBé™åˆ¶")
    
    task_id = f"task_{int(time.time())}_{hash(request.content[:100]) % 10000}"
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, "document.md")
    
    try:
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(request.content)
        
        processing_tasks[task_id] = {
            "status": "processing",
            "start_time": time.time(),
            "progress": "å†…å®¹å·²æ¥æ”¶ï¼Œå¼€å§‹å¤„ç†...",
            "temp_dir": temp_dir,
            "created_at": datetime.now().isoformat()
        }
        
        background_tasks.add_task(
            process_document_background,
            task_id,
            temp_file_path,
            request.max_claims,
            request.max_search_results,
            request.use_section_based_processing
        )
        
        return AsyncTaskResponse(
            task_id=task_id,
            status="processing",
            message="å†…å®¹æ¥æ”¶æˆåŠŸï¼Œå¼€å§‹å¼‚æ­¥å¤„ç†"
        )
        
    except Exception as e:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

# æ—§çš„ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢å‡½æ•°å·²è¢«åˆ é™¤ï¼Œä½¿ç”¨æ–°çš„ get_evidence_task_status

@router.get("/v1/download/{task_id}")
async def download_task_result(task_id: str, file_type: str = "enhanced_document"):
    """ä¸‹è½½å¤„ç†ç»“æœ"""
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = processing_tasks[task_id]
    
    if task_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")
    
    result = task_info.get("result")
    if not result or not result.output_files:
        raise HTTPException(status_code=404, detail="ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
    
    file_mapping = {
        "enhanced_document": "enhanced_document",
        "evidence_analysis": "evidence_analysis",
        "document": "enhanced_document",
        "analysis": "evidence_analysis"
    }
    
    if file_type not in file_mapping:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„æ–‡ä»¶ç±»å‹: {file_type}")
    
    file_path = result.output_files.get(file_mapping[file_type])
    if not file_path or not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    if file_type in ["enhanced_document", "document"]:
        filename = f"enhanced_document_{task_id}.md"
        media_type = "text/markdown"
    else:
        filename = f"evidence_analysis_{task_id}.json"
        media_type = "application/json"
    
        return FileResponse(
            path=file_path,
            filename=filename,
            media_type=media_type
        )

# =============================================================================
# æ–°å¢ï¼šThesis Agenté£æ ¼çš„APIç«¯ç‚¹
# =============================================================================

class EvidencePipelineRequest(BaseModel):
    document_content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    document_title: str = Field(default="æ–‡æ¡£", description="æ–‡æ¡£æ ‡é¢˜")
    max_claims: int = Field(default=7, description="æœ€å¤§è®ºæ–­æ•°é‡")
    max_search_results: int = Field(default=10, description="æœ€å¤§æœç´¢ç»“æœæ•°")

async def process_evidence_pipeline_async(
    task_id: str,
    document_content: str,
    document_title: str,
    max_claims: int,
    max_search_results: int
):
    """å¼‚æ­¥å¤„ç†è¯æ®å¢å¼ºæµæ°´çº¿"""
    try:
        update_task_status(task_id, "running", 10.0, "å¼€å§‹è¯æ®åˆ†æ")
        
        # ä½¿ç”¨ç°æœ‰çš„pipelineå¤„ç†æ–‡æ¡£
        await initialize_pipeline()
        if not pipeline:
            raise Exception("ç³»ç»Ÿæœªåˆå§‹åŒ–")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as temp_file:
            temp_file.write(document_content)
            temp_file_path = temp_file.name
        
        try:
            update_task_status(task_id, "running", 30.0, "æ£€æµ‹è®ºæ–­")
            
            # ä½¿ç”¨pipelineå¤„ç†æ–‡æ¡£
            result = pipeline.process_whole_document(
                document_path=temp_file_path,
                max_claims=max_claims,
                max_search_results=max_search_results,
                use_section_based_processing=True
            )
            
            update_task_status(task_id, "running", 80.0, "ç”Ÿæˆç»Ÿä¸€æ ¼å¼è¾“å‡º")
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # ç¡®ä¿test_resultsç›®å½•å­˜åœ¨
            results_dir = Path("/Users/wangzijian/Desktop/gauz/keyan/review_agent_save/router/test_results")
            results_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            unified_sections_file = results_dir / f"web_agent_unified_{task_id}_{timestamp}.json"
            enhanced_md_file = results_dir / f"web_enhanced_{task_id}_{timestamp}.md"
            
            if result['status'] == 'success':
                # ç”Ÿæˆunified_sectionsæ ¼å¼çš„æ•°æ®
                unified_sections = generate_unified_sections_from_result(result, document_content)
                
                # ä¿å­˜unified_sectionsæ–‡ä»¶
                with open(unified_sections_file, 'w', encoding='utf-8') as f:
                    json.dump(unified_sections, f, ensure_ascii=False, indent=2)
                
                # ç”Ÿæˆå¢å¼ºåçš„æ–‡æ¡£å†…å®¹
                enhanced_content = generate_enhanced_content_from_result(result, document_content)
                
                # ä¿å­˜å¢å¼ºåçš„markdownæ–‡ä»¶
                with open(enhanced_md_file, 'w', encoding='utf-8') as f:
                    f.write(enhanced_content)
                
                # æ„å»ºç»“æœ
                final_result = {
                    "unified_sections_file": str(unified_sections_file),
                    "enhanced_content_file": str(enhanced_md_file),
                    "processing_time": result.get('processing_time', 0),
                    "sections_count": len(unified_sections),
                    "service_type": "web_agent",
                    "message": f"å·²ç”Ÿæˆ2ä¸ªæ–‡ä»¶: {unified_sections_file.name}, {enhanced_md_file.name}",
                    "timestamp": timestamp
                }
                
                update_task_status(task_id, "completed", 100.0, "å¤„ç†å®Œæˆ", final_result)
            else:
                raise Exception(result.get('error', 'å¤„ç†å¤±è´¥'))
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except Exception as e:
        logger.error(f"å¼‚æ­¥ä»»åŠ¡å¤„ç†å¤±è´¥: {e}")
        update_task_status(task_id, "failed", 0.0, "å¤„ç†å¤±è´¥", error=str(e))

def generate_unified_sections_from_result(result: Dict, original_content: str) -> Dict:
    """ä»pipelineç»“æœç”Ÿæˆunified_sectionsæ ¼å¼çš„æ•°æ®"""
    # è§£ææ–‡æ¡£ç»“æ„
    sections = extract_document_sections(original_content)
    unified_sections = {}
    
    # ä»resultä¸­è¯»å–å®é™…çš„evidence_analysisæ–‡ä»¶
    evidence_analysis_data = []
    evidence_results_data = []
    
    if 'output_files' in result and 'evidence_analysis' in result['output_files']:
        evidence_file_path = result['output_files']['evidence_analysis']
        try:
            with open(evidence_file_path, 'r', encoding='utf-8') as f:
                evidence_data = json.load(f)
                evidence_analysis_data = evidence_data.get('unsupported_claims', [])
                evidence_results_data = evidence_data.get('evidence_results', [])
                print(f"âœ… è¯»å–evidenceæ–‡ä»¶: {len(evidence_analysis_data)} ä¸ªè®ºæ–­, {len(evidence_results_data)} ä¸ªè¯æ®ç»“æœ")
        except Exception as e:
            print(f"âŒ è¯»å–evidence_analysisæ–‡ä»¶å¤±è´¥: {e}")
    
    # å¦‚æœæ²¡æœ‰ä»æ–‡ä»¶è¯»å–åˆ°æ•°æ®ï¼Œå°è¯•ä»resultç›´æ¥è·å–
    if not evidence_analysis_data:
        evidence_analysis_data = result.get('evidence_analysis', [])
        # å¦‚æœevidence_analysisæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–unsupported_claims
        if isinstance(evidence_analysis_data, dict):
            evidence_analysis_data = evidence_analysis_data.get('unsupported_claims', [])
    
    print(f"ğŸ” æ‰¾åˆ° {len(evidence_analysis_data)} ä¸ªè®ºæ–­è¿›è¡Œåˆ†æ")
    
    for h1_title, h2_sections in sections.items():
        unified_sections[h1_title] = {}
        
        for h2_title, section_content in h2_sections.items():
            # æŸ¥æ‰¾è¯¥ç« èŠ‚çš„è®ºæ–­
            section_claims = []
            for claim in evidence_analysis_data:
                if isinstance(claim, dict):
                    section_title = claim.get('section_title', '')
                else:
                    section_title = getattr(claim, 'section_title', '')
                
                if (section_title == h2_title or 
                    section_title == f"{h1_title} {h2_title}" or
                    h2_title in section_title or
                    section_title == h1_title):
                    section_claims.append(claim)
            
            if section_claims:
                # æŸ¥æ‰¾å¯¹åº”çš„è¯æ®ç»“æœ
                total_evidence_count = 0
                enhanced_content = section_content
                suggestions = []
                
                for claim in section_claims:
                    claim_id = claim.get('claim_id') if isinstance(claim, dict) else getattr(claim, 'claim_id', '')
                    
                    # åœ¨evidence_resultsä¸­æŸ¥æ‰¾å¯¹åº”çš„è¯æ®
                    for evidence_result in evidence_results_data:
                        if isinstance(evidence_result, dict):
                            result_claim_id = evidence_result.get('claim_id', '')
                            evidence_sources = evidence_result.get('evidence_sources', [])
                            enhanced_text = evidence_result.get('enhanced_text', '')
                        else:
                            result_claim_id = getattr(evidence_result, 'claim_id', '')
                            evidence_sources = getattr(evidence_result, 'evidence_sources', [])
                            enhanced_text = getattr(evidence_result, 'enhanced_text', '')
                        
                        if result_claim_id == claim_id:
                            total_evidence_count += len(evidence_sources)
                            claim_text = claim.get('claim_text') if isinstance(claim, dict) else getattr(claim, 'claim_text', '')
                            
                            if len(evidence_sources) > 0:
                                suggestions.append(f"è®ºæ–­ã€Œ{claim_text}ã€æ‰¾åˆ° {len(evidence_sources)} ä¸ªè¯æ®æ”¯æŒ")
                            else:
                                suggestions.append(f"è®ºæ–­ã€Œ{claim_text}ã€æœªæ‰¾åˆ°å……åˆ†è¯æ®æ”¯æŒ")
                
                # ç”Ÿæˆå¢å¼ºå†…å®¹ï¼šä»enhanced_documentä¸­æå–å¯¹åº”ç« èŠ‚
                if 'output_files' in result and 'enhanced_document' in result['output_files']:
                    try:
                        enhanced_file_path = result['output_files']['enhanced_document']
                        with open(enhanced_file_path, 'r', encoding='utf-8') as f:
                            enhanced_doc = f.read()
                            # å°è¯•æå–å¯¹åº”ç« èŠ‚çš„å¢å¼ºå†…å®¹
                            import re
                            pattern = rf"## {re.escape(h2_title)}(.*?)(?=##|\Z)"
                            match = re.search(pattern, enhanced_doc, re.DOTALL)
                            if match:
                                enhanced_section = match.group(1).strip()
                                if enhanced_section and enhanced_section != section_content:
                                    enhanced_content = enhanced_section
                    except Exception as e:
                        print(f"âŒ æå–å¢å¼ºç« èŠ‚å†…å®¹å¤±è´¥: {e}")
                
                suggestion = "; ".join(suggestions) if suggestions else f"å‘ç° {len(section_claims)} ä¸ªè®ºæ–­ï¼Œæ‰¾åˆ° {total_evidence_count} ä¸ªè¯æ®æ”¯æŒ"
                
                unified_sections[h1_title][h2_title] = {
                    "original_content": section_content,
                    "suggestion": suggestion,
                    "regenerated_content": enhanced_content,
                    "word_count": len(section_content),
                    "status": "enhanced" if total_evidence_count > 0 else "identified"
                }
                
                print(f"âœ… ç« èŠ‚ {h2_title}: {len(section_claims)} ä¸ªè®ºæ–­, {total_evidence_count} ä¸ªè¯æ®")
    
    print(f"ğŸ“Š ç”Ÿæˆunified_sections: {len(unified_sections)} ä¸ªH1æ ‡é¢˜")
    return unified_sections

def generate_enhanced_content_from_result(result: Dict, original_content: str) -> str:
    """ä»pipelineç»“æœç”Ÿæˆå¢å¼ºåçš„æ–‡æ¡£å†…å®¹"""
    # ä»resultä¸­è¯»å–å®é™…çš„enhanced_documentæ–‡ä»¶
    if 'output_files' in result and 'enhanced_document' in result['output_files']:
        enhanced_file_path = result['output_files']['enhanced_document']
        try:
            with open(enhanced_file_path, 'r', encoding='utf-8') as f:
                enhanced_content = f.read()
                print(f"âœ… æˆåŠŸè¯»å–å¢å¼ºæ–‡æ¡£: {len(enhanced_content)} å­—ç¬¦")
                return enhanced_content
        except Exception as e:
            print(f"âŒ è¯»å–å¢å¼ºæ–‡æ¡£å¤±è´¥: {e}")
    
    print("âš ï¸ æœªæ‰¾åˆ°å¢å¼ºæ–‡æ¡£ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
    return original_content

@router.post("/v1/evidence-pipeline-async")
async def evidence_pipeline_async(
    background_tasks: BackgroundTasks,
    request: EvidencePipelineRequest
):
    """å¼‚æ­¥è¯æ®å¢å¼ºæµæ°´çº¿å¤„ç†ï¼ˆThesis Agenté£æ ¼ï¼‰"""
    await initialize_pipeline()
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    import uuid
    task_id = str(uuid.uuid4())
    
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    update_task_status(task_id, "pending", 0.0, "ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…å¤„ç†...")
    print(f"ğŸ”§ ä»»åŠ¡ {task_id} å·²åˆ›å»ºï¼Œå½“å‰å­˜å‚¨ä¸­çš„ä»»åŠ¡æ•°: {len(_task_storage)}")
    print(f"ğŸ” ä»»åŠ¡åˆ›å»ºåå­˜å‚¨å†…å®¹: {task_id in _task_storage}")
    print(f"ğŸ” å­˜å‚¨ä¸­çš„æ‰€æœ‰ä»»åŠ¡: {list(_task_storage.keys())}")
    
    # å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(
        process_evidence_pipeline_async,
        task_id,
        request.document_content,
        request.document_title,
        request.max_claims,
        request.max_search_results
    )
    
    return {
        "task_id": task_id,
        "status": "pending",
        "message": "ä»»åŠ¡å·²æäº¤ï¼Œè¯·ä½¿ç”¨task_idæŸ¥è¯¢è¿›åº¦"
    }

@router.get("/v1/task/{task_id}")
async def get_evidence_task_status(task_id: str):
    """æŸ¥è¯¢è¯æ®å¢å¼ºä»»åŠ¡çŠ¶æ€"""
    print(f"ğŸ” æŸ¥è¯¢ä»»åŠ¡ {task_id}ï¼Œå½“å‰å­˜å‚¨ä¸­çš„ä»»åŠ¡æ•°: {len(_task_storage)}")
    print(f"ğŸ” å­˜å‚¨ä¸­çš„ä»»åŠ¡IDåˆ—è¡¨: {list(_task_storage.keys())}")
    
    if task_id not in _task_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return _task_storage[task_id]

@router.get("/v1/result/{task_id}")
async def get_evidence_result(task_id: str):
    """è·å–çº¯å‡€çš„è®ºæ–­åˆ†æç»“æœJSON"""
    if task_id not in _task_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = _task_storage[task_id]
    if task_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")
    
    # ä»ç»“æœä¸­è·å–unified_sectionsæ–‡ä»¶è·¯å¾„å¹¶è¯»å–å†…å®¹
    result = task_info.get("result", {})
    if isinstance(result, dict) and "unified_sections_file" in result:
        unified_sections_file = result["unified_sections_file"]
        
        try:
            with open(unified_sections_file, 'r', encoding='utf-8') as f:
                unified_sections_data = json.load(f)
            return unified_sections_data
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="unified_sectionsæ–‡ä»¶ä¸å­˜åœ¨")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    else:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°unified_sectionsæ–‡ä»¶")

@router.get("/v1/enhanced/{task_id}")
async def get_enhanced_document(task_id: str):
    """è·å–è¯æ®å¢å¼ºåçš„markdownæ–‡æ¡£"""
    if task_id not in _task_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = _task_storage[task_id]
    if task_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")
    
    # ä»ç»“æœä¸­è·å–enhanced_contentæ–‡ä»¶è·¯å¾„å¹¶è¯»å–å†…å®¹
    result = task_info.get("result", {})
    if isinstance(result, dict) and "enhanced_content_file" in result:
        enhanced_content_file = result["enhanced_content_file"]
        
        try:
            with open(enhanced_content_file, 'r', encoding='utf-8') as f:
                content = f.read()
            return {"enhanced_document": content}
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="å¢å¼ºæ–‡æ¡£ä¸å­˜åœ¨")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    else:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°enhanced_contentæ–‡ä»¶")

# =============================================================================
# åå°å¤„ç†å‡½æ•°
# =============================================================================

