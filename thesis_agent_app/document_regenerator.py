#!/usr/bin/env python3
"""
åŸºäºè®ºç‚¹ä¸€è‡´æ€§ä¿®æ­£ç»“æœé‡æ–°ç”Ÿæˆå®Œæ•´æ–‡æ¡£çš„è„šæœ¬

è¯¥è„šæœ¬è¯»å–è®ºç‚¹ä¸€è‡´æ€§æ£€æŸ¥ç»“æœï¼Œå¯¹æœ‰é—®é¢˜çš„ç« èŠ‚è¿›è¡Œé‡æ–°ç”Ÿæˆï¼Œ
ç„¶åç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£ã€‚
"""

import json
import logging
import os
import sys
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Tuple
from datetime import datetime
from openai import OpenAI

# å¯¼å…¥ç›¸å…³æ¨¡å—
from config import config


class ThesisDocumentRegenerator:
    """
    åŸºäºè®ºç‚¹ä¸€è‡´æ€§çš„æ–‡æ¡£é‡æ–°ç”Ÿæˆå™¨
    
    è¯»å–ä¸€è‡´æ€§æ£€æŸ¥ç»“æœï¼Œé‡æ–°ç”Ÿæˆæœ‰é—®é¢˜çš„ç« èŠ‚ï¼Œå¹¶è¾“å‡ºå®Œæ•´æ–‡æ¡£
    """
    
    def __init__(self, max_workers: int = 5):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # å¤šçº¿ç¨‹é…ç½®
        self.max_workers = max_workers
        self._thread_local = threading.local()
        self._lock = threading.Lock()
        
        # è¿›åº¦è·Ÿè¸ª
        self._progress = {
            'total_sections': 0,
            'completed_sections': 0,
            'failed_sections': 0
        }
        
        self.logger.info(f"âœ… ThesisDocumentRegenerator åˆå§‹åŒ–å®Œæˆ (æœ€å¤§å·¥ä½œçº¿ç¨‹: {max_workers})")
    
    def _get_client(self) -> OpenAI:
        """
        è·å–çº¿ç¨‹æœ¬åœ°çš„OpenAIå®¢æˆ·ç«¯
        
        Returns:
            OpenAI: çº¿ç¨‹æœ¬åœ°çš„å®¢æˆ·ç«¯å®ä¾‹
        """
        if not hasattr(self._thread_local, 'client'):
            self._thread_local.client = OpenAI(
                base_url=config.openrouter_base_url,
                api_key=config.openrouter_api_key,
            )
        return self._thread_local.client
    
    def _update_progress(self, completed: bool = True, failed: bool = False):
        """
        çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°è¿›åº¦
        
        Args:
            completed: æ˜¯å¦æˆåŠŸå®Œæˆ
            failed: æ˜¯å¦å¤±è´¥
        """
        with self._lock:
            if completed:
                self._progress['completed_sections'] += 1
            if failed:
                self._progress['failed_sections'] += 1
            
            total = self._progress['total_sections']
            completed_count = self._progress['completed_sections']
            failed_count = self._progress['failed_sections']
            
            if total > 0:
                progress_pct = (completed_count + failed_count) / total * 100
                self.logger.info(
                    f"ğŸ“Š è¿›åº¦æ›´æ–°: {completed_count}/{total} å®Œæˆ "
                    f"({failed_count} å¤±è´¥) - {progress_pct:.1f}%"
                )
    
    def load_consistency_analysis(self, analysis_file: str) -> tuple[Dict, Dict]:
        """
        åŠ è½½ä¸€è‡´æ€§åˆ†æç»“æœæ–‡ä»¶
        
        Args:
            analysis_file: ä¸€è‡´æ€§åˆ†æç»“æœJSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            tuple: (ä¸€è‡´æ€§åˆ†ææ•°æ®, æ ¸å¿ƒè®ºç‚¹æ•°æ®)
        """
        try:
            with open(analysis_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–ä¸€è‡´æ€§åˆ†æå’Œæ ¸å¿ƒè®ºç‚¹ä¿¡æ¯
            consistency_data = data.get('consistency_analysis', {})
            thesis_data = data.get('thesis_statement', {})
            
            consistency_issues = consistency_data.get('consistency_issues', [])
            
            self.logger.info(f"æˆåŠŸåŠ è½½ä¸€è‡´æ€§åˆ†æç»“æœï¼Œå…±{len(consistency_issues)}ä¸ªéœ€è¦ä¿®æ­£çš„ç« èŠ‚")
            return consistency_data, thesis_data
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ä¸€è‡´æ€§åˆ†æç»“æœå¤±è´¥: {e}")
            return {}, {}
    
    def load_original_document(self, document_file: str) -> tuple[str, Dict]:
        """
        åŠ è½½åŸå§‹æ–‡æ¡£
        
        Args:
            document_file: åŸå§‹æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            
        Returns:
            tuple: (æ–‡æ¡£å†…å®¹, JSONæ•°æ®)
        """
        try:
            json_data = {}
            
            if document_file.endswith('.json'):
                # å¤„ç†JSONæ–‡æ¡£
                with open(document_file, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                
                content_parts = []
                report_guide = json_data.get('report_guide', [])
                total_sections = 0
                
                for part in report_guide:
                    sections = part.get('sections', [])
                    
                    for section in sections:
                        subtitle = section.get('subtitle', '')
                        generated_content = section.get('generated_content', '')
                        if subtitle and generated_content:
                            content_parts.append(f"## {subtitle}\n\n{generated_content}")
                            total_sections += 1
                
                content = "\n\n".join(content_parts)
                self.logger.info(f"æˆåŠŸåŠ è½½JSONæ–‡æ¡£: {document_file}ï¼Œæå–äº†{total_sections}ä¸ªç« èŠ‚")
                return content, json_data
            else:
                # å¤„ç†Markdownæ–‡æ¡£
                with open(document_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                self.logger.info(f"æˆåŠŸåŠ è½½Markdownæ–‡æ¡£: {document_file}")
                return content, json_data
                
        except Exception as e:
            self.logger.error(f"åŠ è½½åŸå§‹æ–‡æ¡£å¤±è´¥: {e}")
            return "", {}
    
    def extract_section_content(self, document_content: str, section_title: str) -> str:
        """
        ä»æ–‡æ¡£ä¸­æå–æŒ‡å®šç« èŠ‚çš„å†…å®¹
        
        Args:
            document_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            section_title: ç« èŠ‚æ ‡é¢˜ï¼ˆæ”¯æŒè·¯å¾„æ ¼å¼ï¼Œå¦‚ "ä¸€ã€å¼•è¨€/1.1ã€ç¼–å†™ç›®çš„"ï¼‰
            
        Returns:
            str: ç« èŠ‚å†…å®¹
        """
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºè·¯å¾„æ ¼å¼ï¼ˆåŒ…å« "/"ï¼‰
            if "/" in section_title:
                # è§£æè·¯å¾„æ ¼å¼
                path_parts = section_title.split("/")
                parent_title = path_parts[0].strip()
                child_title = path_parts[-1].strip()
                
                self.logger.debug(f"è§£æè·¯å¾„æ ¼å¼ç« èŠ‚: çˆ¶ç« èŠ‚='{parent_title}', å­ç« èŠ‚='{child_title}'")
                
                # é¦–å…ˆæ‰¾åˆ°çˆ¶ç« èŠ‚çš„ä½ç½®
                parent_pattern = rf"(?m)^\s*#\s*{re.escape(parent_title)}\s*$"
                parent_match = re.search(parent_pattern, document_content)
                
                if parent_match:
                    # ä»çˆ¶ç« èŠ‚å¼€å§‹çš„ä½ç½®æŸ¥æ‰¾å­ç« èŠ‚
                    parent_start = parent_match.end()
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜çš„ä½ç½®ä½œä¸ºçˆ¶ç« èŠ‚çš„ç»“æŸ
                    next_h1_pattern = r"(?m)^\s*#\s"
                    next_h1_match = re.search(next_h1_pattern, document_content[parent_start:])
                    
                    if next_h1_match:
                        parent_content = document_content[parent_start:parent_start + next_h1_match.start()]
                    else:
                        parent_content = document_content[parent_start:]
                    
                    # åœ¨çˆ¶ç« èŠ‚å†…å®¹ä¸­æŸ¥æ‰¾å­ç« èŠ‚
                    return self._extract_child_section(parent_content, child_title)
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°çˆ¶ç« èŠ‚: {parent_title}")
                    # å¦‚æœæ‰¾ä¸åˆ°çˆ¶ç« èŠ‚ï¼Œå°è¯•ç›´æ¥åŒ¹é…å­ç« èŠ‚æ ‡é¢˜
                    return self._extract_single_section(document_content, child_title)
            else:
                # å•ä¸€ç« èŠ‚æ ‡é¢˜ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
                return self._extract_single_section(document_content, section_title)
                
        except Exception as e:
            self.logger.error(f"æå–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def _extract_single_section(self, document_content: str, section_title: str) -> str:
        """
        æå–å•ä¸€ç« èŠ‚çš„å†…å®¹ï¼Œæ”¯æŒçµæ´»åŒ¹é…
        """
        try:
            # æ¸…ç†ç« èŠ‚æ ‡é¢˜ï¼Œç§»é™¤å¯èƒ½çš„æ‹¬å·å’Œç‰¹æ®Šå­—ç¬¦
            clean_title = section_title.strip()
            
            # å°è¯•å¤šç§åŒ¹é…ç­–ç•¥
            patterns_to_try = [
                # å®Œå…¨åŒ¹é…
                clean_title,
                # å¦‚æœåŒ…å«æ‹¬å·ï¼Œæå–æ‹¬å·å†…å®¹
                None,
                # å¦‚æœåŒ…å«ä¸­æ–‡æ•°å­—ï¼Œå°è¯•åŒ¹é…
                None
            ]
            
            # å¦‚æœæ ‡é¢˜åŒ…å«æ‹¬å·ï¼Œæå–æ‹¬å·å†…çš„å†…å®¹
            if 'ï¼ˆ' in clean_title and 'ï¼‰' in clean_title:
                bracket_content = clean_title[clean_title.find('ï¼ˆ')+1:clean_title.find('ï¼‰')]
                if bracket_content:
                    patterns_to_try[1] = bracket_content
            
            # å¦‚æœæ ‡é¢˜åŒ…å«æ•°å­—ç¼–å·ï¼Œå°è¯•æå–ä¸»è¦éƒ¨åˆ†
            if 'ã€' in clean_title:
                main_part = clean_title.split('ã€', 1)[-1].strip()
                if main_part:
                    patterns_to_try[2] = main_part
            
            # è¿‡æ»¤æ‰Noneå€¼
            patterns_to_try = [p for p in patterns_to_try if p]
            
            for pattern_text in patterns_to_try:
                # å°è¯•åŒ¹é…ä¸€çº§æ ‡é¢˜
                pattern_h1 = rf"(?m)^\s*#\s*.*{re.escape(pattern_text)}.*$([\s\S]*?)(?=^\s*#\s|\Z)"
                match = re.search(pattern_h1, document_content)
                
                if match:
                    content_without_title = (match.group(1) or '').strip()
                    self.logger.debug(f"æ‰¾åˆ°ä¸€çº§æ ‡é¢˜ç« èŠ‚: {pattern_text} (åŸæ ‡é¢˜: {section_title})")
                    return content_without_title
                
                # å°è¯•åŒ¹é…äºŒçº§æ ‡é¢˜
                pattern_h2 = rf"(?m)^\s*##\s*.*{re.escape(pattern_text)}.*$([\s\S]*?)(?=^\s*##\s|^\s*#\s|\Z)"
                match = re.search(pattern_h2, document_content)
                
                if match:
                    content_without_title = (match.group(1) or '').strip()
                    self.logger.debug(f"æ‰¾åˆ°äºŒçº§æ ‡é¢˜ç« èŠ‚: {pattern_text} (åŸæ ‡é¢˜: {section_title})")
                    return content_without_title
                
                # å°è¯•åŒ¹é…ä¸‰çº§æ ‡é¢˜
                pattern_h3 = rf"(?m)^\s*###\s*.*{re.escape(pattern_text)}.*$([\s\S]*?)(?=^\s*###\s|^\s*##\s|^\s*#\s|\Z)"
                match = re.search(pattern_h3, document_content)
                
                if match:
                    content_without_title = (match.group(1) or '').strip()
                    self.logger.debug(f"æ‰¾åˆ°ä¸‰çº§æ ‡é¢˜ç« èŠ‚: {pattern_text} (åŸæ ‡é¢˜: {section_title})")
                    return content_without_title
            
            self.logger.warning(f"æœªæ‰¾åˆ°ç« èŠ‚: {section_title}")
            return ""
        except Exception as e:
            self.logger.error(f"æå–å•ä¸€ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def _extract_child_section(self, parent_content: str, child_title: str) -> str:
        """
        åœ¨çˆ¶ç« èŠ‚å†…å®¹ä¸­æå–å­ç« èŠ‚
        """
        try:
            # åœ¨çˆ¶ç« èŠ‚å†…å®¹ä¸­æŸ¥æ‰¾å­ç« èŠ‚ï¼ˆäºŒçº§æ ‡é¢˜ï¼‰
            pattern_h2 = rf"(?m)^\s*##\s*{re.escape(child_title)}\s*$([\s\S]*?)(?=^\s*##\s|^\s*#\s|\Z)"
            match = re.search(pattern_h2, parent_content)
            
            if match:
                content_without_title = (match.group(1) or '').strip()
                self.logger.debug(f"åœ¨çˆ¶ç« èŠ‚ä¸­æ‰¾åˆ°äºŒçº§æ ‡é¢˜: {child_title}")
                return content_without_title
            
            # å¦‚æœæ²¡æ‰¾åˆ°äºŒçº§æ ‡é¢˜ï¼Œå°è¯•åŒ¹é…ä¸‰çº§æ ‡é¢˜
            pattern_h3 = rf"(?m)^\s*###\s*{re.escape(child_title)}\s*$([\s\S]*?)(?=^\s*###\s|^\s*##\s|^\s*#\s|\Z)"
            match = re.search(pattern_h3, parent_content)
            
            if match:
                content_without_title = (match.group(1) or '').strip()
                self.logger.debug(f"åœ¨çˆ¶ç« èŠ‚ä¸­æ‰¾åˆ°ä¸‰çº§æ ‡é¢˜: {child_title}")
                return content_without_title
            else:
                self.logger.warning(f"åœ¨çˆ¶ç« èŠ‚ä¸­æœªæ‰¾åˆ°å­ç« èŠ‚: {child_title}")
                return ""
        except Exception as e:
            self.logger.error(f"æå–å­ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def regenerate_section_with_thesis(self, section_title: str, original_content: str, 
                                     consistency_issue: Dict, thesis_data: Dict) -> Dict[str, Any]:
        """
        åŸºäºè®ºç‚¹ä¸€è‡´æ€§é—®é¢˜é‡æ–°ç”Ÿæˆç« èŠ‚
        
        Args:
            section_title: ç« èŠ‚æ ‡é¢˜
            original_content: åŸå§‹ç« èŠ‚å†…å®¹
            consistency_issue: ä¸€è‡´æ€§é—®é¢˜ä¿¡æ¯
            thesis_data: æ ¸å¿ƒè®ºç‚¹æ•°æ®
            
        Returns:
            Dict[str, Any]: ç”Ÿæˆç»“æœ
        """
        self.logger.info(f"å¼€å§‹é‡æ–°ç”Ÿæˆç« èŠ‚: {section_title}")
        
        # æ„å»ºåŸºäºè®ºç‚¹ä¸€è‡´æ€§çš„ä¿®æ­£æç¤ºè¯
        prompt = self._build_thesis_correction_prompt(
            section_title, original_content, consistency_issue, thesis_data
        )
        
        try:
            import time
            start_time = time.time()
            
            # ä½¿ç”¨çº¿ç¨‹æœ¬åœ°å®¢æˆ·ç«¯è°ƒç”¨APIè¿›è¡Œä¿®æ­£
            client = self._get_client()
            completion = client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": config.openrouter_http_referer,
                    "X-Title": config.openrouter_x_title,
                },
                extra_body={},
                model=config.openrouter_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=config.content_correction_temperature,
                max_tokens=config.max_tokens
            )
            
            response_content = completion.choices[0].message.content
            content = response_content.strip()
            
            # æ¸…æ´—å†…å®¹ï¼Œç§»é™¤å›¾ç‰‡/è¡¨æ ¼/åª’ä½“ç›¸å…³å†…å®¹
            content = self._sanitize_content_remove_media(content)
            
            generation_time = time.time() - start_time
            
            result = {
                'content': content,
                'quality_score': 0.9,  # åŸºäºè®ºç‚¹ä¿®æ­£ï¼Œè´¨é‡è¾ƒé«˜
                'word_count': len(content),
                'generation_time': f"{generation_time:.2f}s",
                'feedback': f'å·²æ ¹æ®è®ºç‚¹ä¸€è‡´æ€§é—®é¢˜è¿›è¡Œä¿®æ­£: {consistency_issue.get("issue_type", "")}',
                'subtitle': section_title,
                'original_issue': consistency_issue,
                'thesis_alignment': 'improved'
            }
            
            self.logger.info(f"ç« èŠ‚ä¿®æ­£å®Œæˆ: {section_title} ({result['word_count']}å­—)")
            return result
            
        except Exception as e:
            self.logger.error(f"é‡æ–°ç”Ÿæˆç« èŠ‚å¤±è´¥: {e}")
            return {
                'content': f"[ä¿®æ­£å¤±è´¥: {str(e)}]",
                'quality_score': 0.0,
                'word_count': 0,
                'generation_time': "0.00s",
                'feedback': f"ä¿®æ­£å¤±è´¥: {str(e)}",
                'subtitle': section_title,
                'original_issue': consistency_issue,
                'thesis_alignment': 'failed'
            }
    
    def _build_thesis_correction_prompt(self, section_title: str, original_content: str, 
                                      consistency_issue: Dict, thesis_data: Dict) -> str:
        """
        æ„å»ºåŸºäºè®ºç‚¹ä¸€è‡´æ€§çš„ä¿®æ­£æç¤ºè¯
        """
        main_thesis = thesis_data.get('main_thesis', '')
        supporting_arguments = thesis_data.get('supporting_arguments', [])
        key_concepts = thesis_data.get('key_concepts', [])
        
        issue_type = consistency_issue.get('issue_type', '')
        issue_description = consistency_issue.get('description', '')
        suggestion = consistency_issue.get('suggestion', '')
        
        issue_type_guidance = {
            "contradiction": "æ¶ˆé™¤ä¸æ ¸å¿ƒè®ºç‚¹çš„ç›´æ¥å†²çªï¼Œè°ƒæ•´è®ºè¿°æ–¹å‘ä½¿å…¶æ”¯æŒæ ¸å¿ƒè®ºç‚¹",
            "irrelevant": "å¢å¼ºä¸æ ¸å¿ƒè®ºç‚¹çš„å…³è”ï¼Œæ˜ç¡®æœ¬ç« èŠ‚å¦‚ä½•æœåŠ¡äºæ ¸å¿ƒè®ºç‚¹",
            "weak_support": "åŠ å¼ºè®ºæ®å’Œé€»è¾‘é“¾æ¡ï¼Œæä¾›æ›´æœ‰åŠ›çš„æ”¯æ’‘è¯æ®",
            "unclear": "æ˜ç¡®è¡¨è¾¾æœ¬ç« èŠ‚ä¸æ ¸å¿ƒè®ºç‚¹çš„å…³ç³»ï¼Œæ¸…æ™°é˜è¿°æ”¯æ’‘ä½œç”¨"
        }
        
        guidance = issue_type_guidance.get(issue_type, "ç¡®ä¿å†…å®¹ä¸æ ¸å¿ƒè®ºç‚¹ä¿æŒä¸€è‡´")
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¼–è¾‘å’Œé€»è¾‘ä¸“å®¶ï¼Œè¯·æ ¹æ®æ ¸å¿ƒè®ºç‚¹ä¿®æ­£ä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œç¡®ä¿å…¶ä¸æ ¸å¿ƒè®ºç‚¹ä¿æŒé€»è¾‘ä¸€è‡´ã€‚

ã€æ ¸å¿ƒè®ºç‚¹ä¿¡æ¯ã€‘
ä¸»è¦è®ºç‚¹: {main_thesis}
æ”¯æ’‘è®ºæ®: {', '.join(supporting_arguments)}
å…³é”®æ¦‚å¿µ: {', '.join(key_concepts)}

ã€ç« èŠ‚æ ‡é¢˜ã€‘: {section_title}

ã€åŸå§‹å†…å®¹ã€‘:
{original_content}

ã€å‘ç°çš„ä¸€è‡´æ€§é—®é¢˜ã€‘:
é—®é¢˜ç±»å‹: {issue_type}
é—®é¢˜æè¿°: {issue_description}
ä¿®æ­£å»ºè®®: {suggestion}

ã€ä¿®æ­£æŒ‡å¯¼åŸåˆ™ã€‘:
{guidance}

ã€ä¿®æ­£è¦æ±‚ã€‘:
1. ç¡®ä¿ä¿®æ­£åçš„å†…å®¹ä¸æ ¸å¿ƒè®ºç‚¹"{main_thesis}"ä¿æŒé«˜åº¦ä¸€è‡´
2. åœ¨ä¿®æ­£è¿‡ç¨‹ä¸­ä½“ç°ä»¥ä¸‹æ”¯æ’‘è®ºæ®: {', '.join(supporting_arguments[:3])}
3. é€‚å½“èå…¥å…³é”®æ¦‚å¿µ: {', '.join(key_concepts[:3])}
4. ä¿æŒä¸“ä¸šã€å®¢è§‚ã€ä¸¥è°¨çš„å­¦æœ¯å†™ä½œé£æ ¼
5. ç¡®ä¿é€»è¾‘æ¸…æ™°ã€è®ºè¯æœ‰åŠ›ã€ç»“æ„åˆç†
6. ä»…è¾“å‡ºä¿®æ­£åçš„æ­£æ–‡å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡é¢˜ã€å›¾ç‰‡ã€è¡¨æ ¼æˆ–åª’ä½“ç›¸å…³ä¿¡æ¯
7. å­—æ•°å»ºè®®æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´ï¼Œæ®µè½ä¹‹é—´ç”¨ä¸€ä¸ªç©ºè¡Œåˆ†éš”

è¯·ç›´æ¥è¾“å‡ºä¿®æ­£åçš„ç« èŠ‚æ­£æ–‡å†…å®¹ï¼Œç¡®ä¿å…¶å®Œå…¨æœåŠ¡äºæ ¸å¿ƒè®ºç‚¹ï¼š"""
        
        return prompt
    
    def _regenerate_section_worker(self, section_data: Tuple[str, str, Dict, Dict]) -> Tuple[str, Dict[str, Any]]:
        """
        å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œçš„ç« èŠ‚é‡æ–°ç”Ÿæˆä»»åŠ¡
        
        Args:
            section_data: (section_title, original_content, consistency_issue, thesis_data)
            
        Returns:
            Tuple[str, Dict[str, Any]]: (section_title, result)
        """
        section_title, original_content, consistency_issue, thesis_data = section_data
        
        try:
            thread_id = threading.current_thread().ident
            self.logger.info(f"ğŸ“ [çº¿ç¨‹-{thread_id}] å¼€å§‹å¤„ç†ç« èŠ‚: {section_title}")
            
            result = self.regenerate_section_with_thesis(
                section_title, original_content, consistency_issue, thesis_data
            )
            
            # æ›´æ–°è¿›åº¦
            success = result.get('thesis_alignment') != 'failed'
            self._update_progress(completed=success, failed=not success)
            
            self.logger.info(f"âœ… [çº¿ç¨‹-{thread_id}] ç« èŠ‚å¤„ç†å®Œæˆ: {section_title}")
            return section_title, result
            
        except Exception as e:
            self.logger.error(f"âŒ [çº¿ç¨‹-{thread_id}] ç« èŠ‚å¤„ç†å¤±è´¥: {section_title} - {e}")
            self._update_progress(completed=False, failed=True)
            
            # è¿”å›é”™è¯¯ç»“æœ
            error_result = {
                'content': f"[å¹¶è¡Œå¤„ç†å¤±è´¥: {str(e)}]",
                'quality_score': 0.0,
                'word_count': 0,
                'generation_time': "0.00s",
                'feedback': f"å¹¶è¡Œå¤„ç†å¤±è´¥: {str(e)}",
                'subtitle': section_title,
                'original_issue': consistency_issue,
                'thesis_alignment': 'failed'
            }
            return section_title, error_result
    
    def regenerate_sections_parallel(self, sections_data: List[Tuple[str, str, Dict, Dict]]) -> Dict[str, Dict[str, Any]]:
        """
        å¹¶è¡Œé‡æ–°ç”Ÿæˆå¤šä¸ªç« èŠ‚
        
        Args:
            sections_data: ç« èŠ‚æ•°æ®åˆ—è¡¨ [(section_title, original_content, consistency_issue, thesis_data), ...]
            
        Returns:
            Dict[str, Dict[str, Any]]: é‡æ–°ç”Ÿæˆçš„ç« èŠ‚ç»“æœ
        """
        if not sections_data:
            return {}
        
        # åˆå§‹åŒ–è¿›åº¦
        with self._lock:
            self._progress['total_sections'] = len(sections_data)
            self._progress['completed_sections'] = 0
            self._progress['failed_sections'] = 0
        
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œé‡æ–°ç”Ÿæˆ {len(sections_data)} ä¸ªç« èŠ‚ï¼ˆæœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}ï¼‰")
        
        regenerated_sections = {}
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_section = {
                executor.submit(self._regenerate_section_worker, section_data): section_data[0]
                for section_data in sections_data
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_section):
                section_title = future_to_section[future]
                try:
                    result_section_title, result = future.result()
                    regenerated_sections[result_section_title] = result
                except Exception as e:
                    self.logger.error(f"âŒ çº¿ç¨‹æ± ä»»åŠ¡å¼‚å¸¸: {section_title} - {e}")
                    # æ·»åŠ é”™è¯¯ç»“æœ
                    regenerated_sections[section_title] = {
                        'content': f"[çº¿ç¨‹æ± å¼‚å¸¸: {str(e)}]",
                        'quality_score': 0.0,
                        'word_count': 0,
                        'generation_time': "0.00s",
                        'feedback': f"çº¿ç¨‹æ± å¼‚å¸¸: {str(e)}",
                        'subtitle': section_title,
                        'thesis_alignment': 'failed'
                    }
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        with self._lock:
            total = self._progress['total_sections']
            completed = self._progress['completed_sections']
            failed = self._progress['failed_sections']
            
            self.logger.info(
                f"âœ… å¹¶è¡Œé‡æ–°ç”Ÿæˆå®Œæˆ: æ€»è®¡ {total} ä¸ªç« èŠ‚, "
                f"æˆåŠŸ {completed} ä¸ª, å¤±è´¥ {failed} ä¸ª"
            )
        
        return regenerated_sections
    
    def _sanitize_content_remove_media(self, content: str) -> str:
        """
        æ¸…æ´—æ¨¡å‹è¾“å‡ºï¼Œç§»é™¤å›¾ç‰‡/è¡¨æ ¼/åª’ä½“ç›¸å…³æ®µè½ä¸Markdownæ ‡è®°
        """
        import re

        if not content:
            return content

        cleaned_lines: list[str] = []
        for line in content.splitlines():
            stripped = line.strip()

            if not stripped:
                cleaned_lines.append(line)
                continue

            # æ ‡é¢˜è¡Œ
            if stripped.startswith('#'):
                continue

            # è¡¨æ ¼ç›¸å…³
            if stripped.startswith('### ç›¸å…³è¡¨æ ¼èµ„æ–™') or stripped.startswith('|'):
                continue

            # å›¾ç‰‡ç›¸å…³
            if (stripped.startswith('### ç›¸å…³å›¾ç‰‡èµ„æ–™') or 
                stripped == 'ç›¸å…³å›¾ç‰‡èµ„æ–™' or 
                stripped.startswith('ç›¸å…³å›¾ç‰‡èµ„æ–™') or
                stripped.startswith('å›¾ç‰‡æè¿°:') or 
                stripped.startswith('å›¾ç‰‡æ¥æº:')):
                continue

            # Markdown å›¾ç‰‡æˆ–é“¾æ¥
            if (re.search(r'!\[.*?\]\(.*?\)', stripped) or 
                re.search(r'\[[^\]]+\]\(https?://[^\)]+\)', stripped, flags=re.IGNORECASE)):
                continue

            cleaned_lines.append(line)

        # åˆå¹¶å¹¶å»é™¤å¤šä½™ç©ºè¡Œ
        cleaned_text = '\n'.join(cleaned_lines)
        cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text).strip()
        return cleaned_text
    
    def regenerate_complete_document(self, analysis_file: str, document_file: str, 
                                   output_dir: str = None) -> Dict[str, Any]:
        """
        é‡æ–°ç”Ÿæˆå®Œæ•´æ–‡æ¡£
        
        Args:
            analysis_file: ä¸€è‡´æ€§åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
            document_file: åŸå§‹æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, Any]: é‡æ–°ç”Ÿæˆçš„ç»“æœ
        """
        # åŠ è½½ä¸€è‡´æ€§åˆ†æç»“æœå’ŒåŸå§‹æ–‡æ¡£
        consistency_data, thesis_data = self.load_consistency_analysis(analysis_file)
        if not consistency_data:
            return {'error': 'æ— æ³•åŠ è½½ä¸€è‡´æ€§åˆ†æç»“æœ'}
        
        document_content, json_data = self.load_original_document(document_file)
        if not document_content:
            return {'error': 'æ— æ³•åŠ è½½åŸå§‹æ–‡æ¡£'}
        
        consistency_issues = consistency_data.get('consistency_issues', [])
        
        if not consistency_issues:
            self.logger.info("æ²¡æœ‰å‘ç°éœ€è¦ä¿®æ­£çš„ä¸€è‡´æ€§é—®é¢˜")
            return {'message': 'æ–‡æ¡£è®ºç‚¹ä¸€è‡´æ€§è‰¯å¥½ï¼Œæ— éœ€ä¿®æ­£'}
        
        # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„æ•°æ®
        sections_data = []
        
        for issue in consistency_issues:
            section_title = issue.get('section_title', '')
            
            if not section_title:
                continue
            
            # æå–åŸå§‹ç« èŠ‚å†…å®¹
            original_content = ""
            if json_data:
                # ä»JSONç»“æ„ä¸­æå–
                try:
                    for part in json_data.get('report_guide', []):
                        for sec in part.get('sections', []):
                            if sec.get('subtitle', '').strip() == section_title.strip():
                                original_content = (sec.get('generated_content') or '').strip()
                                break
                except:
                    pass
            
            if not original_content:
                # ä»Markdownå†…å®¹ä¸­æå–
                original_content = self.extract_section_content(document_content, section_title)
            
            if original_content:
                # æ·»åŠ åˆ°å¹¶è¡Œå¤„ç†åˆ—è¡¨
                sections_data.append((section_title, original_content, issue, thesis_data))
        
        # ä½¿ç”¨å¹¶è¡Œå¤„ç†é‡æ–°ç”Ÿæˆç« èŠ‚
        regenerated_sections = self.regenerate_sections_parallel(sections_data)
        
        # ç”Ÿæˆå®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£
        complete_document = self._generate_complete_document(
            document_content, json_data, regenerated_sections, thesis_data
        )
        
        # ä¿å­˜ç»“æœ
        if output_dir:
            saved_files = self._save_regeneration_results(
                regenerated_sections, complete_document, thesis_data, output_dir
            )
            return {
                'regenerated_sections': regenerated_sections,
                'complete_document': complete_document,
                'saved_files': saved_files,
                'sections_count': len(regenerated_sections)
            }
        
        return {
            'regenerated_sections': regenerated_sections,
            'complete_document': complete_document,
            'sections_count': len(regenerated_sections)
        }
    
    def _generate_complete_document(self, original_content: str, json_data: Dict, 
                                  regenerated_sections: Dict, thesis_data: Dict) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£
        
        Args:
            original_content: åŸå§‹æ–‡æ¡£å†…å®¹
            json_data: åŸå§‹JSONæ•°æ®
            regenerated_sections: é‡æ–°ç”Ÿæˆçš„ç« èŠ‚
            thesis_data: æ ¸å¿ƒè®ºç‚¹æ•°æ®
            
        Returns:
            str: å®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£
        """
        self.logger.info("å¼€å§‹ç”Ÿæˆå®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£")
        
        # å¦‚æœæœ‰JSONæ•°æ®ï¼ŒåŸºäºJSONç»“æ„ç”Ÿæˆ
        if json_data and 'report_guide' in json_data:
            return self._generate_from_json_structure(json_data, regenerated_sections, thesis_data)
        else:
            # åŸºäºMarkdownå†…å®¹ç”Ÿæˆ
            return self._generate_from_markdown_content(original_content, regenerated_sections, thesis_data)
    
    def _generate_from_json_structure(self, json_data: Dict, regenerated_sections: Dict, 
                                    thesis_data: Dict) -> str:
        """
        åŸºäºJSONç»“æ„ç”Ÿæˆå®Œæ•´æ–‡æ¡£
        """
        document_lines = []
        
        # æ·»åŠ æ–‡æ¡£æ ‡é¢˜å’Œè®ºç‚¹è¯´æ˜
        if json_data.get('title'):
            document_lines.append(f"# {json_data['title']}")
            document_lines.append("")
        
        # æ·»åŠ æ ¸å¿ƒè®ºç‚¹è¯´æ˜
        main_thesis = thesis_data.get('main_thesis', '')
        if main_thesis:
            document_lines.append("## ğŸ“‹ æ ¸å¿ƒè®ºç‚¹")
            document_lines.append(f"**æœ¬æ–‡æ¡£çš„æ ¸å¿ƒè®ºç‚¹**: {main_thesis}")
            document_lines.append("")
            document_lines.append("*ä»¥ä¸‹å„ç« èŠ‚å†…å®¹å‡å›´ç»•æ­¤æ ¸å¿ƒè®ºç‚¹å±•å¼€ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ã€‚*")
            document_lines.append("")
        
        # éå†JSONç»“æ„ç”Ÿæˆå†…å®¹
        report_guide = json_data.get('report_guide', [])
        
        for part in report_guide:
            part_title = part.get('title', '')
            if part_title:
                document_lines.append(f"# {part_title}")
                document_lines.append("")
            
            sections = part.get('sections', [])
            for section in sections:
                subtitle = section.get('subtitle', '')
                
                if subtitle:
                    document_lines.append(f"## {subtitle}")
                    document_lines.append("")
                    
                    # ä½¿ç”¨ä¿®æ­£åçš„å†…å®¹æˆ–åŸå§‹å†…å®¹
                    if subtitle in regenerated_sections:
                        content = regenerated_sections[subtitle]['content']
                        document_lines.append("*[æœ¬ç« èŠ‚å·²æ ¹æ®è®ºç‚¹ä¸€è‡´æ€§è¦æ±‚è¿›è¡Œä¿®æ­£]*")
                        document_lines.append("")
                    else:
                        content = section.get('generated_content', '')
                    
                    if content:
                        document_lines.append(content)
                        document_lines.append("")
                    
                    # æ·»åŠ å›¾ç‰‡å’Œè¡¨æ ¼ï¼ˆå¦‚æœæœ‰ï¼‰
                    if section.get('retrieved_image'):
                        document_lines.append("### ç›¸å…³å›¾ç‰‡èµ„æ–™")
                        document_lines.append("")
                        for img in section['retrieved_image']:
                            if isinstance(img, dict):
                                desc = img.get('description', '')
                                url = img.get('url', '')
                                if desc and url:
                                    document_lines.append(f"![{desc}]({url})")
                        document_lines.append("")
                    
                    if section.get('retrieved_table'):
                        document_lines.append("### ç›¸å…³è¡¨æ ¼èµ„æ–™")
                        document_lines.append("")
                        for table in section['retrieved_table']:
                            if isinstance(table, str):
                                document_lines.append(table)
                        document_lines.append("")
        
        return "\n".join(document_lines)
    
    def _generate_from_markdown_content(self, original_content: str, regenerated_sections: Dict, 
                                      thesis_data: Dict) -> str:
        """
        åŸºäºMarkdownå†…å®¹ç”Ÿæˆå®Œæ•´æ–‡æ¡£
        """
        lines = original_content.split('\n')
        new_lines = []
        current_section = None
        in_section_content = False
        
        # æ·»åŠ æ ¸å¿ƒè®ºç‚¹è¯´æ˜
        main_thesis = thesis_data.get('main_thesis', '')
        if main_thesis:
            new_lines.extend([
                "## ğŸ“‹ æ ¸å¿ƒè®ºç‚¹",
                f"**æœ¬æ–‡æ¡£çš„æ ¸å¿ƒè®ºç‚¹**: {main_thesis}",
                "",
                "*ä»¥ä¸‹å„ç« èŠ‚å†…å®¹å‡å›´ç»•æ­¤æ ¸å¿ƒè®ºç‚¹å±•å¼€ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ã€‚*",
                "",
            ])
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜ï¼ˆä¸€çº§ã€äºŒçº§ã€ä¸‰çº§ï¼‰
            if line.startswith('### '):
                # ä¸‰çº§æ ‡é¢˜å¤„ç†
                if current_section and current_section in regenerated_sections:
                    # æ·»åŠ ä¿®æ­£åçš„å†…å®¹
                    new_lines.append("*[æœ¬ç« èŠ‚å·²æ ¹æ®è®ºç‚¹ä¸€è‡´æ€§è¦æ±‚è¿›è¡Œä¿®æ­£]*")
                    new_lines.append("")
                    new_lines.append(regenerated_sections[current_section]['content'])
                    new_lines.append("")
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_section = line[4:].strip()
                new_lines.append(line)
                new_lines.append("")
                in_section_content = True
                
                # å¦‚æœè¿™ä¸ªç« èŠ‚éœ€è¦ä¿®æ­£ï¼Œè·³è¿‡åŸå§‹å†…å®¹
                if current_section in regenerated_sections:
                    continue
                    
            elif line.startswith('## '):
                # äºŒçº§æ ‡é¢˜å¤„ç†
                if current_section and current_section in regenerated_sections:
                    # æ·»åŠ ä¿®æ­£åçš„å†…å®¹
                    new_lines.append("*[æœ¬ç« èŠ‚å·²æ ¹æ®è®ºç‚¹ä¸€è‡´æ€§è¦æ±‚è¿›è¡Œä¿®æ­£]*")
                    new_lines.append("")
                    new_lines.append(regenerated_sections[current_section]['content'])
                    new_lines.append("")
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_section = line[3:].strip()
                new_lines.append(line)
                new_lines.append("")
                in_section_content = True
                
                # å¦‚æœè¿™ä¸ªç« èŠ‚éœ€è¦ä¿®æ­£ï¼Œè·³è¿‡åŸå§‹å†…å®¹
                if current_section in regenerated_sections:
                    continue
            
            elif line.startswith('# ') or (current_section and current_section in regenerated_sections and in_section_content):
                # ä¸€çº§æ ‡é¢˜æˆ–éœ€è¦ä¿®æ­£çš„ç« èŠ‚å†…å®¹ï¼Œç›´æ¥è·³è¿‡åŸå§‹å†…å®¹
                if line.startswith('# '):
                    in_section_content = False
                    new_lines.append(line)
                continue
            else:
                # å…¶ä»–å†…å®¹ç›´æ¥æ·»åŠ 
                new_lines.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and current_section in regenerated_sections:
            new_lines.append("*[æœ¬ç« èŠ‚å·²æ ¹æ®è®ºç‚¹ä¸€è‡´æ€§è¦æ±‚è¿›è¡Œä¿®æ­£]*")
            new_lines.append("")
            new_lines.append(regenerated_sections[current_section]['content'])
        
        return "\n".join(new_lines)
    
    def _save_regeneration_results(self, regenerated_sections: Dict, complete_document: str, 
                                 thesis_data: Dict, output_dir: str) -> Dict[str, str]:
        """
        ä¿å­˜é‡æ–°ç”Ÿæˆçš„ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼Œåªä¿å­˜å®Œæ•´æ–‡æ¡£ï¼‰
        
        Returns:
            Dict[str, str]: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            saved_files = {}
            
            # åªä¿å­˜å®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£
            complete_doc_file = os.path.join(output_dir, f"thesis_corrected_complete_document_{timestamp}.md")
            with open(complete_doc_file, 'w', encoding='utf-8') as f:
                f.write(complete_document)
            saved_files['complete_document'] = complete_doc_file
            
            self.logger.info(f"å®Œæ•´ä¿®æ­£åæ–‡æ¡£å·²ä¿å­˜åˆ°: {complete_doc_file}")
            return saved_files
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return {}
    
    def _generate_correction_summary(self, regenerated_sections: Dict, thesis_data: Dict) -> str:
        """
        ç”Ÿæˆä¿®æ­£æ‘˜è¦æŠ¥å‘Š
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        main_thesis = thesis_data.get('main_thesis', '')
        
        lines = [
            "# è®ºç‚¹ä¸€è‡´æ€§ä¿®æ­£æ‘˜è¦æŠ¥å‘Š",
            "",
            f"**ä¿®æ­£æ—¶é—´**: {timestamp}",
            f"**æ ¸å¿ƒè®ºç‚¹**: {main_thesis}",
            f"**ä¿®æ­£ç« èŠ‚æ•°**: {len(regenerated_sections)}",
            "",
            "## ğŸ“Š ä¿®æ­£ç»Ÿè®¡",
            ""
        ]
        
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        issue_types = {}
        total_words = 0
        
        for section_title, result in regenerated_sections.items():
            issue = result.get('original_issue', {})
            issue_type = issue.get('issue_type', 'unknown')
            
            if issue_type not in issue_types:
                issue_types[issue_type] = 0
            issue_types[issue_type] += 1
            
            total_words += result.get('word_count', 0)
        
        lines.extend([
            f"**ä¿®æ­£é—®é¢˜ç±»å‹åˆ†å¸ƒ**:",
            ""
        ])
        
        for issue_type, count in issue_types.items():
            type_names = {
                'contradiction': 'ç›´æ¥çŸ›ç›¾',
                'irrelevant': 'åç¦»ä¸»é¢˜', 
                'weak_support': 'è®ºè¯è–„å¼±',
                'unclear': 'è¡¨è¿°ä¸æ¸…'
            }
            type_name = type_names.get(issue_type, issue_type)
            lines.append(f"- {type_name}: {count} ä¸ªç« èŠ‚")
        
        lines.extend([
            "",
            f"**æ€»ä¿®æ­£å­—æ•°**: {total_words:,} å­—",
            "",
            "## ğŸ“‹ ä¿®æ­£ç« èŠ‚åˆ—è¡¨",
            ""
        ])
        
        for i, (section_title, result) in enumerate(regenerated_sections.items(), 1):
            issue = result.get('original_issue', {})
            lines.extend([
                f"### {i}. {section_title}",
                f"**åŸå§‹é—®é¢˜**: {issue.get('issue_type', '')} ({issue.get('severity', '')})",
                f"**ä¿®æ­£è´¨é‡**: {result.get('quality_score', 0):.2f}",
                f"**ä¿®æ­£å­—æ•°**: {result.get('word_count', 0)} å­—",
                f"**è®ºç‚¹å¯¹é½**: {result.get('thesis_alignment', 'æœªçŸ¥')}",
                ""
            ])
        
        lines.extend([
            "## ğŸ’¡ ä¿®æ­£æ•ˆæœ",
            "",
            "é€šè¿‡æœ¬æ¬¡è®ºç‚¹ä¸€è‡´æ€§ä¿®æ­£ï¼Œæ–‡æ¡£çš„ä»¥ä¸‹æ–¹é¢å¾—åˆ°äº†æ”¹å–„ï¼š",
            "",
            "1. **é€»è¾‘ä¸€è‡´æ€§**: æ‰€æœ‰ç« èŠ‚ç°åœ¨éƒ½å›´ç»•æ ¸å¿ƒè®ºç‚¹å±•å¼€",
            "2. **è®ºè¯å¼ºåº¦**: æ¶ˆé™¤äº†ä¸æ ¸å¿ƒè®ºç‚¹çŸ›ç›¾çš„å†…å®¹",
            "3. **ä¸»é¢˜èšç„¦**: å‡å°‘äº†åç¦»ä¸»é¢˜çš„è®ºè¿°",
            "4. **è¡¨è¾¾æ¸…æ™°**: æ˜ç¡®äº†å„ç« èŠ‚ä¸æ ¸å¿ƒè®ºç‚¹çš„å…³ç³»",
            "",
            "## ğŸ“ è¾“å‡ºæ–‡ä»¶",
            "",
            "- `thesis_corrected_sections_*.json` - ä¿®æ­£åç« èŠ‚è¯¦ç»†æ•°æ®",
            "- `thesis_corrected_sections_*.md` - ä¿®æ­£åç« èŠ‚å¯è¯»æ ¼å¼", 
            "- `thesis_corrected_complete_document_*.md` - å®Œæ•´ä¿®æ­£åæ–‡æ¡£",
            "- `thesis_correction_summary_*.md` - æœ¬æ‘˜è¦æŠ¥å‘Š",
            "",
            "---",
            "*æœ¬æŠ¥å‘Šç”±Gauzè®ºç‚¹ä¸€è‡´æ€§Agentè‡ªåŠ¨ç”Ÿæˆ*"
        ])
        
        return "\n".join(lines)


def main():
    """
    ä¸»å‡½æ•°
    """
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('thesis_document_regeneration.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python document_regenerator.py <ä¸€è‡´æ€§åˆ†ææ–‡ä»¶> <åŸå§‹æ–‡æ¡£æ–‡ä»¶> [è¾“å‡ºç›®å½•] [æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°]")
        print("ç¤ºä¾‹: python document_regenerator.py consistency_analysis_document.json document.json ./outputs 5")
        return
    
    analysis_file = sys.argv[1]
    document_file = sys.argv[2]
    output_dir = sys.argv[3] if len(sys.argv) > 3 else "./thesis_regenerated_outputs"
    max_workers = int(sys.argv[4]) if len(sys.argv) > 4 else 5
    
    print(f"ğŸ“‹ ä¸€è‡´æ€§åˆ†ææ–‡ä»¶: {analysis_file}")
    print(f"ğŸ“„ åŸå§‹æ–‡æ¡£æ–‡ä»¶: {document_file}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ’» æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
    print()
    
    # åˆ›å»ºé‡æ–°ç”Ÿæˆå™¨ï¼ˆæ”¯æŒå¤šçº¿ç¨‹ï¼‰
    regenerator = ThesisDocumentRegenerator(max_workers=max_workers)
    
    # æ‰§è¡Œé‡æ–°ç”Ÿæˆ
    print(f"ğŸš€ å¼€å§‹åŸºäºè®ºç‚¹ä¸€è‡´æ€§é‡æ–°ç”Ÿæˆæ–‡æ¡£ï¼ˆå¹¶è¡Œå¤„ç†ï¼Œæœ€å¤§{max_workers}ä¸ªçº¿ç¨‹ï¼‰...")
    results = regenerator.regenerate_complete_document(
        analysis_file=analysis_file,
        document_file=document_file,
        output_dir=output_dir
    )
    
    if 'error' in results:
        print(f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥: {results['error']}")
        return
    
    if 'message' in results:
        print(f"â„¹ï¸ {results['message']}")
        return
    
    print(f"\nâœ… æ–‡æ¡£é‡æ–°ç”Ÿæˆå®Œæˆï¼")
    print(f"   ä¿®æ­£ç« èŠ‚æ•°: {results['sections_count']}")
    
    if 'saved_files' in results:
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for file_type, file_path in results['saved_files'].items():
            print(f"   {file_type}: {file_path}")
    
    print(f"\nğŸ‰ å®Œæ•´çš„ä¿®æ­£åæ–‡æ¡£å·²ç”Ÿæˆï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹å›´ç»•æ ¸å¿ƒè®ºç‚¹å±•å¼€ï¼")


if __name__ == "__main__":
    main()
