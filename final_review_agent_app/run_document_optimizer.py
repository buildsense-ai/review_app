#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ä¼˜åŒ–å™¨ä¸»ç¨‹åº - æ–°çš„å·¥ä½œæµç¨‹

å®ç°ä¸¤æ­¥å¼æ–‡æ¡£ä¼˜åŒ–ï¼š
1. å…¨å±€åˆ†æï¼šæ¥æ”¶å®Œæ•´Markdownæ–‡æ¡£ï¼Œè¾“å‡ºåˆ†æJSON
2. æ–‡æ¡£ä¿®æ”¹ï¼šæ¥æ”¶åŸæ–‡æ¡£å’Œåˆ†æJSONï¼Œè¾“å‡ºä¼˜åŒ–åçš„Markdown
"""

import json
import logging
import sys
import os
import re
from typing import Optional, Dict, Any, List
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # å¦‚æœæ²¡æœ‰å®‰è£…python-dotenvï¼Œç»§ç»­è¿è¡Œ

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from document_reviewer import DocumentReviewer
from document_modifier import DocumentModifier


class DocumentOptimizer:
    """æ–‡æ¡£ä¼˜åŒ–å™¨ - æ•´åˆåˆ†æå’Œä¿®æ”¹åŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æ¡£ä¼˜åŒ–å™¨"""
        # å…ˆè®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('document_optimization.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.reviewer = DocumentReviewer()
        self.modifier = DocumentModifier()
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ç”¨äºåˆ†ç« èŠ‚ç”Ÿæˆ
        api_key = os.getenv("OPENROUTER_API_KEY")
        if api_key:
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://openrouter.ai/api/v1"
            )
        else:
            self.client = None
            self.logger.warning("æœªè®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡ï¼Œåˆ†ç« èŠ‚ç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨")
        self.max_workers = 5
    
    def step1_analyze_document(self, markdown_file_path: str, output_dir: str = "./test_results") -> str:
        """
        æ­¥éª¤1ï¼šå…¨å±€åˆ†ææ–‡æ¡£
        
        Args:
            markdown_file_path: Markdownæ–‡æ¡£è·¯å¾„
            output_dir: åˆ†æç»“æœè¾“å‡ºç›®å½•
            
        Returns:
            str: åˆ†æç»“æœJSONæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ğŸ” æ­¥éª¤1ï¼šå¼€å§‹å…¨å±€åˆ†ææ–‡æ¡£ - {markdown_file_path}")
        
        try:
            # è¯»å–æ–‡æ¡£å†…å®¹
            with open(markdown_file_path, 'r', encoding='utf-8') as f:
                document_content = f.read()
            
            # è·å–æ–‡æ¡£æ ‡é¢˜
            document_title = os.path.basename(markdown_file_path)
            
            # æ‰§è¡Œå…¨å±€åˆ†æ
            analysis_result = self.reviewer.analyze_document_global(document_content, document_title)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_title = re.sub(r'[^\w\s-]', '', document_title).strip()
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            analysis_file_path = os.path.join(output_dir, f"analysis_{safe_title}_{timestamp}.json")
            
            # ä¿å­˜åˆ†æç»“æœ
            with open(analysis_file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # æ˜¾ç¤ºåˆ†ææ‘˜è¦
            issues_found = analysis_result.get('issues_found', 0)
            
            self.logger.info(f"âœ… æ­¥éª¤1å®Œæˆ - åˆ†æç»“æœå·²ä¿å­˜: {analysis_file_path}")
            self.logger.info(f"ğŸ“Š åˆ†ææ‘˜è¦: å‘ç° {issues_found} ä¸ªé—®é¢˜")
            
            return analysis_file_path
            
        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤1å¤±è´¥: {e}")
            raise
    
    def step2_modify_document(self, markdown_file_path: str, analysis_file_path: str, 
                             output_dir: str = "./test_results") -> str:
        """
        æ­¥éª¤2ï¼šåŸºäºåˆ†æç»“æœä¿®æ”¹æ–‡æ¡£
        
        Args:
            markdown_file_path: åŸå§‹Markdownæ–‡æ¡£è·¯å¾„
            analysis_file_path: åˆ†æç»“æœJSONæ–‡ä»¶è·¯å¾„
            output_dir: ä¼˜åŒ–ç»“æœè¾“å‡ºç›®å½•
            
        Returns:
            str: ä¼˜åŒ–åçš„Markdownæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ğŸ”§ æ­¥éª¤2ï¼šå¼€å§‹ä¿®æ”¹æ–‡æ¡£")
        self.logger.info(f"   åŸå§‹æ–‡æ¡£: {markdown_file_path}")
        self.logger.info(f"   åˆ†æç»“æœ: {analysis_file_path}")
        
        try:
            # è¯»å–åŸå§‹æ–‡æ¡£
            with open(markdown_file_path, 'r', encoding='utf-8') as f:
                original_markdown = f.read()
            
            # è¯»å–åˆ†æç»“æœ
            with open(analysis_file_path, 'r', encoding='utf-8') as f:
                analysis_json = json.load(f)
            
            # æ‰§è¡Œæ–‡æ¡£ä¿®æ”¹
            modification_result = self.modifier.modify_document(original_markdown, analysis_json)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            document_title = analysis_json.get('document_title', 'unknown_document')
            safe_title = re.sub(r'[^\w\s-]', '', document_title).strip()
            safe_title = re.sub(r'[-\s]+', '_', safe_title)
            optimized_file_path = os.path.join(output_dir, f"optimized_{safe_title}_{timestamp}.md")
            
            # ä¿å­˜ä¼˜åŒ–åçš„æ–‡æ¡£
            with open(optimized_file_path, 'w', encoding='utf-8') as f:
                f.write(modification_result['modified_markdown'])
            
            # ä¿å­˜ä¿®æ”¹æŠ¥å‘Š
            report_file_path = optimized_file_path.replace('.md', '_report.json')
            report_data = {
                "original_document": markdown_file_path,
                "analysis_file": analysis_file_path,
                "optimized_document": optimized_file_path,
                "modification_timestamp": modification_result.get('modification_timestamp'),
                "sections_modified": modification_result.get('sections_modified'),
                "tables_optimized": modification_result.get('tables_optimized'),
                "modifications_applied": modification_result.get('modifications_applied'),
                "table_optimizations_applied": modification_result.get('table_optimizations_applied'),
                "overall_improvement": modification_result.get('overall_improvement')
            }
            
            with open(report_file_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            # æ˜¾ç¤ºä¿®æ”¹æ‘˜è¦
            sections_modified = modification_result.get('sections_modified', 0)
            tables_optimized = modification_result.get('tables_optimized', 0)
            overall_improvement = modification_result.get('overall_improvement', 'æ— æ”¹è¿›ä¿¡æ¯')
            
            self.logger.info(f"âœ… æ­¥éª¤2å®Œæˆ - ä¼˜åŒ–æ–‡æ¡£å·²ä¿å­˜: {optimized_file_path}")
            self.logger.info(f"ğŸ“ ä¿®æ”¹æ‘˜è¦: ä¿®æ”¹äº† {sections_modified} ä¸ªç« èŠ‚ï¼Œä¼˜åŒ–äº† {tables_optimized} ä¸ªè¡¨æ ¼")
            self.logger.info(f"ğŸ’¡ æ•´ä½“æ”¹è¿›: {overall_improvement}")
            self.logger.info(f"ğŸ“‹ ä¿®æ”¹æŠ¥å‘Š: {report_file_path}")
            
            return optimized_file_path
            
        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤2å¤±è´¥: {e}")
            raise
    
    def optimize_document_complete(self, markdown_file_path: str, 
                                  analysis_output_dir: str = "./test_results",
                                  optimized_output_dir: str = "./test_results") -> Dict[str, str]:
        """
        å®Œæ•´çš„æ–‡æ¡£ä¼˜åŒ–æµç¨‹ï¼ˆä¸¤æ­¥åˆä¸€ï¼‰
        
        Args:
            markdown_file_path: Markdownæ–‡æ¡£è·¯å¾„
            analysis_output_dir: åˆ†æç»“æœè¾“å‡ºç›®å½•
            optimized_output_dir: ä¼˜åŒ–ç»“æœè¾“å‡ºç›®å½•
            
        Returns:
            Dict[str, str]: åŒ…å«åˆ†ææ–‡ä»¶å’Œä¼˜åŒ–æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        self.logger.info(f"ğŸš€ å¼€å§‹å®Œæ•´æ–‡æ¡£ä¼˜åŒ–æµç¨‹: {markdown_file_path}")
        
        try:
            # æ­¥éª¤1ï¼šå…¨å±€åˆ†æ
            analysis_file_path = self.step1_analyze_document(markdown_file_path, analysis_output_dir)
            
            # æ­¥éª¤2ï¼šæ–‡æ¡£ä¿®æ”¹
            optimized_file_path = self.step2_modify_document(
                markdown_file_path, analysis_file_path, optimized_output_dir
            )
            
            self.logger.info(f"ğŸ‰ å®Œæ•´ä¼˜åŒ–æµç¨‹å®Œæˆï¼")
            
            return {
                "original_document": markdown_file_path,
                "analysis_file": analysis_file_path,
                "optimized_document": optimized_file_path
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å®Œæ•´ä¼˜åŒ–æµç¨‹å¤±è´¥: {e}")
            raise
    
    def _call_llm_for_modification(self, section_title: str, original_content: str, suggestion: str) -> str:
        """è°ƒç”¨LLMè¿›è¡Œç« èŠ‚ä¿®æ”¹"""
        # ä¸å†å¼ºåˆ¶å­—æ•°é™åˆ¶ï¼Œè®©LLMä¸“æ³¨äºå†…å®¹ä¼˜åŒ–
        original_word_count = len(original_content)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–æ–‡æ¡£ç« èŠ‚å†…å®¹ã€‚

ã€åŸå§‹ç« èŠ‚ã€‘ï¼š{section_title}
ã€åŸå§‹å†…å®¹ã€‘ï¼š
{original_content}

ã€ä¼˜åŒ–å»ºè®®ã€‘ï¼š
{suggestion}

ã€ä¼˜åŒ–è¦æ±‚ã€‘ï¼š
1. æ ¹æ®ä¼˜åŒ–å»ºè®®æ”¹è¿›æ–‡æ¡£çš„è¡¨è¾¾æ–¹å¼ï¼Œæå‡è¯­è¨€è¡¨è¾¾çš„æ¸…æ™°åº¦å’Œä¸“ä¸šæ€§
2. **å¿…é¡»ä¿æŒæ‰€æœ‰é‡è¦çš„è¯¦ç»†ä¿¡æ¯ã€æ•°æ®ã€æŠ€æœ¯è§„èŒƒå’Œæ”¿ç­–ä¾æ®ï¼Œä¸å¾—åˆ é™¤å®è´¨æ€§å†…å®¹**
3. **ä¼˜åŒ–é‡ç‚¹æ˜¯æ”¹è¿›è¡¨è¾¾æ–¹å¼å’Œæ¶ˆé™¤çœŸæ­£çš„é‡å¤ï¼Œè€Œä¸æ˜¯åˆ å‡å†…å®¹é•¿åº¦**
4. **ä¼˜åŒ–åçš„å†…å®¹åº”ä¿æŒä¸åŸæ–‡æ¡£ç›¸è¿‘çš„ä¿¡æ¯é‡ï¼Œåªåˆ é™¤çœŸæ­£é‡å¤çš„å†…å®¹**
5. **ç‰¹åˆ«æ³¨æ„è¡¨æ ¼ä¼˜åŒ–**ï¼šå¦‚æœå»ºè®®ä¸­åŒ…å«"è¡¨æ ¼ä¼˜åŒ–"ï¼Œè¯·å°†ç›¸å…³çš„æ•°æ®ã€å‚æ•°ã€å¯¹æ¯”ä¿¡æ¯è½¬æ¢ä¸ºMarkdownè¡¨æ ¼æ ¼å¼
6. å¯¹äºæ•°æ®å’Œæ•°å­—ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨è¡¨æ ¼å±•ç¤ºï¼ŒåŒæ—¶ä¿æŒå¿…è¦çš„æ–‡å­—è¯´æ˜
7. ä¿æŒåŸæœ‰çš„Markdownæ ¼å¼å’Œç« èŠ‚ç»“æ„
8. **å¯¹äºæŠ€æœ¯ç»†èŠ‚ã€è§„èŒƒæ ‡å‡†ã€å…·ä½“æ•°æ®ç­‰ä¸“ä¸šå†…å®¹ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™**

ã€è¡¨æ ¼æ ¼å¼ç¤ºä¾‹ã€‘ï¼š
| é¡¹ç›® | å‚æ•°1 | å‚æ•°2 | å¤‡æ³¨ |
|------|-------|-------|------|
| ç¤ºä¾‹1 | æ•°å€¼1 | æ•°å€¼2 | è¯´æ˜1 |
| ç¤ºä¾‹2 | æ•°å€¼3 | æ•°å€¼4 | è¯´æ˜2 |

**ã€æœ€ç»ˆæé†’ã€‘**ï¼š
- è¿™æ˜¯å†…å®¹ä¼˜åŒ–ä»»åŠ¡ï¼Œä¸æ˜¯å†…å®¹åˆ å‡ä»»åŠ¡
- å¿…é¡»ä¿æŒç« èŠ‚çš„å®Œæ•´æ€§å’Œä¸“ä¸šæ€§
- åªä¼˜åŒ–è¡¨è¾¾æ–¹å¼ï¼Œä¸åˆ é™¤å®è´¨æ€§ä¿¡æ¯
- ç¡®ä¿è¾“å‡ºå†…å®¹ä¸åŸæ–‡æ¡£ä¿¡æ¯é‡ç›¸è¿‘

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„Markdownå†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ï¼š"""

        if not self.client:
            self.logger.error("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è°ƒç”¨LLM")
            return original_content
            
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°ï¼Œä¼˜å…ˆä½¿ç”¨OPENROUTER_MODEL
            model_name = os.getenv('OPENROUTER_MODEL') or os.getenv('DEFAULT_MODEL') or "anthropic/claude-3.5-sonnet"
            
            response = self.client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return original_content
    
    def regenerate_section(self, section_title: str, original_content: str, suggestion: str, original_json_data: dict) -> dict:
        """é‡æ–°ç”Ÿæˆå•ä¸ªç« èŠ‚"""
        self.logger.info(f"å¼€å§‹å¤„ç†ç« èŠ‚: {section_title}")
        
        try:
            # è°ƒç”¨LLMç”Ÿæˆæ–°å†…å®¹
            new_content = self._call_llm_for_modification(section_title, original_content, suggestion)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "section_title": section_title,
                "original_content": original_content,
                "suggestion": suggestion,
                "regenerated_content": new_content,
                "word_count": len(new_content),
                "status": "success"
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"ç« èŠ‚ {section_title} å¤„ç†å¤±è´¥: {e}")
            return {
                "section_title": section_title,
                "original_content": original_content,
                "suggestion": suggestion,
                "regenerated_content": original_content,  # å¤±è´¥æ—¶è¿”å›åŸå†…å®¹
                "word_count": len(original_content),
                "status": "failed",
                "error": str(e)
            }
    
    def regenerate_document_sections(self, evaluation_file: str, document_file: str) -> Dict[str, Any]:
        """å¹¶è¡Œé‡æ–°ç”Ÿæˆæ–‡æ¡£ç« èŠ‚"""
        self.logger.info(f"å¼€å§‹åˆ†ç« èŠ‚é‡æ–°ç”Ÿæˆæ–‡æ¡£")
        self.logger.info(f"è¯„ä¼°æ–‡ä»¶: {evaluation_file}")
        self.logger.info(f"åŸå§‹æ–‡æ¡£: {document_file}")
        
        # è¯»å–è¯„ä¼°ç»“æœ
        with open(evaluation_file, 'r', encoding='utf-8') as f:
            evaluation_data = json.load(f)
        
        # è¯»å–åŸå§‹æ–‡æ¡£
        with open(document_file, 'r', encoding='utf-8') as f:
            original_document = f.read()
        
        # è§£æç« èŠ‚å†…å®¹
        sections = self._parse_document_sections(original_document)
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []
        modification_instructions = evaluation_data.get('modification_instructions', [])
        table_opportunities = evaluation_data.get('table_opportunities', [])
        
        # åˆ›å»ºç« èŠ‚åˆ°å»ºè®®çš„æ˜ å°„
        section_suggestions = {}
        section_table_opportunities = {}
        
        # æ”¶é›†ä¿®æ”¹å»ºè®®
        for instruction in modification_instructions:
            subtitle = instruction.get('subtitle', '')
            suggestion = instruction.get('suggestion', '')
            if subtitle not in section_suggestions:
                section_suggestions[subtitle] = []
            section_suggestions[subtitle].append(suggestion)
        
        # æ”¶é›†è¡¨æ ¼ä¼˜åŒ–æœºä¼š
        for table_opp in table_opportunities:
            section_title = table_opp.get('section_title', '')
            table_suggestion = table_opp.get('table_opportunity', '')
            if section_title not in section_table_opportunities:
                section_table_opportunities[section_title] = []
            section_table_opportunities[section_title].append(table_suggestion)
        
        # åˆå¹¶æ‰€æœ‰éœ€è¦å¤„ç†çš„ç« èŠ‚
        all_sections = set(section_suggestions.keys())
        
        # ä¸ºè¡¨æ ¼ä¼˜åŒ–æœºä¼šæ‰¾åˆ°åŒ¹é…çš„ç« èŠ‚
        for table_section_title in section_table_opportunities.keys():
            matched_sections = []
            
            # æ ¹æ®è¡¨æ ¼æœºä¼šçš„ç« èŠ‚æ ‡é¢˜æ‰¾åˆ°å¯¹åº”çš„å®é™…ç« èŠ‚
            if "é¡¹ç›®éœ€æ±‚åˆ†æä¸äº§å‡ºæ–¹æ¡ˆ" in table_section_title:
                # åŒ¹é…éœ€æ±‚åˆ†æå’Œäº§å‡ºæ–¹æ¡ˆç›¸å…³ç« èŠ‚
                for section_key in sections.keys():
                    if any(keyword in section_key for keyword in ["éœ€æ±‚åˆ†æ", "äº§å‡ºæ–¹æ¡ˆ", "å»ºè®¾å†…å®¹å’Œè§„æ¨¡"]):
                        matched_sections.append(section_key)
            elif "é¡¹ç›®é€‰å€ä¸è¦ç´ ä¿éšœ" in table_section_title:
                # åŒ¹é…é€‰å€ç›¸å…³ç« èŠ‚
                for section_key in sections.keys():
                    if any(keyword in section_key for keyword in ["é€‰å€", "é€‰çº¿"]):
                        matched_sections.append(section_key)
            elif "é¡¹ç›®å»ºè®¾æ–¹æ¡ˆ" in table_section_title:
                # åŒ¹é…å»ºè®¾æ–¹æ¡ˆç›¸å…³ç« èŠ‚
                for section_key in sections.keys():
                    if any(keyword in section_key for keyword in ["å»ºè®¾å†…å®¹", "å»ºè®¾æ–¹æ¡ˆ", "æ•°å­—åŒ–æ–¹æ¡ˆ", "å»ºè®¾ç®¡ç†", "å·¥ç¨‹æ–¹æ¡ˆ", "è®¾å¤‡æ–¹æ¡ˆ"]):
                        matched_sections.append(section_key)
            elif "ï¼ˆäºŒï¼‰é¡¹ç›®å•ä½æ¦‚å†µ" in table_section_title:
                # ç›´æ¥åŒ¹é…é¡¹ç›®å•ä½æ¦‚å†µ
                for section_key in sections.keys():
                    if "é¡¹ç›®å•ä½æ¦‚å†µ" in section_key:
                        matched_sections.append(section_key)
            else:
                # é€šç”¨åŒ¹é…é€»è¾‘
                for section_key in sections.keys():
                    if table_section_title in section_key or section_key in table_section_title:
                        matched_sections.append(section_key)
            
            # å°†åŒ¹é…åˆ°çš„ç« èŠ‚æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
            all_sections.update(matched_sections)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•ç« èŠ‚éœ€è¦å¤„ç†ï¼Œè¯´æ˜å¯èƒ½æ˜¯åˆ†æå¤±è´¥ï¼Œè¿”å›åŸå§‹ç« èŠ‚å†…å®¹
        if not all_sections:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„ç« èŠ‚ï¼Œå¯èƒ½æ˜¯åˆ†æç»“æœä¸ºç©ºï¼Œè¿”å›åŸå§‹å†…å®¹")
            # è¿”å›åŸå§‹ç« èŠ‚å†…å®¹ï¼Œç¡®ä¿æœ‰å†…å®¹å¯ä»¥æ˜¾ç¤º
            result = {}
            for section_title, section_data in sections.items():
                result[section_title] = {
                    "content": section_data['content'],
                    "quality_score": 1.0,
                    "word_count": len(section_data['content'].split()),
                    "generation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "status": "original",
                    "note": "åˆ†æå¤±è´¥ï¼Œä¿ç•™åŸå§‹å†…å®¹"
                }
            return result
        
        # ä¸ºæ¯ä¸ªç« èŠ‚åˆ›å»ºä»»åŠ¡
        for section_title in all_sections:
            section_content = self._find_section_content(sections, section_title)
            if section_content:
                # åˆå¹¶è¯¥ç« èŠ‚çš„æ‰€æœ‰å»ºè®®
                combined_suggestions = []
                
                # æ·»åŠ ä¿®æ”¹å»ºè®®
                if section_title in section_suggestions:
                    combined_suggestions.extend([f"è¡¨è¾¾ä¼˜åŒ–ï¼š{s}" for s in section_suggestions[section_title]])
                
                # æ·»åŠ è¡¨æ ¼ä¼˜åŒ–å»ºè®®
                for table_section_title, table_opps in section_table_opportunities.items():
                    # æ£€æŸ¥å½“å‰ç« èŠ‚æ˜¯å¦åº”è¯¥åŒ…å«è¿™äº›è¡¨æ ¼ä¼˜åŒ–å»ºè®®
                    should_include = False
                    
                    if "é¡¹ç›®éœ€æ±‚åˆ†æä¸äº§å‡ºæ–¹æ¡ˆ" in table_section_title:
                        if any(keyword in section_title for keyword in ["éœ€æ±‚åˆ†æ", "äº§å‡ºæ–¹æ¡ˆ", "å»ºè®¾å†…å®¹å’Œè§„æ¨¡"]):
                            should_include = True
                    elif "é¡¹ç›®é€‰å€ä¸è¦ç´ ä¿éšœ" in table_section_title:
                        if any(keyword in section_title for keyword in ["é€‰å€", "é€‰çº¿"]):
                            should_include = True
                    elif "é¡¹ç›®å»ºè®¾æ–¹æ¡ˆ" in table_section_title:
                        if any(keyword in section_title for keyword in ["å»ºè®¾å†…å®¹", "å»ºè®¾æ–¹æ¡ˆ", "æ•°å­—åŒ–æ–¹æ¡ˆ", "å»ºè®¾ç®¡ç†"]):
                            should_include = True
                    else:
                        if table_section_title in section_title or section_title in table_section_title:
                            should_include = True
                    
                    if should_include:
                        combined_suggestions.extend([f"è¡¨æ ¼ä¼˜åŒ–ï¼š{s}" for s in table_opps])
                
                if combined_suggestions:
                    tasks.append({
                        'section_title': section_title,
                        'original_content': section_content,
                        'suggestion': '\n'.join(combined_suggestions),
                        'original_json_data': evaluation_data
                    })
        
        # å¹¶è¡Œå¤„ç†ç« èŠ‚
        regeneration_results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_task = {
                executor.submit(
                    self.regenerate_section,
                    task['section_title'],
                    task['original_content'],
                    task['suggestion'],
                    task['original_json_data']
                ): task for task in tasks
            }
            
            completed_count = 0
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                section_title = task['section_title']
                try:
                    result = future.result()
                    regeneration_results[section_title] = result
                    completed_count += 1
                    self.logger.info(f"å®Œæˆç« èŠ‚ ({completed_count}/{len(tasks)}): {section_title}")
                except Exception as e:
                    self.logger.error(f"å¤„ç†ç« èŠ‚å¤±è´¥ {section_title}: {e}")
                    regeneration_results[section_title] = {
                        "section_title": section_title,
                        "status": "failed",
                        "error": str(e)
                    }
        
        return regeneration_results
    
    def _parse_document_sections(self, document_content: str) -> Dict[str, str]:
        """è§£ææ–‡æ¡£ç« èŠ‚ï¼Œä¿æŒå±‚çº§ç»“æ„"""
        sections = {}
        lines = document_content.split('\n')
        current_section = None
        current_content = []
        current_h1 = None  # å½“å‰ä¸€çº§æ ‡é¢˜
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
            if line.strip().startswith('#'):
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section:
                    sections[current_section] = {
                        'content': '\n'.join(current_content).strip(),
                        'h1_parent': current_h1,
                        'full_title': current_section
                    }
                
                # åˆ¤æ–­æ ‡é¢˜çº§åˆ«
                if line.strip().startswith('# ') and not line.strip().startswith('## '):
                    # ä¸€çº§æ ‡é¢˜
                    current_h1 = line.strip().replace('# ', '').strip()
                    current_section = current_h1
                elif line.strip().startswith('## '):
                    # äºŒçº§æ ‡é¢˜
                    current_section = line.strip().replace('## ', '').strip()
                else:
                    # å…¶ä»–çº§åˆ«æ ‡é¢˜ï¼ŒæŒ‰äºŒçº§å¤„ç†
                    current_section = line.strip().replace('#', '').strip()
                
                current_content = [line]
            else:
                if current_section:
                    current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            sections[current_section] = {
                'content': '\n'.join(current_content).strip(),
                'h1_parent': current_h1,
                'full_title': current_section
            }
        
        return sections
    
    def _find_section_content(self, sections: Dict[str, dict], section_title: str) -> Optional[str]:
        """æŸ¥æ‰¾ç« èŠ‚å†…å®¹"""
        # ç›´æ¥åŒ¹é…
        if section_title in sections:
            return sections[section_title]['content']
        
        # æ¨¡ç³ŠåŒ¹é…
        for title, section_data in sections.items():
            if section_title in title or title in section_title:
                return section_data['content']
        
        return None
    
    def regenerate_and_merge_document(self, evaluation_file: str, document_file: str, 
                                    output_dir: str = "./test_results", auto_merge: bool = False) -> Dict[str, str]:
        """é‡æ–°ç”Ÿæˆå¹¶åˆå¹¶æ–‡æ¡£"""
        self.logger.info("å¼€å§‹é‡æ–°ç”Ÿæˆå¹¶åˆå¹¶æ–‡æ¡£")
        
        # é‡æ–°ç”Ÿæˆç« èŠ‚
        regeneration_results = self.regenerate_document_sections(evaluation_file, document_file)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(os.path.basename(document_file))[0]
        
        # ä¿å­˜é‡æ–°ç”Ÿæˆçš„ç« èŠ‚åˆ°JSON
        regenerated_json_path = os.path.join(output_dir, f"regenerated_sections_{timestamp}.json")
        with open(regenerated_json_path, 'w', encoding='utf-8') as f:
            json.dump(regeneration_results, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæ–‡æ¡£
        regenerated_md_path = os.path.join(output_dir, f"regenerated_sections_{timestamp}.md")
        self._generate_markdown_document(regeneration_results, regenerated_md_path, document_file)
        
        return {
            "regenerated_sections": regenerated_md_path,
            "regenerated_json": regenerated_json_path,
            "original_document": document_file,
            "evaluation_file": evaluation_file
        }
    
    def _generate_markdown_document(self, regeneration_results: Dict[str, Any], output_path: str, original_document_path: str):
        """ç”ŸæˆMarkdownæ–‡æ¡£ï¼Œä¿æŒåŸå§‹å±‚çº§ç»“æ„"""
        # è¯»å–åŸå§‹æ–‡æ¡£
        with open(original_document_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        # å¦‚æœæ²¡æœ‰é‡æ–°ç”Ÿæˆçš„å†…å®¹ï¼Œç›´æ¥å¤åˆ¶åŸæ–‡æ¡£
        if not regeneration_results:
            self.logger.info("æ²¡æœ‰é‡æ–°ç”Ÿæˆçš„å†…å®¹ï¼Œä¿æŒåŸæ–‡æ¡£ç»“æ„")
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(original_content)
            return
        
        # è§£æåŸå§‹æ–‡æ¡£çš„ç« èŠ‚ç»“æ„
        original_sections = self._parse_document_sections(original_content)
        
        # é‡å»ºå®Œæ•´æ–‡æ¡£ç»“æ„
        with open(output_path, 'w', encoding='utf-8') as f:
            lines = original_content.split('\n')
            i = 0
            
            while i < len(lines):
                line = lines[i]
                
                if line.strip().startswith('# ') and not line.strip().startswith('## '):
                    # ä¸€çº§æ ‡é¢˜ï¼Œç›´æ¥å†™å…¥
                    f.write(f"{line}\n\n")
                    i += 1
                    
                elif line.strip().startswith('## '):
                    # äºŒçº§æ ‡é¢˜
                    h2_title = line.strip().replace('## ', '').strip()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é‡æ–°ç”Ÿæˆçš„å†…å®¹
                    if h2_title in regeneration_results and regeneration_results[h2_title].get('status') == 'success':
                        # å†™å…¥æ ‡é¢˜
                        f.write(f"{line}\n\n")
                        
                        # å†™å…¥é‡æ–°ç”Ÿæˆçš„å†…å®¹
                        content = regeneration_results[h2_title].get('regenerated_content', '')
                        # ç§»é™¤å†…å®¹å¼€å¤´çš„æ ‡é¢˜è¡Œ
                        content_lines = content.split('\n')
                        if content_lines and content_lines[0].strip().startswith('#'):
                            content = '\n'.join(content_lines[1:]).strip()
                        
                        f.write(f"{content}\n\n")
                        
                        # è·³è¿‡åŸå§‹å†…å®¹ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜
                        i += 1
                        while i < len(lines) and not lines[i].strip().startswith('#'):
                            i += 1
                    else:
                        # ä½¿ç”¨åŸå§‹å†…å®¹ï¼Œå†™å…¥æ ‡é¢˜
                        f.write(f"{line}\n")
                        i += 1
                        
                        # å†™å…¥åŸå§‹å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜
                        while i < len(lines) and not lines[i].strip().startswith('#'):
                            f.write(f"{lines[i]}\n")
                            i += 1
                        f.write("\n")  # ç« èŠ‚é—´ç©ºè¡Œ
                        
                elif line.strip().startswith('#'):
                    # å…¶ä»–çº§åˆ«æ ‡é¢˜ï¼Œç›´æ¥å†™å…¥
                    f.write(f"{line}\n")
                    i += 1
                else:
                    # éæ ‡é¢˜è¡Œï¼Œåœ¨ä¸€çº§æ ‡é¢˜ä¸‹ç›´æ¥å†™å…¥
                    f.write(f"{line}\n")
                    i += 1
    
    def _find_original_title_format(self, original_content: str, section_title: str) -> Optional[str]:
        """æŸ¥æ‰¾åŸå§‹æ–‡æ¡£ä¸­çš„æ ‡é¢˜æ ¼å¼"""
        lines = original_content.split('\n')
        for line in lines:
            if line.strip().startswith('#') and section_title in line:
                return line.strip()
        return None


def main():
    """ä¸»å‡½æ•° - ç»Ÿä¸€å¯åŠ¨å…¥å£"""
    # é»˜è®¤æµ‹è¯•æ–‡æ¡£è·¯å¾„
    default_document_path = "/Users/wangzijian/Desktop/gauz/keyan/final_review_agent_app/final_markdown_merged_document_20250904_162736.md"
    
    if len(sys.argv) < 2:
        print("ğŸš€ æ–‡æ¡£ä¼˜åŒ–å™¨ - ç»Ÿä¸€å¯åŠ¨å…¥å£")
        print()
        print("ğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
        print("  1. å¿«é€Ÿä¼˜åŒ–ï¼ˆæ¨èï¼‰- åˆ†ç« èŠ‚å¹¶è¡Œå¤„ç†ï¼Œä¿æŒ80-90%å­—æ•°:")
        print("    python run_document_optimizer.py")
        print("    python run_document_optimizer.py <markdownæ–‡ä»¶è·¯å¾„>")
        print()
        print("  2. ä¼ ç»Ÿå…¨æ–‡ä¼˜åŒ–:")
        print("    python run_document_optimizer.py --full <markdownæ–‡ä»¶è·¯å¾„>")
        print("    python run_document_optimizer.py --step1 <markdownæ–‡ä»¶è·¯å¾„>")
        print("    python run_document_optimizer.py --step2 <markdownæ–‡ä»¶è·¯å¾„> <åˆ†æç»“æœjsonè·¯å¾„>")
        print()
        print("  3. ä»…åˆ†æä¸ä¿®æ”¹:")
        print("    python run_document_optimizer.py --analyze-only <markdownæ–‡ä»¶è·¯å¾„>")
        print()
        print("ğŸ“ ç¤ºä¾‹:")
        print("  python run_document_optimizer.py                    # ä½¿ç”¨é»˜è®¤æ–‡æ¡£ï¼Œå¿«é€Ÿä¼˜åŒ–")
        print("  python run_document_optimizer.py document.md        # å¿«é€Ÿä¼˜åŒ–æŒ‡å®šæ–‡æ¡£")
        print("  python run_document_optimizer.py --full document.md # ä¼ ç»Ÿå…¨æ–‡ä¼˜åŒ–")
        print()
        print("ğŸ§ª é»˜è®¤é…ç½®:")
        print(f"  é»˜è®¤æ–‡æ¡£: {default_document_path}")
        print("  è¾“å‡ºç›®å½•: ./test_results")
        print("  ä¼˜åŒ–æ–¹å¼: åˆ†ç« èŠ‚å¹¶è¡Œå¤„ç†ï¼ˆä¿æŒ80-90%å­—æ•°ï¼‰")
        print()
        
        # ä½¿ç”¨é»˜è®¤æ–‡æ¡£è¿›è¡Œå¿«é€Ÿä¼˜åŒ–
        print(f"ğŸš€ ä½¿ç”¨é»˜è®¤æ–‡æ¡£è¿›è¡Œå¿«é€Ÿä¼˜åŒ–: {default_document_path}")
        print("=" * 60)
        
        # å…ˆåˆ†æ
        optimizer = DocumentOptimizer()
        analysis_file = optimizer.step1_analyze_document(default_document_path, "./test_results")
        
        # ç„¶ååˆ†ç« èŠ‚ä¼˜åŒ–
        result_paths = optimizer.regenerate_and_merge_document(
            evaluation_file=analysis_file,
            document_file=default_document_path,
            output_dir="./test_results",
            auto_merge=False  # ä¸è‡ªåŠ¨åˆå¹¶ï¼Œå› ä¸ºæ˜¯Markdownæ–‡æ¡£
        )
        
        print(f"\nğŸ‰ å¿«é€Ÿä¼˜åŒ–å®Œæˆï¼")
        print(f"ğŸ“„ åŸå§‹æ–‡æ¡£: {default_document_path}")
        print(f"ğŸ“Š åˆ†æç»“æœ: {analysis_file}")
        print(f"ğŸ“ ä¼˜åŒ–ç« èŠ‚: {result_paths.get('regenerated_sections')}")
        return
    
    optimizer = DocumentOptimizer()
    
    try:
        if sys.argv[1] == "--full":
            # ä¼ ç»Ÿå…¨æ–‡ä¼˜åŒ–æ¨¡å¼
            if len(sys.argv) < 3:
                print("âŒ ç¼ºå°‘Markdownæ–‡ä»¶è·¯å¾„")
                return
            
            markdown_file = sys.argv[2]
            print(f"ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿå…¨æ–‡ä¼˜åŒ–æ¨¡å¼: {markdown_file}")
            result = optimizer.optimize_document_complete(
                markdown_file,
                analysis_output_dir="./test_results",
                optimized_output_dir="./test_results"
            )
            
            print(f"\nğŸ‰ ä¼ ç»Ÿå…¨æ–‡ä¼˜åŒ–å®Œæˆï¼")
            print(f"ğŸ“„ åŸå§‹æ–‡æ¡£: {result['original_document']}")
            print(f"ğŸ“Š åˆ†æç»“æœ: {result['analysis_file']}")
            print(f"âœ¨ ä¼˜åŒ–æ–‡æ¡£: {result['optimized_document']}")
            
        elif sys.argv[1] == "--step1":
            # ä»…æ‰§è¡Œæ­¥éª¤1ï¼šå…¨å±€åˆ†æ
            if len(sys.argv) < 3:
                print("âŒ ç¼ºå°‘Markdownæ–‡ä»¶è·¯å¾„")
                return
            
            markdown_file = sys.argv[2]
            analysis_file = optimizer.step1_analyze_document(markdown_file, "./test_results")
            
            print(f"\nğŸ¯ æ­¥éª¤1å®Œæˆï¼")
            print(f"ğŸ“„ åŸå§‹æ–‡æ¡£: {markdown_file}")
            print(f"ğŸ“Š åˆ†æç»“æœ: {analysis_file}")
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œ:")
            print(f"   python run_document_optimizer.py --step2 {markdown_file} {analysis_file}")
            print(f"   æˆ–è€…: python regenerate_sections.py {analysis_file}")
            
        elif sys.argv[1] == "--step2":
            # ä»…æ‰§è¡Œæ­¥éª¤2ï¼šæ–‡æ¡£ä¿®æ”¹
            if len(sys.argv) < 4:
                print("âŒ ç¼ºå°‘Markdownæ–‡ä»¶è·¯å¾„æˆ–åˆ†æç»“æœæ–‡ä»¶è·¯å¾„")
                return
            
            markdown_file = sys.argv[2]
            analysis_file = sys.argv[3]
            optimized_file = optimizer.step2_modify_document(markdown_file, analysis_file, "./test_results")
            
            print(f"\nğŸ¯ æ­¥éª¤2å®Œæˆï¼")
            print(f"ğŸ“„ åŸå§‹æ–‡æ¡£: {markdown_file}")
            print(f"ğŸ“Š åˆ†æç»“æœ: {analysis_file}")
            print(f"âœ¨ ä¼˜åŒ–æ–‡æ¡£: {optimized_file}")
            
        elif sys.argv[1] == "--analyze-only":
            # ä»…åˆ†æä¸ä¿®æ”¹
            if len(sys.argv) < 3:
                print("âŒ ç¼ºå°‘Markdownæ–‡ä»¶è·¯å¾„")
                return
            
            markdown_file = sys.argv[2]
            analysis_file = optimizer.step1_analyze_document(markdown_file, "./test_results")
            
            print(f"\nğŸ“Š åˆ†æå®Œæˆï¼")
            print(f"ğŸ“„ åŸå§‹æ–‡æ¡£: {markdown_file}")
            print(f"ğŸ“Š åˆ†æç»“æœ: {analysis_file}")
            print(f"\nğŸ’¡ å¦‚éœ€ä¼˜åŒ–ï¼Œå¯ä»¥è¿è¡Œ:")
            print(f"   python regenerate_sections.py {analysis_file}")
            
        else:
            # å¿«é€Ÿä¼˜åŒ–æ¨¡å¼ï¼ˆé»˜è®¤æ¨èï¼‰
            markdown_file = sys.argv[1]
            print(f"ğŸš€ ä½¿ç”¨å¿«é€Ÿä¼˜åŒ–æ¨¡å¼: {markdown_file}")
            print("=" * 60)
            
            # å…ˆåˆ†æ
            analysis_file = optimizer.step1_analyze_document(markdown_file, "./test_results")
            
            # ç„¶ååˆ†ç« èŠ‚ä¼˜åŒ–
            result_paths = optimizer.regenerate_and_merge_document(
                evaluation_file=analysis_file,
                document_file=markdown_file,
                output_dir="./test_results",
                auto_merge=False  # ä¸è‡ªåŠ¨åˆå¹¶ï¼Œå› ä¸ºæ˜¯Markdownæ–‡æ¡£
            )
            
            print(f"\nğŸ‰ å¿«é€Ÿä¼˜åŒ–å®Œæˆï¼")
            print(f"ğŸ“„ åŸå§‹æ–‡æ¡£: {markdown_file}")
            print(f"ğŸ“Š åˆ†æç»“æœ: {analysis_file}")
            print(f"ğŸ“ ä¼˜åŒ–ç« èŠ‚: {result_paths.get('regenerated_sections')}")
            
    except Exception as e:
        print(f"\nâŒ ä¼˜åŒ–è¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
