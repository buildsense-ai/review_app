#!/usr/bin/env python3
"""
è¯æ®æ£€æµ‹å™¨ - ç»Ÿä¸€çš„è®ºæ–­æ£€æµ‹å’Œè¯æ®å¡«å……ç³»ç»Ÿ
æ›¿ä»£åŸæœ‰çš„claim_detector.pyå’Œevidence_analyzer.py
ä¸»è¦é€»è¾‘ï¼šAIæ‰¾å‡ºæ‰€æœ‰æ²¡æœ‰è®ºæ®çš„claims -> websearchæ‰¾è¯æ® -> å¡«å…¥æ–‡æ¡£
"""

import json
import re
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from openai import OpenAI
import config

# é…ç½®HTTPè¯·æ±‚æ—¥å¿—
def setup_http_logging():
    """è®¾ç½®HTTPè¯·æ±‚æ—¥å¿—"""
    if getattr(config, 'ENABLE_HTTP_LOGS', True):
        httpx_logger = logging.getLogger("httpx")
        httpx_logger.setLevel(logging.INFO)
        
        if not httpx_logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            httpx_logger.addHandler(handler)

setup_http_logging()

@dataclass
class UnsupportedClaim:
    """ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­"""
    claim_id: str
    claim_text: str
    section_title: str
    claim_type: str  # 'factual', 'statistical', 'causal', 'comparative', 'historical'
    confidence_level: float  # 0.0-1.0ï¼Œè¡¨ç¤ºéœ€è¦è¯æ®æ”¯æ’‘çš„ç´§è¿«ç¨‹åº¦
    context: str  # è®ºæ–­çš„ä¸Šä¸‹æ–‡
    search_keywords: List[str]  # æœç´¢å…³é”®è¯
    original_position: int  # åœ¨åŸæ–‡ä¸­çš„ä½ç½®ï¼ˆè¡Œå·æˆ–æ®µè½å·ï¼‰

@dataclass
class EvidenceResult:
    """è¯æ®æœç´¢ç»“æœ"""
    claim_id: str
    claim_text: str
    section_title: str
    search_query: str
    evidence_sources: List[Dict[str, Any]]  # æœç´¢åˆ°çš„è¯æ®æ¥æº
    enhanced_text: str  # å¢å¼ºåçš„æ–‡æœ¬ï¼ˆåŒ…å«è¯æ®ï¼‰
    confidence_score: float  # è¯æ®å¯ä¿¡åº¦è¯„åˆ†
    processing_status: str  # 'success', 'partial', 'failed'

class EvidenceDetector:
    """ç»Ÿä¸€çš„è¯æ®æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯æ®æ£€æµ‹å™¨"""
        self.client = OpenAI(
            base_url=config.OPENROUTER_BASE_URL,
            api_key=config.OPENROUTER_API_KEY
        )
        self.model = config.MODEL_NAME
        
        # å¹¶è¡Œå¤„ç†é…ç½®
        self.max_workers = getattr(config, 'MAX_WORKERS', 5)
        self.thread_lock = threading.Lock()
        
        # å¯¼å…¥webæœç´¢ä»£ç†
        from web_search_agent import WebSearchAgent
        self.web_search_agent = WebSearchAgent()
    
    def process_section(self, section_title: str, section_content: str, 
                       max_claims: int = 10) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªç« èŠ‚ï¼šæ£€æµ‹ç¼ºä¹è¯æ®çš„è®ºæ–­ -> æœç´¢è¯æ® -> ç”Ÿæˆå¢å¼ºæ–‡æ¡£
        
        Args:
            section_title: ç« èŠ‚æ ‡é¢˜
            section_content: ç« èŠ‚å†…å®¹
            max_claims: æœ€å¤§å¤„ç†è®ºæ–­æ•°
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        print(f"ğŸ” å¤„ç†ç« èŠ‚: {section_title}")
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­
            unsupported_claims = self._detect_unsupported_claims(section_title, section_content)
            
            if len(unsupported_claims) > max_claims:
                # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–å‰Nä¸ª
                unsupported_claims.sort(key=lambda x: x.confidence_level, reverse=True)
                unsupported_claims = unsupported_claims[:max_claims]
                print(f"  ğŸ“Š é™åˆ¶å¤„ç†è®ºæ–­æ•°é‡ä¸º {max_claims} ä¸ªï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰")
            
            if not unsupported_claims:
                print(f"  âœ… ç« èŠ‚ '{section_title}' æœªå‘ç°éœ€è¦è¯æ®æ”¯æ’‘çš„è®ºæ–­")
                return {
                    'section_title': section_title,
                    'status': 'success',
                    'message': 'æœªå‘ç°éœ€è¦è¯æ®æ”¯æ’‘çš„è®ºæ–­',
                    'unsupported_claims': [],
                    'evidence_results': [],
                    'enhanced_content': section_content,
                    'original_content': section_content,
                    'processing_time': time.time() - start_time,
                    'statistics': {
                        'claims_detected': 0,
                        'evidence_found': 0,
                        'claims_enhanced': 0
                    }
                }
            
            print(f"  ğŸ“‹ æ£€æµ‹åˆ° {len(unsupported_claims)} ä¸ªç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­")
            
            # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œæœç´¢è¯æ®
            evidence_results = self._batch_search_evidence(unsupported_claims)
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå¢å¼ºå†…å®¹ï¼ˆå°†è¯æ®æ•´åˆåˆ°åŸæ–‡ä¸­ï¼‰
            enhanced_content = self._enhance_content_with_evidence(section_content, evidence_results)
            
            # ç»Ÿè®¡ä¿¡æ¯
            successful_evidence = sum(1 for er in evidence_results if er.processing_status == 'success')
            total_evidence_sources = sum(len(er.evidence_sources) for er in evidence_results)
            
            processing_time = time.time() - start_time
            
            # ç®€åŒ–æ—¥å¿—è¾“å‡ºï¼Œè¯¦ç»†ç»Ÿè®¡ç”±ä¸Šå±‚æµæ°´çº¿å¤„ç†
            # print(f"  âœ… ç« èŠ‚ '{section_title}' å¤„ç†å®Œæˆ")
            # print(f"     - æ£€æµ‹è®ºæ–­: {len(unsupported_claims)} ä¸ª")
            # print(f"     - æˆåŠŸæœç´¢: {successful_evidence} ä¸ª")
            # print(f"     - è¯æ®æ¥æº: {total_evidence_sources} æ¡")
            # print(f"     - å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return {
                'section_title': section_title,
                'status': 'success',
                'message': f'æˆåŠŸå¤„ç† {len(unsupported_claims)} ä¸ªè®ºæ–­',
                'unsupported_claims': [asdict(claim) for claim in unsupported_claims],
                'evidence_results': [asdict(result) for result in evidence_results],
                'enhanced_content': enhanced_content,
                'original_content': section_content,
                'processing_time': processing_time,
                'statistics': {
                    'claims_detected': len(unsupported_claims),
                    'evidence_found': total_evidence_sources,
                    'claims_enhanced': successful_evidence,
                    'success_rate': (successful_evidence / len(unsupported_claims) * 100) if unsupported_claims else 0
                }
            }
            
        except Exception as e:
            error_msg = f"å¤„ç†ç« èŠ‚ '{section_title}' æ—¶å‡ºé”™: {str(e)}"
            print(f"  âŒ {error_msg}")
            
            return {
                'section_title': section_title,
                'status': 'failed',
                'message': error_msg,
                'unsupported_claims': [],
                'evidence_results': [],
                'enhanced_content': section_content,
                'original_content': section_content,
                'processing_time': time.time() - start_time,
                'error': str(e)
            }
    
    def _detect_unsupported_claims(self, section_title: str, section_content: str) -> List[UnsupportedClaim]:
        """æ£€æµ‹ç« èŠ‚ä¸­ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯å†™ä½œåˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œæ‰¾å‡ºæ‰€æœ‰ç¼ºä¹å……åˆ†è¯æ®æ”¯æ’‘çš„è®ºæ–­ã€‚

ç« èŠ‚æ ‡é¢˜ï¼š{section_title}

ç« èŠ‚å†…å®¹ï¼š
{section_content}

å¯¹äºæ¯ä¸ªè®ºæ–­ï¼Œè¯·è¯„ä¼°ï¼š
- æœç´¢å…³é”®è¯ï¼šå»ºè®®ç”¨äºæœç´¢è¯æ®çš„å…³é”®è¯ï¼ˆ3-5ä¸ªï¼‰
- ä¸Šä¸‹æ–‡ï¼šè®ºæ–­åœ¨æ–‡ä¸­çš„ä¸Šä¸‹æ–‡
- ç« èŠ‚ä½ç½®ï¼šç¡®ä¿æ ‡è®°è®ºæ–­æ‰€åœ¨çš„ç« èŠ‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š

{{
    "unsupported_claims": [
        {{
            "claim_text": "å…·ä½“çš„è®ºæ–­æ–‡æœ¬",
            "context": "è®ºæ–­çš„ä¸Šä¸‹æ–‡èƒŒæ™¯",
            "search_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
            "section_title": "{section_title}"
        }}
    ]
}}

é‡è¦è¦æ±‚ï¼š
1. åªè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Š
2. åªè¯†åˆ«çœŸæ­£ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­
3. section_titleå­—æ®µå¿…é¡»ä¸æä¾›çš„ç« èŠ‚æ ‡é¢˜å®Œå…¨ä¸€è‡´
4. å¦‚æœæ²¡æœ‰å‘ç°ç¼ºä¹è¯æ®çš„è®ºæ–­ï¼Œè¿”å›ç©ºæ•°ç»„
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=4000
            )
            
            result_text = response.choices[0].message.content.strip()
            print(f"    ğŸ” AIæ£€æµ‹ç»“æœé¢„è§ˆ: {result_text[:150]}...")
            
            # è§£æJSONç»“æœ
            cleaned_text = self._clean_json_text(result_text)
            try:
                result_json = json.loads(cleaned_text)
            except json.JSONDecodeError as json_error:
                print(f"    âš ï¸ JSONè§£æå¤±è´¥: {str(json_error)}")
                print(f"    ğŸ“„ åŸå§‹å“åº”: {result_text[:500]}...")
                print(f"    ğŸ§¹ æ¸…ç†åå†…å®¹: {cleaned_text[:500]}...")
                # å°è¯•ä»å“åº”ä¸­æå–å¯èƒ½çš„JSONéƒ¨åˆ†
                try:
                    # æŸ¥æ‰¾JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
                    start_idx = cleaned_text.find('{')
                    if start_idx != -1:
                        # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                        brace_count = 0
                        end_idx = -1
                        for i in range(start_idx, len(cleaned_text)):
                            if cleaned_text[i] == '{':
                                brace_count += 1
                            elif cleaned_text[i] == '}':
                                brace_count -= 1
                                if brace_count == 0:
                                    end_idx = i + 1
                                    break
                        
                        if end_idx != -1:
                            json_part = cleaned_text[start_idx:end_idx]
                            result_json = json.loads(json_part)
                            print(f"    âœ… æˆåŠŸæå–JSONç‰‡æ®µ")
                        else:
                            raise json_error
                    else:
                        raise json_error
                except:
                    print(f"    âŒ æ— æ³•ä¿®å¤JSONï¼Œè¿”å›ç©ºç»“æœ")
                    return []
            
            claims = []
            for i, claim_data in enumerate(result_json.get('unsupported_claims', [])):
                # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ç« èŠ‚æ ‡é¢˜
                claim_section_title = claim_data.get('section_title', section_title)
                
                claim = UnsupportedClaim(
                    claim_id=f"{claim_section_title}_claim_{i+1}",
                    claim_text=claim_data['claim_text'],
                    section_title=claim_section_title,
                    claim_type='factual',  # é»˜è®¤ç±»å‹
                    confidence_level=0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                    context=claim_data.get('context', ''),
                    search_keywords=claim_data.get('search_keywords', []),
                    original_position=i+1
                )
                claims.append(claim)
            
            return claims
            
        except Exception as e:
            print(f"    âŒ æ£€æµ‹è®ºæ–­æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def _batch_search_evidence(self, unsupported_claims: List[UnsupportedClaim]) -> List[EvidenceResult]:
        """æ‰¹é‡æœç´¢è¯æ®"""
        print(f"    ğŸ” å¼€å§‹ä¸º {len(unsupported_claims)} ä¸ªè®ºæ–­æœç´¢è¯æ®...")
        
        evidence_results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæœç´¢
        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(unsupported_claims))) as executor:
            future_to_claim = {
                executor.submit(self._search_evidence_for_claim, claim): claim
                for claim in unsupported_claims
            }
            
            completed = 0
            for future in as_completed(future_to_claim):
                claim = future_to_claim[future]
                try:
                    evidence_result = future.result()
                    evidence_results.append(evidence_result)
                    completed += 1
                    
                    with self.thread_lock:
                        print(f"      âœ… è®ºæ–­ {completed}/{len(unsupported_claims)} è¯æ®æœç´¢å®Œæˆ")
                        if evidence_result.processing_status == 'success':
                            print(f"         æ‰¾åˆ° {len(evidence_result.evidence_sources)} æ¡è¯æ®")
                        else:
                            print(f"         æœç´¢å¤±è´¥: {evidence_result.processing_status}")
                
                except Exception as e:
                    with self.thread_lock:
                        print(f"      âŒ è®ºæ–­ '{claim.claim_id}' æœç´¢å¼‚å¸¸: {str(e)}")
                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    evidence_results.append(EvidenceResult(
                        claim_id=claim.claim_id,
                        claim_text=claim.claim_text,
                        section_title=claim.section_title,
                        search_query='',
                        evidence_sources=[],
                        enhanced_text=claim.claim_text,
                        confidence_score=0.0,
                        processing_status='failed'
                    ))
        
        return evidence_results
    
    def _search_evidence_for_claim(self, claim: UnsupportedClaim) -> EvidenceResult:
        """ä¸ºå•ä¸ªè®ºæ–­æœç´¢è¯æ®"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = ' '.join(claim.search_keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
            
            # ä½¿ç”¨webæœç´¢ä»£ç†æœç´¢è¯æ®
            evidence_collection = self.web_search_agent.search_evidence_for_claim(
                claim_id=claim.claim_id,
                search_keywords=claim.search_keywords,
                claim_text=claim.claim_text,
                max_results=5  # æ¯ä¸ªè®ºæ–­æœ€å¤š5ä¸ªç»“æœ
            )
            
            # è½¬æ¢æœç´¢ç»“æœæ ¼å¼
            evidence_sources = []
            for search_result in evidence_collection.search_results:
                evidence_sources.append({
                    'title': search_result.title,
                    'url': search_result.url,
                    'snippet': search_result.snippet,
                    'source_domain': search_result.source_domain,
                    'relevance_score': search_result.relevance_score,
                    'authority_score': search_result.authority_score
                })
            
            # è®¡ç®—ç½®ä¿¡åº¦è¯„åˆ†
            if evidence_sources:
                confidence_score = sum(es.get('relevance_score', 0) for es in evidence_sources) / len(evidence_sources)
                processing_status = 'success'
                enhanced_text = claim.claim_text  # ä¸åœ¨è¿™é‡Œç”Ÿæˆå¢å¼ºæ–‡æœ¬
            else:
                confidence_score = 0.0
                processing_status = 'failed'
                enhanced_text = claim.claim_text
            
            return EvidenceResult(
                claim_id=claim.claim_id,
                claim_text=claim.claim_text,
                section_title=claim.section_title,
                search_query=search_query,
                evidence_sources=evidence_sources,
                enhanced_text=enhanced_text,
                confidence_score=confidence_score,
                processing_status=processing_status
            )
            
        except Exception as e:
            print(f"      âš ï¸ æœç´¢è®ºæ–­ '{claim.claim_id}' è¯æ®æ—¶å‡ºé”™: {str(e)}")
            return EvidenceResult(
                claim_id=claim.claim_id,
                claim_text=claim.claim_text,
                section_title=claim.section_title,
                search_query='',
                evidence_sources=[],
                enhanced_text=claim.claim_text,
                confidence_score=0.0,
                processing_status='failed'
            )
    
    
    def _enhance_content_with_evidence(self, original_content: str, evidence_results: List[EvidenceResult]) -> str:
        """
        å°†è¯æ®æ•´åˆåˆ°åŸå§‹å†…å®¹ä¸­ï¼Œç”Ÿæˆå¢å¼ºç‰ˆæœ¬
        
        Args:
            original_content: åŸå§‹ç« èŠ‚å†…å®¹
            evidence_results: è¯æ®æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            str: å¢å¼ºåçš„å†…å®¹
        """
        # ç­›é€‰æœ‰æ•ˆçš„è¯æ®ç»“æœ
        valid_evidence = [er for er in evidence_results if er.processing_status == 'success' and er.evidence_sources]
        
        if not valid_evidence:
            return original_content
        
        try:
            # æ„å»ºè¯æ®å¢å¼ºæç¤º
            evidence_text = []
            for er in valid_evidence:
                if er.evidence_sources:
                    sources_text = "\n".join([f"- {source['snippet']} (æ¥æº: {source['source_domain']})" 
                                            for source in er.evidence_sources[:3]])  # åªä½¿ç”¨å‰3ä¸ªè¯æ®
                    evidence_text.append(f"""
è®ºæ–­: {er.claim_text}
æœç´¢åˆ°çš„è¯æ®:
{sources_text}
""")
            
            if not evidence_text:
                return original_content
            
            evidence_summary = "\n".join(evidence_text)
            
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯å†™ä½œä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„è¯æ®ï¼Œå¯¹ä»¥ä¸‹ç« èŠ‚å†…å®¹è¿›è¡Œé€‚å½“çš„å¢å¼ºå’Œæ”¹è¿›ã€‚

åŸå§‹å†…å®¹:
{original_content}

è¯æ®ä¿¡æ¯:
{evidence_summary}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ä¿®æ”¹ç« èŠ‚å†…å®¹ï¼š

1. **ä¿æŒåŸæœ‰çš„ç« èŠ‚ç»“æ„å’Œæ ¼å¼**
2. **åŸºäºè¯æ®ä¿¡æ¯ï¼Œå¯¹ç›¸å…³è®ºæ–­è¿›è¡Œé€‚å½“çš„å¢å¼º**ï¼š
   - å¯ä»¥æ·»åŠ å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹æˆ–å¼•ç”¨
   - å¯ä»¥è¡¥å……ç›¸å…³çš„æ”¿ç­–æ³•è§„æˆ–æ ‡å‡†
   - ç”¨ç²—ä½“æ ‡è®°æ–°å¢æˆ–ä¿®æ”¹çš„å†…å®¹ï¼š**æ–°å¢å†…å®¹**
3. **ä¿æŒå­¦æœ¯å†™ä½œçš„ä¸¥è°¨æ€§å’Œå®¢è§‚æ€§**
4. **ç¡®ä¿ä¿®æ”¹åçš„å†…å®¹ä¸åŸæ–‡é£æ ¼ä¸€è‡´**
5. **ä¸è¦åˆ é™¤åŸæœ‰çš„é‡è¦ä¿¡æ¯**

è¯·ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´ç« èŠ‚å†…å®¹ï¼š"""

            # è°ƒç”¨LLMç”Ÿæˆå¢å¼ºå†…å®¹
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            enhanced_content = response.choices[0].message.content.strip()
            
            # åŸºæœ¬æ¸…ç†å’ŒéªŒè¯
            if enhanced_content and len(enhanced_content) > 50:
                return enhanced_content
            else:
                return original_content
                
        except Exception as e:
            print(f"    âš ï¸ ç”Ÿæˆå¢å¼ºå†…å®¹æ—¶å‡ºé”™: {str(e)}")
            return original_content

    def _clean_json_text(self, text: str) -> str:
        """æ¸…ç†JSONæ–‡æœ¬ä¸­çš„æ— æ•ˆå­—ç¬¦"""
        if not text:
            return text
        
        # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼‰
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        text = text.replace('\ufeff', '')
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        text = re.sub(r'```json\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'```\s*$', '', text, flags=re.MULTILINE)
        text = re.sub(r'```.*?$', '', text, flags=re.MULTILINE)
        
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€æ–‡æœ¬
        text = re.sub(r'^[^{]*?(?={)', '', text)
        
        # ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
        text = re.sub(r"'([^'\\]*(?:\\.[^'\\]*)*)':", r'"\1":', text)
        text = re.sub(r":\s*'([^'\\]*(?:\\.[^'\\]*)*)'", r': "\1"', text)
        
        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
        text = re.sub(r',\s*}', '}', text)  # ç§»é™¤å¤šä½™çš„é€—å·
        text = re.sub(r',\s*]', ']', text)  # ç§»é™¤æ•°ç»„ä¸­å¤šä½™çš„é€—å·
        
        # å°è¯•æå–å®Œæ•´çš„JSONå¯¹è±¡
        try:
            brace_count = 0
            start_pos = -1
            end_pos = -1
            
            for i, char in enumerate(text):
                if char == '{':
                    if start_pos == -1:
                        start_pos = i
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and start_pos != -1:
                        end_pos = i
                        break
            
            if start_pos != -1 and end_pos != -1:
                text = text[start_pos:end_pos+1]
        except:
            pass
        
        return text.strip()
    

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª è¯æ®æ£€æµ‹å™¨æµ‹è¯•")
    
    detector = EvidenceDetector()
    
    # åˆ›å»ºæµ‹è¯•ç« èŠ‚
    test_section_title = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
    test_section_content = """
äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜åŒ»ç–—è¡Œä¸šã€‚AIåœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä¸­çš„å‡†ç¡®ç‡å·²ç»è¶…è¿‡90%ã€‚

æ·±åº¦å­¦ä¹ ç®—æ³•èƒ½å¤Ÿè¯†åˆ«Xå…‰ç‰‡ã€CTæ‰«æå’ŒMRIå›¾åƒä¸­çš„å¼‚å¸¸æƒ…å†µï¼Œå¸®åŠ©åŒ»ç”Ÿæ›´å¿«é€Ÿã€å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—…ã€‚
æœºå™¨å­¦ä¹ ç®—æ³•åœ¨ç™Œç—‡ç—…ç†è¯Šæ–­ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å®Œæˆé€šå¸¸éœ€è¦æ•°å°æ—¶çš„åˆ†æå·¥ä½œã€‚

AIç³»ç»Ÿèƒ½å¤Ÿåˆ†ææ‚£è€…çš„åŸºå› ä¿¡æ¯ã€ç—…å²å’Œç”Ÿæ´»æ–¹å¼ï¼Œä¸ºæ¯ä¸ªæ‚£è€…åˆ¶å®šä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚
äººå·¥æ™ºèƒ½å¤§å¤§ç¼©çŸ­äº†æ–°è¯ç ”å‘å‘¨æœŸï¼Œä»ä¼ ç»Ÿçš„10-15å¹´ç¼©çŸ­åˆ°5-7å¹´ã€‚
"""
    
    # æ‰§è¡Œæµ‹è¯•
    result = detector.process_section(test_section_title, test_section_content)
    
    print(f"âœ… æµ‹è¯•å®Œæˆ")
    print(f"   çŠ¶æ€: {result['status']}")
    print(f"   æ£€æµ‹è®ºæ–­: {result['statistics']['claims_detected']} ä¸ª")
    print(f"   æ‰¾åˆ°è¯æ®: {result['statistics']['evidence_found']} æ¡")
    print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.2f} ç§’")
