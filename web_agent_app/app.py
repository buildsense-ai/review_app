#!/usr/bin/env python3
"""
è®ºæ®æ”¯æŒåº¦è¯„ä¼°ç³»ç»Ÿ FastAPI åº”ç”¨ - é‡æ„ç‰ˆ
æŒ‰ç…§æ–°çš„API v1ç»“æ„è®¾è®¡
"""

import os
import json
import time
import tempfile
import shutil
import re
from typing import Optional, Dict, Any, List
from datetime import datetime
import traceback

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

from whole_document_pipeline import WholeDocumentPipeline
from evidence_detector import UnsupportedClaim, EvidenceResult

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="è®ºæ®æ”¯æŒåº¦è¯„ä¼°ç³»ç»Ÿ",
    description="åŸºäºAIçš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œç”¨äºéªŒè¯å­¦æœ¯æ–‡æ¡£ä¸­è®ºç‚¹çš„äº‹å®æ”¯æ’‘",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
pipeline = None
processing_tasks = {}


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def generate_unified_sections(original_content: str, enhanced_content: str, 
                            evidence_analysis: Dict[str, Any]) -> Dict[str, dict]:
    """ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ç« èŠ‚ç»“æœ - ä½¿ç”¨ä¸€çº§æ ‡é¢˜åµŒå¥—äºŒçº§æ ‡é¢˜çš„ç»“æ„"""
    unified_sections = {}
    
    # è§£æåŸå§‹å†…å®¹çš„å±‚çº§ç« èŠ‚
    original_hierarchy = parse_hierarchical_sections(original_content)
    # è§£æå¢å¼ºåå†…å®¹çš„å±‚çº§ç« èŠ‚
    enhanced_hierarchy = parse_hierarchical_sections(enhanced_content) if enhanced_content else original_hierarchy
    
    # ä»è¯æ®åˆ†æä¸­æå–ç« èŠ‚ä¿¡æ¯
    section_claims = {}
    if evidence_analysis and 'claims' in evidence_analysis:
        for claim in evidence_analysis['claims']:
            section_title = claim.get('section_title', 'æœªçŸ¥ç« èŠ‚')
            if section_title not in section_claims:
                section_claims[section_title] = []
            section_claims[section_title].append(claim)
    
    # ä¸ºæ¯ä¸ªä¸€çº§æ ‡é¢˜ç”Ÿæˆç»“æœ
    for h1_title, h2_sections in original_hierarchy.items():
        unified_sections[h1_title] = {}
        
        # ä¸ºæ¯ä¸ªäºŒçº§æ ‡é¢˜ç”Ÿæˆç»“æœ
        for h2_title, original_section_content in h2_sections.items():
            enhanced_section_content = enhanced_hierarchy.get(h1_title, {}).get(h2_title, original_section_content)
            
            # ç”Ÿæˆè¯¥ç« èŠ‚çš„å»ºè®®
            suggestion = ""
            # å°è¯•å¤šç§åŒ¹é…æ–¹å¼æŸ¥æ‰¾claims
            claims = []
            for section_title, section_claims_list in section_claims.items():
                if (section_title == h2_title or 
                    section_title == f"{h1_title} {h2_title}" or
                    section_title == f"{h1_title}_{h2_title}" or
                    h2_title in section_title or
                    section_title in h2_title):
                    claims.extend(section_claims_list)
            
            if claims:
                claim_count = len(claims)
                evidence_count = sum(len(claim.get('evidence_sources', [])) for claim in claims)
                suggestion = f"æ£€æµ‹åˆ°{claim_count}ä¸ªè®ºæ–­ï¼Œæ‰¾åˆ°{evidence_count}æ¡æ”¯æ’‘è¯æ®"
            else:
                if original_section_content != enhanced_section_content:
                    suggestion = "å†…å®¹å·²å¢å¼º"
                else:
                    # è·³è¿‡æ— éœ€ä¿®æ”¹çš„ç« èŠ‚ï¼Œä¸åŒ…å«åœ¨è¾“å‡ºä¸­
                    continue
            
            # åªæœ‰æœ‰è®ºæ–­åˆ†ææˆ–å†…å®¹å˜åŒ–çš„ç« èŠ‚æ‰åŒ…å«åœ¨è¾“å‡ºä¸­
            # è®¡ç®—å­—æ•°
            word_count = len(enhanced_section_content.replace(' ', '').replace('\n', ''))
            
            # ç¡®ä¿ä¸€çº§æ ‡é¢˜å­˜åœ¨
            if h1_title not in unified_sections:
                unified_sections[h1_title] = {}
            
            unified_sections[h1_title][h2_title] = {
                "original_content": original_section_content,
                "suggestion": suggestion,
                "regenerated_content": enhanced_section_content,
                "word_count": word_count,
                "status": "success"
            }
    
    return unified_sections


def parse_hierarchical_sections(content: str) -> Dict[str, Dict[str, str]]:
    """è§£æMarkdownå†…å®¹çš„å±‚çº§ç« èŠ‚ç»“æ„"""
    hierarchy = {}
    lines = content.split('\n')
    
    current_h1 = None
    current_h2 = None
    current_content = []
    
    for line in lines:
        line_stripped = line.strip()
        
        # æ£€æµ‹ä¸€çº§æ ‡é¢˜ (# æ ‡é¢˜)
        if line_stripped.startswith('# ') and not line_stripped.startswith('## '):
            # ä¿å­˜ä¹‹å‰çš„äºŒçº§æ ‡é¢˜å†…å®¹
            if current_h1 and current_h2:
                if current_h1 not in hierarchy:
                    hierarchy[current_h1] = {}
                hierarchy[current_h1][current_h2] = '\n'.join(current_content).strip()
            
            # å¼€å§‹æ–°çš„ä¸€çº§æ ‡é¢˜
            current_h1 = line_stripped[2:].strip()
            current_h2 = None
            current_content = []
            
        # æ£€æµ‹äºŒçº§æ ‡é¢˜ (## æ ‡é¢˜)
        elif line_stripped.startswith('## '):
            # ä¿å­˜ä¹‹å‰çš„äºŒçº§æ ‡é¢˜å†…å®¹
            if current_h1 and current_h2:
                if current_h1 not in hierarchy:
                    hierarchy[current_h1] = {}
                hierarchy[current_h1][current_h2] = '\n'.join(current_content).strip()
            
            # å¼€å§‹æ–°çš„äºŒçº§æ ‡é¢˜
            if current_h1:  # ç¡®ä¿æœ‰ä¸€çº§æ ‡é¢˜
                current_h2 = line_stripped[3:].strip()
                current_content = [line]  # åŒ…å«æ ‡é¢˜è¡Œ
            else:
                # å¦‚æœæ²¡æœ‰ä¸€çº§æ ‡é¢˜ï¼Œåˆ›å»ºé»˜è®¤çš„
                current_h1 = "æ–‡æ¡£å†…å®¹"
                current_h2 = line_stripped[3:].strip()
                current_content = [line]
                
        else:
            # æ™®é€šå†…å®¹è¡Œ
            if current_h1 and current_h2:
                current_content.append(line)
            elif current_h1 and not current_h2:
                # ä¸€çº§æ ‡é¢˜ä¸‹æ²¡æœ‰äºŒçº§æ ‡é¢˜çš„å†…å®¹ï¼Œè·³è¿‡ç©ºè¡Œï¼Œç­‰å¾…äºŒçº§æ ‡é¢˜
                if line.strip():  # åªæœ‰éç©ºè¡Œæ‰åˆ›å»ºé»˜è®¤äºŒçº§æ ‡é¢˜
                    current_h2 = "æ¦‚è¿°"
                    current_content = [line]
    
    # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
    if current_h1 and current_h2:
        if current_h1 not in hierarchy:
            hierarchy[current_h1] = {}
        hierarchy[current_h1][current_h2] = '\n'.join(current_content).strip()
    
    return hierarchy

def parse_sections(content: str) -> Dict[str, str]:
    """è§£æMarkdownå†…å®¹çš„ç« èŠ‚"""
    sections = {}
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…äºŒçº§æ ‡é¢˜
    section_pattern = r'^## (.+?)$'
    lines = content.split('\n')
    
    current_section = None
    current_content = []
    
    for line in lines:
        match = re.match(section_pattern, line)
        if match:
            # ä¿å­˜å‰ä¸€ä¸ªç« èŠ‚
            if current_section:
                sections[current_section] = '\n'.join(current_content).strip()
            
            # å¼€å§‹æ–°ç« èŠ‚
            current_section = match.group(1).strip()
            current_content = [line]  # åŒ…å«æ ‡é¢˜è¡Œ
        else:
            if current_section:
                current_content.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
    if current_section:
        sections[current_section] = '\n'.join(current_content).strip()
    
    return sections

# =============================================================================
# Pydanticæ¨¡å‹å®šä¹‰
# =============================================================================

class SectionResult(BaseModel):
    """ç»Ÿä¸€çš„ç« èŠ‚ç»“æœæ ¼å¼"""
    section_title: str = Field(..., description="ç« èŠ‚æ ‡é¢˜")
    original_content: str = Field(..., description="åŸå§‹å†…å®¹")
    suggestion: str = Field(..., description="ä¿®æ”¹å»ºè®®æˆ–åˆ†æç»“æœ")
    regenerated_content: str = Field(..., description="ä¿®æ”¹åçš„å†…å®¹")
    word_count: int = Field(..., description="å­—æ•°ç»Ÿè®¡")
    status: str = Field(default="success", description="å¤„ç†çŠ¶æ€")

class SystemInfoResponse(BaseModel):
    system_name: str = "è®ºæ®æ”¯æŒåº¦è¯„ä¼°ç³»ç»Ÿ"
    version: str = "1.0.0"
    description: str = "åŸºäºAIçš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œç”¨äºéªŒè¯å­¦æœ¯æ–‡æ¡£ä¸­è®ºç‚¹çš„äº‹å®æ”¯æ’‘"
    supported_formats: List[str] = ["markdown", "json", "txt"]
    max_file_size_mb: int = 50
    features: List[str] = [
        "æ£€æµ‹ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­",
        "å¹¶è¡Œç½‘ç»œæœç´¢è¯æ®æ”¯æ’‘", 
        "ç« èŠ‚å¹¶è¡Œå¤„ç†æ¨¡å¼",
        "AIç›´æ¥ç”Ÿæˆä¿®æ”¹æ–‡æ¡£",
        "ä¸‰æ­¥éª¤å®Œæ•´æµæ°´çº¿å¤„ç†",
        "å¼‚æ­¥å¤„ç†æ”¯æŒ"
    ]

class ExtractClaimsRequest(BaseModel):
    content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    max_claims: int = Field(default=15, ge=1, le=50, description="æœ€å¤§è®ºæ–­æå–æ•°é‡")

class ExtractClaimsResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    claims: List[Dict[str, Any]] = Field(default=[], description="æå–çš„è®ºæ–­åˆ—è¡¨")
    total_claims: int = Field(default=0, description="è®ºæ–­æ€»æ•°")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class SearchEvidenceRequest(BaseModel):
    claims: List[Dict[str, Any]] = Field(..., description="éœ€è¦æœç´¢è¯æ®çš„è®ºæ–­åˆ—è¡¨")
    max_results_per_claim: int = Field(default=10, ge=1, le=20, description="æ¯ä¸ªè®ºæ–­çš„æœ€å¤§æœç´¢ç»“æœæ•°")

class SearchEvidenceResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    evidence_collections: List[Dict[str, Any]] = Field(default=[], description="è¯æ®æ”¶é›†ç»“æœ")
    total_evidence_found: int = Field(default=0, description="æ‰¾åˆ°çš„è¯æ®æ€»æ•°")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class AnalyzeEvidenceRequest(BaseModel):
    claims: List[Dict[str, Any]] = Field(..., description="è®ºæ–­åˆ—è¡¨")
    evidence_collections: List[Dict[str, Any]] = Field(..., description="è¯æ®æ”¶é›†ç»“æœ")
    original_content: Optional[str] = Field(default=None, description="åŸå§‹æ–‡æ¡£å†…å®¹")

class AnalyzeEvidenceResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    analysis_results: List[Dict[str, Any]] = Field(default=[], description="è¯æ®åˆ†æç»“æœ")
    summary: Dict[str, Any] = Field(default={}, description="åˆ†ææ‘˜è¦")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class WebSearchRequest(BaseModel):
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    max_results: int = Field(default=10, ge=1, le=20, description="æœ€å¤§æœç´¢ç»“æœæ•°")

class WebSearchResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    query: str = Field(description="æœç´¢æŸ¥è¯¢")
    search_results: List[Dict[str, Any]] = Field(default=[], description="æœç´¢ç»“æœ")
    total_results: int = Field(default=0, description="ç»“æœæ€»æ•°")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class PipelineRequest(BaseModel):
    content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    max_claims: int = Field(default=15, ge=1, le=50, description="æœ€å¤§è®ºæ–­å¤„ç†æ•°é‡")
    max_search_results: int = Field(default=10, ge=1, le=20, description="æ¯ä¸ªè®ºæ–­çš„æœ€å¤§æœç´¢ç»“æœæ•°")
    use_section_based_processing: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨ç« èŠ‚å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼ˆæ¨èï¼‰")

class PipelineResponse(BaseModel):
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    enhanced_document: Optional[str] = Field(default=None, description="å¢å¼ºåçš„æ–‡æ¡£")
    evidence_analysis: Optional[Dict[str, Any]] = Field(default=None, description="è¯æ®åˆ†æç»“æœ")
    statistics: Optional[Dict[str, Any]] = Field(default=None, description="å¤„ç†ç»Ÿè®¡")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")
    # æ–°å¢ç»Ÿä¸€æ ¼å¼çš„JSONè¾“å‡º
    unified_sections: Dict[str, SectionResult] = Field(default_factory=dict, description="ç»Ÿä¸€æ ¼å¼çš„ç« èŠ‚ç»“æœ")

class AsyncTaskResponse(BaseModel):
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="ä»»åŠ¡çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")

class TaskStatusResponse(BaseModel):
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="ä»»åŠ¡çŠ¶æ€")
    progress: Optional[str] = Field(default=None, description="è¿›åº¦ä¿¡æ¯")
    result: Optional[Dict[str, Any]] = Field(default=None, description="å¤„ç†ç»“æœ")
    created_at: Optional[str] = Field(default=None, description="åˆ›å»ºæ—¶é—´")
    completed_at: Optional[str] = Field(default=None, description="å®Œæˆæ—¶é—´")

class DocumentProcessResponse(BaseModel):
    task_id: str = Field(description="ä»»åŠ¡ID")
    status: str = Field(description="å¤„ç†çŠ¶æ€")
    message: str = Field(description="çŠ¶æ€æ¶ˆæ¯")
    processing_time: Optional[float] = Field(default=None, description="å¤„ç†æ—¶é—´")
    statistics: Optional[Dict[str, Any]] = Field(default=None, description="å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
    output_files: Optional[Dict[str, str]] = Field(default=None, description="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")
    # æ–°å¢ç»Ÿä¸€æ ¼å¼çš„JSONè¾“å‡º
    unified_sections: Dict[str, SectionResult] = Field(default_factory=dict, description="ç»Ÿä¸€æ ¼å¼çš„ç« èŠ‚ç»“æœ")

# =============================================================================
# å¯åŠ¨å’ŒåŸºç¡€ç«¯ç‚¹
# =============================================================================

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global pipeline
    try:
        print("ğŸš€ åˆå§‹åŒ–è®ºæ®æ”¯æŒåº¦è¯„ä¼°ç³»ç»Ÿ...")
        pipeline = WholeDocumentPipeline()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise

@app.get("/", response_model=SystemInfoResponse)
async def root():
    """ç³»ç»Ÿä¿¡æ¯"""
    return SystemInfoResponse()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "pipeline_ready": pipeline is not None
    }

# =============================================================================
# API v1 ç«¯ç‚¹
# =============================================================================

@app.post("/api/v1/extract-claims", response_model=ExtractClaimsResponse)
async def extract_claims(request: ExtractClaimsRequest):
    """æå–æ ¸å¿ƒè®ºç‚¹"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ” å¼€å§‹æå–ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­ï¼Œæ–‡æ¡£é•¿åº¦: {len(request.content)} å­—ç¬¦")
        
        # ä½¿ç”¨æ–°çš„evidence_detectoræ£€æµ‹è®ºæ–­
        detector = pipeline.evidence_detector
        
        # ç›´æ¥å¤„ç†å†…å®¹ï¼Œä¸éœ€è¦ä¸´æ—¶æ–‡ä»¶
        unsupported_claims = detector._detect_unsupported_claims("å®Œæ•´æ–‡æ¡£", request.content)
        
        if len(unsupported_claims) > request.max_claims:
            unsupported_claims = sorted(unsupported_claims, key=lambda x: x.confidence_level, reverse=True)[:request.max_claims]
        
        claims_data = []
        for claim in unsupported_claims:
            claims_data.append({
                "claim_id": claim.claim_id,
                "claim_text": claim.claim_text,
                "claim_type": claim.claim_type,
                "confidence_level": claim.confidence_level,
                "section_title": claim.section_title,
                "search_keywords": claim.search_keywords,
                "context": claim.context
            })
        
        processing_time = time.time() - start_time
        print(f"âœ… ç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­æå–å®Œæˆï¼Œå…±æå– {len(claims_data)} ä¸ªè®ºæ–­ï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        return ExtractClaimsResponse(
            status="success",
            message=f"æˆåŠŸæå– {len(claims_data)} ä¸ªç¼ºä¹è¯æ®æ”¯æ’‘çš„è®ºæ–­",
            claims=claims_data,
            total_claims=len(claims_data),
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"æå–å®¢è§‚æ€§è®ºæ–­æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return ExtractClaimsResponse(
            status="failed",
            message="å®¢è§‚æ€§è®ºæ–­æå–å¤±è´¥",
            processing_time=processing_time,
            error=error_msg
        )

@app.post("/api/v1/search-evidence", response_model=SearchEvidenceResponse)
async def search_evidence(request: SearchEvidenceRequest):
    """ä¸ºè®ºæ–­æœç´¢è¯æ®æ”¯æ’‘"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ” å¼€å§‹ä¸º {len(request.claims)} ä¸ªè®ºæ–­æœç´¢è¯æ®...")
        
        # è½¬æ¢è®ºæ–­æ•°æ®ä¸º UnsupportedClaim å¯¹è±¡
        claims = []
        for i, claim_data in enumerate(request.claims):
            claim = UnsupportedClaim(
                claim_id=claim_data.get('claim_id', f'claim_{i+1}'),
                claim_text=claim_data['claim_text'],
                section_title=claim_data.get('section_title', f'ä½ç½®_{i+1}'),
                claim_type=claim_data.get('claim_type', 'factual'),
                confidence_level=claim_data.get('confidence_level', 0.8),
                context=claim_data.get('context', ''),
                search_keywords=claim_data.get('search_keywords', []),
                original_position=i+1
            )
            claims.append(claim)
        
        # ä½¿ç”¨æ–°çš„evidence_detectoræ‰¹é‡æœç´¢è¯æ®
        evidence_results = pipeline.evidence_detector._batch_search_evidence(claims)
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        evidence_data = []
        total_evidence = 0
        for er in evidence_results:
            evidence_info = {
                'claim_id': er.claim_id,
                'search_query': er.search_query,
                'search_results': [
                    {
                        'title': source.get('title', ''),
                        'url': source.get('url', ''),
                        'snippet': source.get('snippet', ''),
                        'source_domain': source.get('source_domain', ''),
                        'relevance_score': source.get('relevance_score', 0.0),
                        'authority_score': source.get('authority_score', 0.0)
                    } for source in er.evidence_sources
                ],
                'total_results': len(er.evidence_sources),
                'processing_status': er.processing_status,
                'confidence_score': er.confidence_score
            }
            evidence_data.append(evidence_info)
            total_evidence += len(er.evidence_sources)
        
        processing_time = time.time() - start_time
        print(f"âœ… è¯æ®æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_evidence} æ¡è¯æ®ï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        return SearchEvidenceResponse(
            status="success",
            message=f"æˆåŠŸä¸º {len(request.claims)} ä¸ªè®ºæ–­æœç´¢åˆ° {total_evidence} æ¡è¯æ®",
            evidence_collections=evidence_data,
            total_evidence_found=total_evidence,
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"æœç´¢è¯æ®æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return SearchEvidenceResponse(
            status="failed",
            message="è¯æ®æœç´¢å¤±è´¥",
            processing_time=processing_time,
            error=error_msg
        )

@app.post("/api/v1/analyze-evidence", response_model=AnalyzeEvidenceResponse)
async def analyze_evidence(request: AnalyzeEvidenceRequest):
    """åˆ†æè¯æ®å¹¶ç”Ÿæˆå¢å¼ºå†…å®¹"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ¤– å¼€å§‹åˆ†æ {len(request.claims)} ä¸ªè®ºæ–­çš„è¯æ®...")
        
        # è½¬æ¢è®ºæ–­æ•°æ®
        claims = []
        for i, claim_data in enumerate(request.claims):
            claim = UnsupportedClaim(
                claim_id=claim_data.get('claim_id', f'claim_{i+1}'),
                claim_text=claim_data['claim_text'],
                section_title=claim_data.get('section_title', f'ä½ç½®_{i+1}'),
                claim_type=claim_data.get('claim_type', 'factual'),
                confidence_level=claim_data.get('confidence_level', 0.8),
                context=claim_data.get('context', ''),
                search_keywords=claim_data.get('search_keywords', []),
                original_position=i+1
            )
            claims.append(claim)
        
        # è½¬æ¢è¯æ®æ•°æ®ä¸ºEvidenceResultæ ¼å¼
        evidence_results = []
        for ec_data in request.evidence_collections:
            er = EvidenceResult(
                claim_id=ec_data['claim_id'],
                claim_text=next((c.claim_text for c in claims if c.claim_id == ec_data['claim_id']), ''),
                section_title=next((c.section_title for c in claims if c.claim_id == ec_data['claim_id']), ''),
                search_query=ec_data.get('search_query', ''),
                evidence_sources=ec_data.get('search_results', []),
                enhanced_text='',  # å°†ç”±æ–‡æ¡£ç”Ÿæˆå™¨å¡«å……
                confidence_score=ec_data.get('confidence_score', 0.0),
                processing_status=ec_data.get('processing_status', 'success')
            )
            evidence_results.append(er)
        
        # ä½¿ç”¨æ–‡æ¡£ç”Ÿæˆå™¨ç”Ÿæˆå¢å¼ºå†…å®¹
        if request.original_content:
            enhanced_content = pipeline.document_generator.generate_section_with_evidence(
                section_title="å®Œæ•´æ–‡æ¡£",
                original_content=request.original_content,
                evidence_results=evidence_results
            )
        else:
            enhanced_content = "æ— åŸå§‹å†…å®¹æä¾›"
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        analysis_data = []
        for er in evidence_results:
            analysis_info = {
                'claim_id': er.claim_id,
                'claim_text': er.claim_text,
                'section_title': er.section_title,
                'search_query': er.search_query,
                'evidence_sources': er.evidence_sources,
                'confidence_score': er.confidence_score,
                'processing_status': er.processing_status
            }
            analysis_data.append(analysis_info)
        
        processing_time = time.time() - start_time
        print(f"âœ… è¯æ®åˆ†æå’Œæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        summary = {
            'total_claims_analyzed': len(evidence_results),
            'successful_claims': sum(1 for er in evidence_results if er.processing_status == 'success'),
            'total_evidence_sources': sum(len(er.evidence_sources) for er in evidence_results),
            'enhanced_content_generated': bool(enhanced_content and enhanced_content != "æ— åŸå§‹å†…å®¹æä¾›")
        }
        
        return AnalyzeEvidenceResponse(
            status="success",
            message=f"æˆåŠŸåˆ†æ {len(evidence_results)} ä¸ªè®ºæ–­çš„è¯æ®å¹¶ç”Ÿæˆå¢å¼ºå†…å®¹",
            analysis_results=analysis_data,
            summary=summary,
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"åˆ†æè¯æ®æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return AnalyzeEvidenceResponse(
            status="failed",
            message="è¯æ®åˆ†æå¤±è´¥",
            processing_time=processing_time,
            error=error_msg
        )

@app.post("/api/v1/websearch", response_model=WebSearchResponse)
async def websearch(request: WebSearchRequest):
    """ç½‘ç»œæœç´¢æ¥å£"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    try:
        print(f"ğŸ” æ‰§è¡Œç½‘ç»œæœç´¢: {request.query}")
        
        # ä½¿ç”¨webæœç´¢ä»£ç†æ‰§è¡Œæœç´¢
        from web_search_agent import EvidenceCollection
        evidence = pipeline.web_search_agent.search_evidence_for_claim(
            claim_id="websearch_query",
            search_keywords=[request.query],
            claim_text=request.query,
            max_results=request.max_results
        )
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        search_results = []
        for sr in evidence.search_results:
            result_info = {
                'title': sr.title,
                'url': sr.url,
                'snippet': sr.snippet,
                'source_domain': sr.source_domain,
                'relevance_score': sr.relevance_score,
                'authority_score': sr.authority_score
            }
            search_results.append(result_info)
        
        processing_time = time.time() - start_time
        print(f"âœ… ç½‘ç»œæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time:.1f}ç§’")
        
        return WebSearchResponse(
            status="success",
            message=f"æˆåŠŸæœç´¢åˆ° {len(search_results)} ä¸ªç»“æœ",
            query=request.query,
            search_results=search_results,
            total_results=len(search_results),
            processing_time=processing_time
        )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"ç½‘ç»œæœç´¢æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return WebSearchResponse(
            status="failed",
            message="ç½‘ç»œæœç´¢å¤±è´¥",
            query=request.query,
            processing_time=processing_time,
            error=error_msg
        )

@app.post("/api/v1/pipeline", response_model=PipelineResponse)
async def pipeline_sync(request: PipelineRequest):
    """å®Œæ•´æµæ°´çº¿å¤„ç†ï¼ˆåŒæ­¥ï¼‰"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    start_time = time.time()
    
    max_content_length = 1024 * 1024  # 1MB
    if len(request.content.encode('utf-8')) > max_content_length:
        raise HTTPException(status_code=413, detail="æ–‡æ¡£å†…å®¹è¿‡å¤§ï¼Œè¶…è¿‡1MBé™åˆ¶")
    
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, "document.md")
    
    try:
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(request.content)
        
        print(f"ğŸ”„ å¼€å§‹å®Œæ•´æµæ°´çº¿å¤„ç†ï¼Œæ–‡æ¡£é•¿åº¦: {len(request.content)} å­—ç¬¦")
        
        result = pipeline.process_whole_document(
            document_path=temp_file_path,
            max_claims=request.max_claims,
            max_search_results=request.max_search_results,
            use_section_based_processing=request.use_section_based_processing
        )
        
        processing_time = time.time() - start_time
        
        if result.get('status') == 'success':
            output_files = result.get('output_files', {})
            
            enhanced_document = request.content
            if 'enhanced_document' in output_files and os.path.exists(output_files['enhanced_document']):
                try:
                    with open(output_files['enhanced_document'], 'r', encoding='utf-8') as f:
                        enhanced_document = f.read()
                except Exception as e:
                    print(f"âš ï¸ è¯»å–å¢å¼ºæ–‡æ¡£å¤±è´¥: {str(e)}")
            
            evidence_analysis = {}
            if 'evidence_analysis' in output_files and os.path.exists(output_files['evidence_analysis']):
                try:
                    with open(output_files['evidence_analysis'], 'r', encoding='utf-8') as f:
                        evidence_analysis = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–è¯æ®åˆ†æå¤±è´¥: {str(e)}")
            
            print(f"âœ… å®Œæ•´æµæ°´çº¿å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.1f}ç§’")
            
            # ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ç« èŠ‚ç»“æœ
            unified_sections = generate_unified_sections(
                request.content,
                enhanced_document,
                evidence_analysis
            )
            
            return PipelineResponse(
                status="success",
                message="å®Œæ•´æµæ°´çº¿å¤„ç†æˆåŠŸå®Œæˆ",
                enhanced_document=enhanced_document,
                evidence_analysis=evidence_analysis,
                statistics=result.get('statistics', {}),
                processing_time=processing_time,
                unified_sections=unified_sections
            )
        else:
            error_msg = result.get('error', 'å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯')
            print(f"âŒ æµæ°´çº¿å¤„ç†å¤±è´¥: {error_msg}")
            
            return PipelineResponse(
                status="failed",
                message="æµæ°´çº¿å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸæ–‡æ¡£",
                enhanced_document=request.content,
                evidence_analysis={},
                processing_time=processing_time,
                error=error_msg
            )
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"æµæ°´çº¿å¤„ç†æ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        
        return PipelineResponse(
            status="failed",
            message="æµæ°´çº¿å¤„ç†å¼‚å¸¸ï¼Œè¿”å›åŸæ–‡æ¡£",
            enhanced_document=request.content,
            evidence_analysis={},
            processing_time=processing_time,
            error=error_msg
        )
    
    finally:
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {str(e)}")

@app.post("/api/v1/upload", response_model=AsyncTaskResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    max_claims: int = Form(default=15),
    max_search_results: int = Form(default=10),
    use_section_based_processing: bool = Form(default=True)
):
    """æ–‡ä»¶ä¸Šä¼ å¤„ç†ï¼ˆå¼‚æ­¥ï¼‰"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    allowed_extensions = {'.md', '.json', '.txt'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(allowed_extensions)}"
        )
    
    max_size = 50 * 1024 * 1024  # 50MB
    content = await file.read()
    if len(content) > max_size:
        raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡50MBé™åˆ¶")
    
    task_id = f"task_{int(time.time())}_{hash(file.filename) % 10000}"
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, file.filename)
    
    try:
        with open(temp_file_path, 'wb') as f:
            f.write(content)
        
        processing_tasks[task_id] = {
            "status": "processing",
            "start_time": time.time(),
            "progress": "æ–‡ä»¶å·²ä¸Šä¼ ï¼Œå¼€å§‹å¤„ç†...",
            "temp_dir": temp_dir,
            "created_at": datetime.now().isoformat()
        }
        
        background_tasks.add_task(
            process_document_background,
            task_id,
            temp_file_path,
            max_claims,
            max_search_results,
            use_section_based_processing
        )
        
        return AsyncTaskResponse(
            task_id=task_id,
            status="processing",
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹å¤„ç†"
        )
        
    except Exception as e:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.post("/api/v1/pipeline-async", response_model=AsyncTaskResponse)
async def pipeline_async(
    background_tasks: BackgroundTasks,
    request: PipelineRequest
):
    """å¼‚æ­¥æµæ°´çº¿å¤„ç†"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    max_content_length = 1024 * 1024  # 1MB
    if len(request.content.encode('utf-8')) > max_content_length:
        raise HTTPException(status_code=413, detail="æ–‡æ¡£å†…å®¹è¿‡å¤§ï¼Œè¶…è¿‡1MBé™åˆ¶")
    
    task_id = f"task_{int(time.time())}_{hash(request.content[:100]) % 10000}"
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, "document.md")
    
    try:
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(request.content)
        
        processing_tasks[task_id] = {
            "status": "processing",
            "start_time": time.time(),
            "progress": "å†…å®¹å·²æ¥æ”¶ï¼Œå¼€å§‹å¤„ç†...",
            "temp_dir": temp_dir,
            "created_at": datetime.now().isoformat()
        }
        
        background_tasks.add_task(
            process_document_background,
            task_id,
            temp_file_path,
            request.max_claims,
            request.max_search_results,
            request.use_section_based_processing
        )
        
        return AsyncTaskResponse(
            task_id=task_id,
            status="processing",
            message="å†…å®¹æ¥æ”¶æˆåŠŸï¼Œå¼€å§‹å¼‚æ­¥å¤„ç†"
        )
        
    except Exception as e:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.get("/api/v1/task/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = processing_tasks[task_id]
    
    result = None
    if task_info["status"] == "completed" and "result" in task_info:
        task_result = task_info["result"]
        result = {
            "status": task_result.status,
            "message": task_result.message,
            "processing_time": task_result.processing_time,
            "statistics": task_result.statistics,
            "output_files": task_result.output_files,
            "error": task_result.error,
            "unified_sections": {k: v.dict() for k, v in task_result.unified_sections.items()} if task_result.unified_sections else {}
        }
    elif task_info["status"] == "failed" and "result" in task_info:
        task_result = task_info["result"]
        result = {
            "status": task_result.status,
            "message": task_result.message,
            "error": task_result.error
        }
    
    return TaskStatusResponse(
        task_id=task_id,
        status=task_info["status"],
        progress=task_info.get("progress"),
        result=result,
        created_at=task_info.get("created_at"),
        completed_at=task_info.get("completed_at")
    )

@app.get("/api/v1/download/{task_id}")
async def download_task_result(task_id: str, file_type: str = "enhanced_document"):
    """ä¸‹è½½å¤„ç†ç»“æœ"""
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = processing_tasks[task_id]
    
    if task_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")
    
    result = task_info.get("result")
    if not result or not result.output_files:
        raise HTTPException(status_code=404, detail="ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
    
    file_mapping = {
        "enhanced_document": "enhanced_document",
        "evidence_analysis": "evidence_analysis",
        "document": "enhanced_document",
        "analysis": "evidence_analysis"
    }
    
    if file_type not in file_mapping:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„æ–‡ä»¶ç±»å‹: {file_type}")
    
    file_path = result.output_files.get(file_mapping[file_type])
    if not file_path or not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    if file_type in ["enhanced_document", "document"]:
        filename = f"enhanced_document_{task_id}.md"
        media_type = "text/markdown"
    else:
        filename = f"evidence_analysis_{task_id}.json"
        media_type = "application/json"
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type=media_type
    )

# =============================================================================
# åå°å¤„ç†å‡½æ•°å’Œå¼‚å¸¸å¤„ç†
# =============================================================================

async def process_document_background(
    task_id: str,
    document_path: str,
    max_claims: int,
    max_search_results: int,
    use_section_based_processing: bool = False
):
    """åå°å¤„ç†æ–‡æ¡£çš„å‡½æ•°"""
    try:
        processing_tasks[task_id]["progress"] = "æ­£åœ¨æ£€æµ‹è®ºæ–­..."
        
        result = pipeline.process_whole_document(
            document_path=document_path,
            max_claims=max_claims,
            max_search_results=max_search_results,
            use_section_based_processing=use_section_based_processing
        )
        
        if result['status'] == 'success':
            # è¯»å–åŸå§‹å†…å®¹å’Œå¢å¼ºåçš„å†…å®¹æ¥ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„JSON
            original_content = ""
            enhanced_content = ""
            evidence_analysis = {}
            
            try:
                # è¯»å–åŸå§‹æ–‡æ¡£å†…å®¹
                with open(document_path, 'r', encoding='utf-8') as f:
                    original_content = f.read()
                
                # è¯»å–å¢å¼ºåçš„æ–‡æ¡£å†…å®¹
                output_files = result.get('output_files', {})
                if 'enhanced_document' in output_files and os.path.exists(output_files['enhanced_document']):
                    with open(output_files['enhanced_document'], 'r', encoding='utf-8') as f:
                        enhanced_content = f.read()
                else:
                    enhanced_content = original_content
                
                # è¯»å–è¯æ®åˆ†æç»“æœ
                if 'evidence_analysis' in output_files and os.path.exists(output_files['evidence_analysis']):
                    with open(output_files['evidence_analysis'], 'r', encoding='utf-8') as f:
                        evidence_analysis = json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
            
            # ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ç« èŠ‚ç»“æœ
            unified_sections = generate_unified_sections(
                original_content,
                enhanced_content,
                evidence_analysis
            )
            
            # ç”Ÿæˆä¸¤ä¸ªè¾“å‡ºæ–‡ä»¶
            import os
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ç”Ÿæˆunified_sections JSONæ–‡ä»¶
            results_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "router", "outputs", "web_evidence")
            os.makedirs(results_dir, exist_ok=True)
            unified_sections_file = os.path.join(results_dir, f"unified_sections_{timestamp}.json")
            with open(unified_sections_file, 'w', encoding='utf-8') as f:
                json.dump(unified_sections, f, ensure_ascii=False, indent=2)
            
            # 2. ç”Ÿæˆå¢å¼ºåçš„markdownæ–‡ä»¶
            enhanced_md_file = os.path.join(results_dir, f"enhanced_content_{task_id}.md")
            with open(enhanced_md_file, 'w', encoding='utf-8') as f:
                f.write(enhanced_content)
            
            # æ„å»ºç®€åŒ–çš„ç»“æœ - åªè¿”å›æ–‡ä»¶è·¯å¾„å’ŒåŸºæœ¬ä¿¡æ¯
            unified_result = {
                "unified_sections_file": unified_sections_file,
                "optimized_content_file": enhanced_md_file,
                "processing_time": result['processing_time'],
                "sections_count": len(unified_sections),
                "service_type": "web_agent",
                "message": f"å·²ç”Ÿæˆ2ä¸ªæ–‡ä»¶: {os.path.basename(unified_sections_file)}, {os.path.basename(enhanced_md_file)}"
            }
            
            processing_tasks[task_id].update({
                "status": "completed",
                "progress": "å¤„ç†å®Œæˆ",
                "completed_at": datetime.now().isoformat(),
                "result": unified_result
            })
        else:
            processing_tasks[task_id].update({
                "status": "failed",
                "progress": "å¤„ç†å¤±è´¥",
                "completed_at": datetime.now().isoformat(),
                "result": DocumentProcessResponse(
                    task_id=task_id,
                    status="failed",
                    message="æ–‡æ¡£å¤„ç†å¤±è´¥",
                    error=result.get('error', 'æœªçŸ¥é”™è¯¯')
                )
            })
    
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}"
        print(f"âŒ ä»»åŠ¡ {task_id} å¤„ç†å¤±è´¥: {error_msg}")
        traceback.print_exc()
        
        processing_tasks[task_id].update({
            "status": "failed",
            "progress": "å¤„ç†å¼‚å¸¸",
            "completed_at": datetime.now().isoformat(),
            "result": DocumentProcessResponse(
                task_id=task_id,
                status="failed",
                message="æ–‡æ¡£å¤„ç†å¼‚å¸¸",
                error=error_msg
            )
        })

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    print(f"âŒ å…¨å±€å¼‚å¸¸: {str(exc)}")
    traceback.print_exc()
    
    return JSONResponse(
        status_code=500,
        content={
            "error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
            "detail": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
