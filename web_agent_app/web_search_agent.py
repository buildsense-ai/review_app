"""
Webæœç´¢ä»£ç†
ä¸ºå®¢è§‚æ€§è®ºæ–­æœç´¢æƒå¨è¯æ®æ”¯æ’‘
"""

import json
import time
import requests
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
from urllib.parse import quote_plus
import config

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    url: str
    snippet: str
    source_domain: str
    relevance_score: float
    authority_score: float  # ä¿¡æºæƒå¨æ€§è¯„åˆ†

@dataclass
class EvidenceCollection:
    """è¯æ®æ”¶é›†ç»“æœ"""
    claim_id: str
    search_query: str
    search_results: List[SearchResult]
    total_results: int
    search_timestamp: str
    summary: str  # AIç”Ÿæˆçš„è¯æ®æ‘˜è¦

class WebSearchAgent:
    """Webæœç´¢ä»£ç†"""
    
    def __init__(self):
        # æƒå¨ä¿¡æºåŸŸååˆ—è¡¨
        self.authority_domains = {
            # å­¦æœ¯æœŸåˆŠå’Œæ•°æ®åº“
            'scholar.google.com': 0.95,
            'pubmed.ncbi.nlm.nih.gov': 0.95,
            'ieee.org': 0.9,
            'acm.org': 0.9,
            'springer.com': 0.9,
            'elsevier.com': 0.9,
            'nature.com': 0.95,
            'science.org': 0.95,
            'cell.com': 0.9,
            
            # æ”¿åºœå’Œå›½é™…ç»„ç»‡
            'gov': 0.9,  # æ”¿åºœåŸŸå
            'edu': 0.85,  # æ•™è‚²æœºæ„
            'who.int': 0.95,
            'worldbank.org': 0.9,
            'un.org': 0.9,
            'oecd.org': 0.9,
            
            # çŸ¥ååª’ä½“å’Œæ™ºåº“
            'reuters.com': 0.8,
            'bbc.com': 0.8,
            'economist.com': 0.85,
            'ft.com': 0.85,
            'wsj.com': 0.8,
            'nytimes.com': 0.75,
            'brookings.edu': 0.85,
            
            # ä¸­æ–‡æƒå¨æº
            'cnki.net': 0.9,
            'wanfangdata.com.cn': 0.85,
            'cas.cn': 0.9,
            'xinhuanet.com': 0.7,
            'people.com.cn': 0.7,
        }
    
    def search_evidence_for_claim(self, claim_id: str, search_keywords: List[str], 
                                claim_text: str, max_results: int = 10) -> EvidenceCollection:
        """ä¸ºç‰¹å®šè®ºæ–­æœç´¢è¯æ®"""
        print(f"ğŸ” ä¸ºè®ºæ–­ {claim_id} æœç´¢è¯æ®...")
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = self._build_search_query(search_keywords, claim_text)
        print(f"  ğŸ“ æœç´¢æŸ¥è¯¢: {search_query}")
        
        # æ‰§è¡Œæœç´¢
        search_results = self._perform_web_search(search_query, max_results)
        
        # è¯„ä¼°ç»“æœæƒå¨æ€§å’Œç›¸å…³æ€§
        evaluated_results = self._evaluate_search_results(search_results, claim_text)
        
        # ç”Ÿæˆè¯æ®æ‘˜è¦
        summary = self._generate_evidence_summary(claim_text, evaluated_results)
        
        evidence_collection = EvidenceCollection(
            claim_id=claim_id,
            search_query=search_query,
            search_results=evaluated_results,
            total_results=len(evaluated_results),
            search_timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            summary=summary
        )
        
        print(f"  âœ… æ‰¾åˆ° {len(evaluated_results)} ä¸ªç›¸å…³ç»“æœ")
        return evidence_collection
    
    def _build_search_query(self, keywords: List[str], claim_text: str) -> str:
        """æ„å»ºä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢"""
        # åŸºç¡€å…³é”®è¯
        base_query = " ".join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
        
        # æ·»åŠ å­¦æœ¯æœç´¢ä¿®é¥°ç¬¦
        academic_modifiers = [
            "research", "study", "analysis", "report", "data", 
            "statistics", "evidence", "findings"
        ]
        
        # æ ¹æ®è®ºæ–­ç±»å‹æ·»åŠ ç‰¹å®šä¿®é¥°ç¬¦
        if any(word in claim_text.lower() for word in ['increase', 'decrease', 'trend', 'growth']):
            base_query += " statistics data trend"
        elif any(word in claim_text.lower() for word in ['cause', 'effect', 'impact', 'influence']):
            base_query += " causal relationship impact study"
        elif any(word in claim_text.lower() for word in ['compare', 'versus', 'than', 'better']):
            base_query += " comparison analysis study"
        
        return base_query
    
    def _perform_web_search(self, query: str, max_results: int) -> List[Dict]:
        """æ‰§è¡ŒWebæœç´¢"""
        search_results = []
        
        try:
            # ä½¿ç”¨è‡ªå®šä¹‰æœç´¢API
            custom_results = self._search_custom_api(query, max_results)
            if custom_results:
                search_results.extend(custom_results)
                print(f"  âœ… ä½¿ç”¨è‡ªå®šä¹‰æœç´¢APIè·å¾— {len(custom_results)} ä¸ªç»“æœ")
            else:
                print("âš ï¸ è‡ªå®šä¹‰æœç´¢APIæœªè¿”å›ç»“æœ")
            
        except Exception as e:
            print(f"âš ï¸ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            # å¦‚æœAPIå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœè€Œä¸æ˜¯æ¨¡æ‹Ÿç»“æœ
            print("âŒ æœç´¢å¤±è´¥ï¼Œæ— æ³•è·å–è¯æ®æ”¯æ’‘")
        
        return search_results[:max_results]
    
    def _search_custom_api(self, query: str, max_results: int) -> List[Dict]:
        """ä½¿ç”¨åŒäº‹çš„è‡ªå®šä¹‰æœç´¢API"""
        try:
            # ä»é…ç½®æ–‡ä»¶è·å–APIè®¾ç½®
            api_url = getattr(config, 'CUSTOM_SEARCH_API_URL', "http://43.139.19.144:8005/search")
            engines = getattr(config, 'CUSTOM_SEARCH_ENGINES', ["serp"])
            timeout = getattr(config, 'CUSTOM_SEARCH_TIMEOUT', 30)
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "query": query,
                "engines": engines
            }
            
            # å‘é€POSTè¯·æ±‚
            response = requests.post(
                api_url, 
                json=request_data,
                headers={"Content-Type": "application/json"},
                timeout=timeout
            )
            response.raise_for_status()
            
            # è§£æå“åº”
            data = response.json()
            
            results = []
            for item in data.get('items', [])[:max_results]:
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('link', ''),
                    'snippet': item.get('content', '')[:500],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                    'source': f"CustomAPI-{item.get('engine', 'unknown')}"
                })
            
            print(f"  ğŸ” è‡ªå®šä¹‰APIè¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except requests.exceptions.Timeout:
            print(f"âš ï¸ è‡ªå®šä¹‰æœç´¢APIè¶…æ—¶")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ è‡ªå®šä¹‰æœç´¢APIè¯·æ±‚å¤±è´¥: {str(e)}")
            return []
        except Exception as e:
            print(f"âš ï¸ è‡ªå®šä¹‰æœç´¢APIè§£æå¤±è´¥: {str(e)}")
            return []
    
    
    def _evaluate_search_results(self, search_results: List[Dict], claim_text: str) -> List[SearchResult]:
        """è¯„ä¼°æœç´¢ç»“æœçš„æƒå¨æ€§å’Œç›¸å…³æ€§"""
        evaluated_results = []
        
        for result in search_results:
            # è®¡ç®—æƒå¨æ€§è¯„åˆ†
            authority_score = self._calculate_authority_score(result['url'])
            
            # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
            relevance_score = self._calculate_relevance_score(
                result.get('title', '') + ' ' + result.get('snippet', ''),
                claim_text
            )
            
            # æå–åŸŸå
            try:
                from urllib.parse import urlparse
                domain = urlparse(result['url']).netloc.lower()
            except:
                domain = 'unknown'
            
            search_result = SearchResult(
                title=result.get('title', ''),
                url=result.get('url', ''),
                snippet=result.get('snippet', ''),
                source_domain=domain,
                relevance_score=relevance_score,
                authority_score=authority_score
            )
            
            evaluated_results.append(search_result)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åºï¼ˆæƒå¨æ€§ * 0.6 + ç›¸å…³æ€§ * 0.4ï¼‰
        evaluated_results.sort(
            key=lambda x: x.authority_score * 0.6 + x.relevance_score * 0.4,
            reverse=True
        )
        
        return evaluated_results
    
    def _calculate_authority_score(self, url: str) -> float:
        """è®¡ç®—ä¿¡æºæƒå¨æ€§è¯„åˆ†"""
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc.lower()
            
            # æ£€æŸ¥å®Œå…¨åŒ¹é…
            if domain in self.authority_domains:
                return self.authority_domains[domain]
            
            # æ£€æŸ¥åŸŸååç¼€åŒ¹é…
            for auth_domain, score in self.authority_domains.items():
                if domain.endswith(auth_domain):
                    return score
            
            # æ£€æŸ¥ç‰¹æ®ŠåŸŸåç±»å‹
            if domain.endswith('.gov'):
                return 0.9
            elif domain.endswith('.edu'):
                return 0.85
            elif domain.endswith('.org'):
                return 0.7
            elif 'university' in domain or 'college' in domain:
                return 0.8
            elif 'research' in domain or 'institute' in domain:
                return 0.75
            else:
                return 0.5  # é»˜è®¤è¯„åˆ†
                
        except:
            return 0.3  # è§£æå¤±è´¥çš„é»˜è®¤è¯„åˆ†
    
    def _calculate_relevance_score(self, text: str, claim_text: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸å…³æ€§è¯„åˆ†"""
        if not text or not claim_text:
            return 0.0
        
        text_lower = text.lower()
        claim_lower = claim_text.lower()
        
        # æå–å…³é”®è¯
        claim_words = set(claim_lower.split())
        text_words = set(text_lower.split())
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        common_words = claim_words.intersection(text_words)
        if len(claim_words) == 0:
            return 0.0
        
        word_overlap = len(common_words) / len(claim_words)
        
        # æ£€æŸ¥é‡è¦çŸ­è¯­åŒ¹é…
        important_phrases = []
        if len(claim_text) > 20:
            # æå–å¯èƒ½çš„é‡è¦çŸ­è¯­ï¼ˆç®€åŒ–å®ç°ï¼‰
            words = claim_text.split()
            for i in range(len(words) - 1):
                phrase = f"{words[i]} {words[i+1]}".lower()
                if phrase in text_lower:
                    important_phrases.append(phrase)
        
        phrase_bonus = min(len(important_phrases) * 0.2, 0.4)
        
        return min(word_overlap + phrase_bonus, 1.0)
    
    def _generate_evidence_summary(self, claim_text: str, search_results: List[SearchResult]) -> str:
        """ç”Ÿæˆè¯æ®æ‘˜è¦"""
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³è¯æ®æ”¯æ’‘ã€‚"
        
        # é€‰æ‹©å‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        top_results = search_results[:3]
        
        summary_parts = []
        summary_parts.append(f"é’ˆå¯¹è®ºæ–­ã€Œ{claim_text}ã€ï¼Œæ‰¾åˆ°ä»¥ä¸‹è¯æ®æ”¯æ’‘ï¼š\n")
        
        for i, result in enumerate(top_results, 1):
            authority_level = "é«˜" if result.authority_score >= 0.8 else "ä¸­" if result.authority_score >= 0.6 else "ä½"
            relevance_level = "é«˜" if result.relevance_score >= 0.7 else "ä¸­" if result.relevance_score >= 0.4 else "ä½"
            
            summary_parts.append(
                f"{i}. ã€æƒå¨æ€§ï¼š{authority_level}ï¼Œç›¸å…³æ€§ï¼š{relevance_level}ã€‘\n"
                f"   æ¥æºï¼š{result.source_domain}\n"
                f"   æ ‡é¢˜ï¼š{result.title}\n"
                f"   æ‘˜è¦ï¼š{result.snippet}\n"
                f"   é“¾æ¥ï¼š{result.url}\n"
            )
        
        return "\n".join(summary_parts)
    
    def save_evidence_collection(self, evidence: EvidenceCollection, output_path: str):
        """ä¿å­˜è¯æ®æ”¶é›†ç»“æœ"""
        evidence_data = asdict(evidence)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(evidence_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯æ®æ”¶é›†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    agent = WebSearchAgent()
    
    # æ¨¡æ‹Ÿæœç´¢
    evidence = agent.search_evidence_for_claim(
        claim_id="test_1",
        search_keywords=["artificial intelligence", "productivity", "workplace"],
        claim_text="äººå·¥æ™ºèƒ½æŠ€æœ¯æ˜¾è‘—æé«˜äº†å·¥ä½œåœºæ‰€çš„ç”Ÿäº§æ•ˆç‡"
    )
    
    print(f"\nğŸ“Š æœç´¢ç»“æœæ‘˜è¦:")
    print(evidence.summary)
    
    # ä¿å­˜ç»“æœ
    agent.save_evidence_collection(evidence, "test_evidence.json")
