"""
æ•´ä½“æ–‡æ¡£å¤„ç†æµæ°´çº¿
ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªæ–‡æ¡£ï¼Œè¿›è¡Œclaimæ£€æµ‹ã€è¯æ®æœç´¢ã€åˆ†æå’Œå¢å¼º
"""

import json
import os
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from evidence_detector import EvidenceDetector, UnsupportedClaim, EvidenceResult
from dataclasses import asdict
from document_generator import DocumentGenerator
from direct_document_merger import DirectDocumentMerger
from web_search_agent import WebSearchAgent, EvidenceCollection
# ç¡®ä¿å¯¼å…¥å½“å‰ç›®å½•çš„configæ¨¡å—
import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# å¼ºåˆ¶é‡æ–°å¯¼å…¥configæ¨¡å—
import importlib
try:
    import config
    # é‡æ–°åŠ è½½configæ¨¡å—ä»¥ç¡®ä¿è·å–æœ€æ–°çš„ç¯å¢ƒå˜é‡
    importlib.reload(config)
except:
    # å¦‚æœé‡æ–°åŠ è½½å¤±è´¥ï¼Œç›´æ¥å¯¼å…¥
    import config

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®ï¼ŒåŒ…æ‹¬HTTPè¯·æ±‚æ—¥å¿—"""
    # å®‰å…¨åœ°è·å–LOG_LEVELï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
    log_level = getattr(config, 'LOG_LEVEL', 'INFO')
    if isinstance(log_level, str):
        log_level = log_level.upper()
    else:
        log_level = 'INFO'
    
    logging.basicConfig(
        level=getattr(logging, log_level, logging.INFO),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # å®‰å…¨åœ°æ£€æŸ¥ENABLE_HTTP_LOGS
    enable_http_logs = getattr(config, 'ENABLE_HTTP_LOGS', True)
    if enable_http_logs:
        httpx_logger = logging.getLogger("httpx")
        httpx_logger.setLevel(logging.INFO)
        openai_logger = logging.getLogger("openai")
        openai_logger.setLevel(logging.INFO)
        print("ğŸ” HTTPè¯·æ±‚æ—¥å¿—å·²å¯ç”¨")

setup_logging()

class WholeDocumentPipeline:
    """æ•´ä½“æ–‡æ¡£å¤„ç†æµæ°´çº¿"""
    
    def __init__(self):
        self.evidence_detector = EvidenceDetector()
        self.direct_merger = DirectDocumentMerger()
        self.web_search_agent = WebSearchAgent()
        # ç»Ÿä¸€è¾“å‡ºåˆ° router/outputs/web_evidence
        from pathlib import Path
        self.output_dir = str(Path(__file__).parent.parent / "router" / "outputs" / "web_evidence")
        
        # å¹¶è¡Œå¤„ç†é…ç½®
        self.max_workers = config.MAX_WORKERS
        self.thread_lock = threading.Lock()
        self.enable_parallel_search = config.ENABLE_PARALLEL_SEARCH
        self.enable_parallel_enhancement = config.ENABLE_PARALLEL_ENHANCEMENT
    
    def process_whole_document(self, document_path: str, 
                              max_claims: Optional[int] = None,
                              max_search_results: int = 10,
                              use_section_based_processing: bool = False) -> Dict[str, Any]:
        """å¤„ç†æ•´ä¸ªæ–‡æ¡£çš„å®Œæ•´æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ•´ä½“æ–‡æ¡£å¤„ç†æµæ°´çº¿...")
        start_time = time.time()
        
        # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å‘½å
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # å¦‚æœå¯ç”¨ç« èŠ‚å¤„ç†æ¨¡å¼ï¼Œä½¿ç”¨æ–°çš„å¤„ç†æ–¹å¼
        if use_section_based_processing:
            return self._process_document_by_sections(document_path, max_claims, max_search_results, timestamp)
        
        try:
            # ä½¿ç”¨ä¼ ç»Ÿæ•´ä½“æ–‡æ¡£å¤„ç†æ¨¡å¼ï¼ˆå›é€€åˆ°æ–°çš„evidence_detectorï¼‰
            return self._process_whole_document_legacy(document_path, max_claims, max_search_results, timestamp)
            
        except Exception as e:
            print(f"âŒ æµæ°´çº¿æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._create_error_result(document_path, str(e), timestamp)
    
    def _extract_content_from_json(self, document_data: Dict) -> str:
        """ä»JSONæ–‡æ¡£ä¸­æå–å®Œæ•´å†…å®¹"""
        if isinstance(document_data, dict) and 'content' in document_data:
            return document_data['content']
        elif isinstance(document_data, dict):
            # é€’å½’æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            content_parts = []
            for key, value in document_data.items():
                if isinstance(value, str) and len(value.strip()) > 50:
                    content_parts.append(f"## {key}\n\n{value}")
                elif isinstance(value, dict):
                    nested_content = self._extract_content_from_json(value)
                    if nested_content:
                        content_parts.append(f"## {key}\n\n{nested_content}")
            return "\n\n".join(content_parts)
        else:
            return str(document_data)
    
    
    def _create_empty_result(self, document_path: str, timestamp: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            "status": "no_claims_detected",
            "document_path": document_path,
            "timestamp": timestamp,
            "message": "æœªæ£€æµ‹åˆ°éœ€è¦è¯æ®æ”¯æ’‘çš„å®¢è§‚æ€§è®ºæ–­"
        }
    
    def _create_error_result(self, document_path: str, error_message: str, timestamp: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "status": "error",
            "document_path": document_path,
            "timestamp": timestamp,
            "error": error_message
        }
    
    def _process_document_by_sections(self, document_path: str, 
                                    max_claims: Optional[int] = None,
                                    max_search_results: int = 10,
                                    timestamp: str = None) -> Dict[str, Any]:
        """
        æŒ‰ç« èŠ‚å¤„ç†æ–‡æ¡£ï¼ˆæ–°çš„å¤„ç†æ–¹å¼ï¼‰
        
        Args:
            document_path: æ–‡æ¡£è·¯å¾„
            max_claims: æ¯ä¸ªç« èŠ‚æœ€å¤§è®ºæ–­æ•°
            max_search_results: æ¯ä¸ªè®ºæ–­æœ€å¤§æœç´¢ç»“æœæ•°
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        print("ğŸ”„ ä½¿ç”¨ç« èŠ‚å¹¶è¡Œå¤„ç†æ¨¡å¼...")
        start_time = time.time()
        
        if timestamp is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        try:
            # è¯»å–æ–‡æ¡£å†…å®¹
            with open(document_path, 'r', encoding='utf-8') as f:
                if document_path.endswith('.json'):
                    document_data = json.load(f)
                    full_content = self._extract_content_from_json(document_data)
                else:
                    full_content = f.read()
                    document_data = {"content": full_content}
            
            print(f"ğŸ“Š æ–‡æ¡£é•¿åº¦: {len(full_content)} å­—ç¬¦")
            
            # æå–ç« èŠ‚
            sections = self._extract_sections_from_content(full_content)
            
            if not sections:
                print("âš ï¸ æœªæ£€æµ‹åˆ°ç« èŠ‚ï¼Œå›é€€åˆ°æ•´ä½“å¤„ç†æ¨¡å¼")
                return self._process_whole_document_legacy(document_path, max_claims, max_search_results, timestamp)
            
            print(f"ğŸ“‘ æ£€æµ‹åˆ° {len(sections)} ä¸ªç« èŠ‚")
            
            # å¹¶è¡Œå¤„ç†ç« èŠ‚
            section_results = self._process_sections_parallel(sections, max_claims or 5, max_search_results)
            
            # è·å–ç« èŠ‚é¡ºåº
            section_order = getattr(sections, '_section_order', None)
            
            # åˆå¹¶ç»“æœï¼ˆæ–‡ä»¶ä¿å­˜å·²åœ¨æ­¤æ–¹æ³•ä¸­å®Œæˆï¼‰
            enhanced_content = self._merge_section_results(section_results, timestamp, document_path, section_order)
            
            # ä¸å†éœ€è¦é¢å¤–çš„æ–‡ä»¶ä¿å­˜ï¼Œå·²åœ¨_merge_section_resultsä¸­å®Œæˆ
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_claims = sum(result.get('statistics', {}).get('claims_detected', 0) for result in section_results.values())
            total_evidence = sum(result.get('statistics', {}).get('evidence_found', 0) for result in section_results.values())
            successful_sections = sum(1 for result in section_results.values() if result.get('status') == 'success')
            
            print(f"\nâœ… ç« èŠ‚å¹¶è¡Œå¤„ç†å®Œæˆï¼")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   - å¤„ç†ç« èŠ‚: {len(sections)} ä¸ª")
            print(f"   - æˆåŠŸç« èŠ‚: {successful_sections} ä¸ª")
            print(f"   - æ£€æµ‹è®ºæ–­: {total_claims} ä¸ª")
            print(f"   - æœç´¢è¯æ®: {total_evidence} æ¡")
            print(f"   - å¤„ç†æ—¶é—´: {processing_time:.1f} ç§’")
            # æ–‡ä»¶è·¯å¾„
            enhanced_file = os.path.join(self.output_dir, f"enhanced_document_{timestamp}.md")
            analysis_file = os.path.join(self.output_dir, f"evidence_analysis_{timestamp}.json")
            
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
            print(f"   - å¢å¼ºæ–‡æ¡£: {enhanced_file}")
            print(f"   - è¯æ®åˆ†æ: {analysis_file}")
            
            return {
                'status': 'success',
                'document_path': document_path,
                'processing_time': processing_time,
                'processing_mode': 'section_based_parallel',
                'statistics': {
                    'total_sections': len(sections),
                    'successful_sections': successful_sections,
                    'total_claims_detected': total_claims,
                    'total_evidence_sources': total_evidence,
                    'total_analysis_results': len(section_results)
                },
                'output_files': {
                    'enhanced_document': os.path.join(self.output_dir, f"enhanced_document_{timestamp}.md"),
                    'evidence_analysis': os.path.join(self.output_dir, f"evidence_analysis_{timestamp}.json")
                }
            }
            
        except Exception as e:
            print(f"âŒ ç« èŠ‚å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._create_error_result(document_path, str(e), timestamp)
    
    def _extract_sections_from_content(self, content: str) -> Dict[str, str]:
        """ä»æ–‡æ¡£å†…å®¹ä¸­æå–ç« èŠ‚ - åªæŒ‰ä¸€çº§æ ‡é¢˜(#)åˆ†å‰²ï¼Œä¿æŒå±‚çº§ç»“æ„å’Œé¡ºåº"""
        from collections import OrderedDict
        sections = OrderedDict()  # ä½¿ç”¨æœ‰åºå­—å…¸ä¿æŒç« èŠ‚é¡ºåº
        section_order = []  # é¢å¤–è®°å½•ç« èŠ‚é¡ºåº
        current_section = None
        current_content = []
        
        lines = content.split('\n')
        
        for line in lines:
            # åªæ£€æµ‹ä¸€çº§æ ‡é¢˜ï¼ˆ# å¼€å¤´ï¼‰ä½œä¸ºä¸»è¦ç« èŠ‚åˆ†å‰²ç‚¹
            header_match = re.match(r'^#\s+(.+)$', line.strip())
            if header_match:
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_section = header_match.group(1).strip()
                section_order.append(current_section)  # è®°å½•ç« èŠ‚é¡ºåº
                current_content = [line]  # åŒ…å«æ ‡é¢˜è¡Œ
            else:
                if current_section:
                    current_content.append(line)
                else:
                    # å¦‚æœè¿˜æ²¡æœ‰é‡åˆ°ä¸€çº§æ ‡é¢˜ï¼Œå°†å†…å®¹æ·»åŠ åˆ°ä¸´æ—¶ç« èŠ‚
                    if not current_section:
                        current_section = "æ–‡æ¡£å¼€å¤´"
                        section_order.append(current_section)
                        current_content = [line]
                    else:
                        current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            sections[current_section] = '\n'.join(current_content).strip()
        
        # å°†ç« èŠ‚é¡ºåºä¿¡æ¯å­˜å‚¨åœ¨sectionså¯¹è±¡ä¸­
        sections._section_order = section_order
        
        print(f"ğŸ“‘ æå–ç« èŠ‚é¡ºåº: {section_order}")
        return sections
    
    def _process_sections_parallel(self, sections: Dict[str, str], 
                                 max_claims_per_section: int,
                                 max_search_results: int) -> Dict[str, Dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†ç« èŠ‚ - ç®€åŒ–ç‰ˆï¼šç›´æ¥ä½¿ç”¨evidence_detector"""
        print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œç« èŠ‚å¤„ç†ï¼ˆç« èŠ‚æ•°: {len(sections)}ï¼‰")
        
        results = {}
        
        with ThreadPoolExecutor(max_workers=min(5, len(sections))) as executor:
            future_to_section = {
                executor.submit(
                    self.evidence_detector.process_section,
                    section_title,
                    section_content,
                    max_claims_per_section
                ): section_title
                for section_title, section_content in sections.items()
            }
            
            for future in as_completed(future_to_section):
                section_title = future_to_section[future]
                try:
                    result = future.result()
                    results[section_title] = result
                    print(f"  âœ… ç« èŠ‚å¤„ç†å®Œæˆ: {section_title}")
                except Exception as e:
                    print(f"  âŒ ç« èŠ‚å¤„ç†å¤±è´¥: {section_title} - {str(e)}")
                    results[section_title] = {
                        'section_title': section_title,
                        'status': 'failed',
                        'error': str(e),
                        'enhanced_content': sections[section_title],
                        'original_content': sections[section_title]
                    }
        
        print(f"âœ… ç« èŠ‚å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªç« èŠ‚")
        return results
    
    def _parallel_detect_claims(self, sections: Dict[str, str], max_claims: int) -> Dict[str, List[UnsupportedClaim]]:
        """é˜¶æ®µ1ï¼šå¹¶è¡Œæ£€æµ‹æ‰€æœ‰ç« èŠ‚çš„è®ºæ–­"""
        section_claims = {}
        
        with ThreadPoolExecutor(max_workers=min(10, len(sections))) as executor:
            future_to_section = {
                executor.submit(
                    self.evidence_detector._detect_unsupported_claims,
                    title,
                    content
                ): title
                for title, content in sections.items()
            }
            
            completed = 0
            total_claims = 0
            for future in as_completed(future_to_section):
                section_title = future_to_section[future]
                try:
                    claims = future.result()
                    if len(claims) > max_claims:
                        claims = sorted(claims, key=lambda x: x.confidence_level, reverse=True)[:max_claims]
                    
                    section_claims[section_title] = claims
                    completed += 1
                    total_claims += len(claims)
                    
                    with self.thread_lock:
                        print(f"  ğŸ“‹ ç« èŠ‚ {completed}/{len(sections)} è®ºæ–­æ£€æµ‹å®Œæˆ: {section_title} ({len(claims)} ä¸ªè®ºæ–­)")
                
                except Exception as e:
                    with self.thread_lock:
                        print(f"  âŒ ç« èŠ‚ {section_title} è®ºæ–­æ£€æµ‹å¤±è´¥: {str(e)}")
                    section_claims[section_title] = []
        
        print(f"âœ… è®ºæ–­æ£€æµ‹å®Œæˆï¼Œå…±æ£€æµ‹åˆ° {total_claims} ä¸ªè®ºæ–­")
        return section_claims
    
    def _parallel_search_evidence(self, section_claims: Dict[str, List[UnsupportedClaim]]) -> Dict[str, List[EvidenceResult]]:
        """é˜¶æ®µ2ï¼šå¹¶è¡Œæœç´¢æ‰€æœ‰è®ºæ–­çš„è¯æ®"""
        section_evidence = {}
        
        # æ”¶é›†æ‰€æœ‰è®ºæ–­
        all_claims = []
        claim_to_section = {}
        for section_title, claims in section_claims.items():
            for claim in claims:
                all_claims.append(claim)
                claim_to_section[claim.claim_id] = section_title
        
        # å…¨å±€è®ºæ®æ•°é‡é™åˆ¶
        MAX_TOTAL_CLAIMS = 25
        if len(all_claims) > MAX_TOTAL_CLAIMS:
            print(f"âš ï¸ è®ºæ®æ€»æ•° {len(all_claims)} è¶…è¿‡é™åˆ¶ {MAX_TOTAL_CLAIMS}ï¼ŒæŒ‰ç½®ä¿¡åº¦æ’åºå¹¶é™åˆ¶å¤„ç†æ•°é‡")
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–å‰25ä¸ª
            all_claims.sort(key=lambda x: x.confidence_level, reverse=True)
            limited_claims = all_claims[:MAX_TOTAL_CLAIMS]
            
            # é‡æ–°æ„å»º claim_to_section æ˜ å°„
            claim_to_section = {}
            for claim in limited_claims:
                claim_to_section[claim.claim_id] = claim.section_title
            
            all_claims = limited_claims
        
        if not all_claims:
            return {title: [] for title in section_claims.keys()}
        
        print(f"ğŸ” å¼€å§‹å¹¶è¡Œæœç´¢ {len(all_claims)} ä¸ªè®ºæ–­çš„è¯æ®...")
        
        # å¹¶è¡Œæœç´¢æ‰€æœ‰è®ºæ–­çš„è¯æ®
        with ThreadPoolExecutor(max_workers=min(15, len(all_claims))) as executor:
            future_to_claim = {
                executor.submit(
                    self.evidence_detector._search_evidence_for_claim,
                    claim
                ): claim
                for claim in all_claims
            }
            
            # åˆå§‹åŒ–ç»“æœå­—å…¸
            for section_title in section_claims.keys():
                section_evidence[section_title] = []
            
            completed = 0
            total_evidence = 0
            for future in as_completed(future_to_claim):
                claim = future_to_claim[future]
                try:
                    evidence_result = future.result()
                    section_title = claim_to_section[claim.claim_id]
                    section_evidence[section_title].append(evidence_result)
                    completed += 1
                    
                    if evidence_result.processing_status == 'success':
                        total_evidence += len(evidence_result.evidence_sources)
                    
                    with self.thread_lock:
                        print(f"  ğŸ” è®ºæ–­ {completed}/{len(all_claims)} è¯æ®æœç´¢å®Œæˆ: {claim.claim_id}")
                
                except Exception as e:
                    with self.thread_lock:
                        print(f"  âŒ è®ºæ–­ {claim.claim_id} è¯æ®æœç´¢å¤±è´¥: {str(e)}")
                    section_title = claim_to_section[claim.claim_id]
                    # åˆ›å»ºå¤±è´¥çš„è¯æ®ç»“æœ
                    failed_result = EvidenceResult(
                        claim_id=claim.claim_id,
                        claim_text=claim.claim_text,
                        section_title=claim.section_title,
                        search_query='',
                        evidence_sources=[],
                        enhanced_text=claim.claim_text,
                        confidence_score=0.0,
                        processing_status='failed'
                    )
                    section_evidence[section_title].append(failed_result)
        
        print(f"âœ… è¯æ®æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {total_evidence} æ¡è¯æ®")
        return section_evidence
    
    def _parallel_generate_modifications(self, sections: Dict[str, str], 
                                       section_evidence: Dict[str, List[EvidenceResult]]) -> Dict[str, Dict[str, Any]]:
        """é˜¶æ®µ3ï¼šå¹¶è¡Œç”Ÿæˆæ‰€æœ‰ç« èŠ‚çš„ä¿®æ”¹å†…å®¹"""
        results = {}
        
        # ç­›é€‰éœ€è¦ä¿®æ”¹çš„ç« èŠ‚ï¼ˆæœ‰è¯æ®çš„ç« èŠ‚ï¼‰
        sections_to_modify = {}
        sections_to_skip = {}
        
        for title, content in sections.items():
            evidence_list = section_evidence.get(title, [])
            # åªæœ‰å½“ç« èŠ‚æœ‰æˆåŠŸæ‰¾åˆ°è¯æ®çš„è®ºæ–­æ—¶æ‰éœ€è¦è°ƒç”¨APIä¿®æ”¹
            has_successful_evidence = any(er.processing_status == 'success' for er in evidence_list)
            
            if has_successful_evidence:
                sections_to_modify[title] = content
            else:
                # è·³è¿‡ä¿®æ”¹ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
                sections_to_skip[title] = {
                    'section_title': title,
                    'status': 'skipped',
                    'message': 'æ— éœ€ä¿®æ”¹ï¼ˆæœªæ‰¾åˆ°æœ‰æ•ˆè¯æ®ï¼‰',
                    'unsupported_claims': [],
                    'evidence_results': [],
                    'enhanced_content': content,
                    'original_content': content,
                    'processing_time': 0,
                    'statistics': {
                        'claims_detected': len(evidence_list),
                        'evidence_found': 0,
                        'claims_enhanced': 0,
                        'success_rate': 0
                    }
                }
        
        print(f"ğŸ“ éœ€è¦ä¿®æ”¹çš„ç« èŠ‚: {len(sections_to_modify)} ä¸ªï¼Œè·³è¿‡çš„ç« èŠ‚: {len(sections_to_skip)} ä¸ª")
        
        # å°†è·³è¿‡çš„ç« èŠ‚ç›´æ¥åŠ å…¥ç»“æœ
        results.update(sections_to_skip)
        
        # åªå¯¹éœ€è¦ä¿®æ”¹çš„ç« èŠ‚è¿›è¡Œå¹¶è¡ŒAPIè°ƒç”¨
        if sections_to_modify:
            # é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…APIå‹åŠ›è¿‡å¤§
            max_concurrent_api_calls = min(3, len(sections_to_modify))  # æœ€å¤š3ä¸ªå¹¶å‘APIè°ƒç”¨
            with ThreadPoolExecutor(max_workers=max_concurrent_api_calls) as executor:
                future_to_section = {
                    executor.submit(
                        self._generate_section_result,
                        title,
                        content,
                        section_evidence.get(title, [])
                    ): title
                    for title, content in sections_to_modify.items()
                }
            
            completed = 0
            for future in as_completed(future_to_section):
                section_title = future_to_section[future]
                try:
                    result = future.result()
                    results[section_title] = result
                    completed += 1
                    
                    with self.thread_lock:
                        print(f"  ğŸ“ ç« èŠ‚ {completed}/{len(sections_to_modify)} ä¿®æ”¹ç”Ÿæˆå®Œæˆ: {section_title}")
                
                except Exception as e:
                    with self.thread_lock:
                        print(f"  âŒ ç« èŠ‚ {section_title} ä¿®æ”¹ç”Ÿæˆå¤±è´¥: {str(e)}")
                    results[section_title] = {
                        'section_title': section_title,
                        'status': 'failed',
                        'error': str(e),
                        'enhanced_content': sections_to_modify[section_title],
                        'original_content': sections_to_modify[section_title]
                    }
        
        print(f"âœ… ä¿®æ”¹ç”Ÿæˆå®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªç« èŠ‚")
        return results
    
    def _generate_section_result(self, section_title: str, section_content: str, 
                               evidence_results: List[EvidenceResult]) -> Dict[str, Any]:
        """ç”Ÿæˆå•ä¸ªç« èŠ‚çš„å¤„ç†ç»“æœ"""
        start_time = time.time()
        
        try:
            # ç›´æ¥ä½¿ç”¨åŸå†…å®¹ï¼Œå¢å¼ºé€»è¾‘å·²åœ¨evidence_detectorä¸­å¤„ç†
            modified_content = section_content
            
            # ç»Ÿè®¡ä¿¡æ¯
            successful_evidence = sum(1 for er in evidence_results if er.processing_status == 'success')
            total_evidence_sources = sum(len(er.evidence_sources) for er in evidence_results)
            
            processing_time = time.time() - start_time
            
            return {
                'section_title': section_title,
                'status': 'success',
                'message': f'æˆåŠŸå¤„ç† {len(evidence_results)} ä¸ªè®ºæ–­',
                'unsupported_claims': [asdict(er) for er in evidence_results],  # å…¼å®¹æ€§
                'evidence_results': [asdict(er) for er in evidence_results],
                'enhanced_content': modified_content,
                'original_content': section_content,
                'processing_time': processing_time,
                'statistics': {
                    'claims_detected': len(evidence_results),
                    'evidence_found': total_evidence_sources,
                    'claims_enhanced': successful_evidence,
                    'success_rate': (successful_evidence / len(evidence_results) * 100) if evidence_results else 0
                }
            }
            
        except Exception as e:
            return {
                'section_title': section_title,
                'status': 'failed',
                'error': str(e),
                'enhanced_content': section_content,
                'original_content': section_content,
                'unsupported_claims': [],
                'evidence_results': [],
                'processing_time': time.time() - start_time,
                'statistics': {
                    'claims_detected': 0,
                    'evidence_found': 0,
                    'claims_enhanced': 0
                }
            }
    
    
    def _merge_section_results(self, section_results: Dict[str, Dict[str, Any]], 
                             timestamp: str, document_path: str, section_order: List[str] = None) -> str:
        """åˆå¹¶ç« èŠ‚å¤„ç†ç»“æœä¸ºå®Œæ•´æ–‡æ¡£ - ä½¿ç”¨æ–°çš„ä¸‰æ­¥éª¤æµç¨‹"""
        print("ğŸ“‹ å¼€å§‹åˆå¹¶ç« èŠ‚ç»“æœ...")
        
        # ä½¿ç”¨ç›´æ¥åˆå¹¶å™¨ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆæ— éœ€APIè°ƒç”¨ï¼‰ï¼Œä¼ é€’ç« èŠ‚é¡ºåº
        final_document = self.direct_merger.merge_sections_to_markdown(section_results, section_order)
        
        # åªä¿å­˜ä¸¤ä¸ªæ–‡ä»¶ï¼š
        # 1. ä¿®æ”¹å®Œæˆçš„markdownæ–‡æ¡£
        final_doc_path = os.path.join(self.output_dir, f"enhanced_document_{timestamp}.md")
        self.direct_merger.save_enhanced_document(final_document, final_doc_path)
        
        # 2. è¯æ®åˆ†ææŠ¥å‘Š
        analysis_json_path = os.path.join(self.output_dir, f"evidence_analysis_{timestamp}.json")
        self.direct_merger.generate_evidence_analysis(section_results, analysis_json_path, timestamp)
        
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
        print(f"   ğŸ“„ å¢å¼ºæ–‡æ¡£: {final_doc_path}")
        print(f"   ğŸ“Š è¯æ®åˆ†æ: {analysis_json_path}")
        
        return final_document
    
    # ç§»é™¤äº†_generate_section_analysis_jsonæ–¹æ³•ï¼Œç°åœ¨ä½¿ç”¨ç›´æ¥åˆå¹¶å™¨çš„åˆ†æåŠŸèƒ½
    
    def _process_whole_document_legacy(self, document_path: str, 
                                     max_claims: Optional[int] = None,
                                     max_search_results: int = 10,
                                     timestamp: str = None) -> Dict[str, Any]:
        """åŸæœ‰çš„æ•´ä½“æ–‡æ¡£å¤„ç†æ–¹å¼ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        print("ğŸ”„ å›é€€åˆ°åŸæœ‰çš„æ•´ä½“æ–‡æ¡£å¤„ç†æ¨¡å¼...")
        
        if timestamp is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä½¿ç”¨æ–°çš„evidence_detector + document_generatorå¤„ç†æ•´ä¸ªæ–‡æ¡£
        try:
            with open(document_path, 'r', encoding='utf-8') as f:
                if document_path.endswith('.json'):
                    document_data = json.load(f)
                    full_content = self._extract_content_from_json(document_data)
                else:
                    full_content = f.read()
                    document_data = {"content": full_content}
            
            # å°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºå•ä¸€ç« èŠ‚å¤„ç†
            result = self.evidence_detector.process_section(
                section_title="å®Œæ•´æ–‡æ¡£",
                section_content=full_content,
                max_claims=max_claims or 20
            )
            
            if result['status'] != 'success':
                return self._create_error_result(document_path, result.get('error', 'å¤„ç†å¤±è´¥'), timestamp)
            
            # ä½¿ç”¨æ–‡æ¡£ç”Ÿæˆå™¨ç”Ÿæˆä¿®æ”¹å†…å®¹
            if result.get('evidence_results'):
                modified_content = self.document_generator.generate_section_with_evidence(
                    section_title="å®Œæ•´æ–‡æ¡£",
                    original_content=full_content,
                    evidence_results=[
                        EvidenceResult(**er_data) if isinstance(er_data, dict) else er_data
                        for er_data in result['evidence_results']
                    ]
                )
            else:
                modified_content = full_content
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(self.output_dir, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡æ¡£
            analysis_file = os.path.join(self.output_dir, f"evidence_analysis_{timestamp}.json")
            
            # åˆ›å»ºå…¼å®¹çš„åˆ†ææ•°æ®
            analysis_data = {
                'metadata': {
                    'document_path': document_path,
                    'analysis_timestamp': timestamp,
                    'processing_mode': 'legacy_whole_document',
                    'analysis_version': '8.0_evidence_detector_document_generator'
                },
                'summary': result.get('statistics', {}),
                'unsupported_claims': result.get('unsupported_claims', []),
                'evidence_results': result.get('evidence_results', []),
                'processing_notes': [
                    "ä½¿ç”¨ä¼ ç»Ÿæ•´ä½“æ–‡æ¡£å¤„ç†æ¨¡å¼",
                    "åŸºäºæ–°çš„evidence_detector + document_generatorç³»ç»Ÿ",
                    "æ•´ä¸ªæ–‡æ¡£ä½œä¸ºå•ä¸€ç« èŠ‚å¤„ç†",
                    "ä½¿ç”¨ç‹¬ç«‹çš„æ–‡æ¡£ç”Ÿæˆå™¨ç”Ÿæˆå¢å¼ºå†…å®¹"
                ]
            }
            
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            enhanced_file = os.path.join(self.output_dir, f"ai_enhanced_document_{timestamp}.md")
            
            # ä½¿ç”¨æ–‡æ¡£ç”Ÿæˆå™¨ä¿å­˜ä¿®æ”¹æ–‡æ¡£
            self.document_generator.save_enhanced_document(modified_content, enhanced_file)
            
            return {
                'status': 'success',
                'document_path': document_path,
                'processing_time': result.get('processing_time', 0),
                'statistics': result.get('statistics', {}),
                'output_files': {
                    'evidence_analysis': analysis_file,
                    'enhanced_document': enhanced_file
                }
            }
            
        except Exception as e:
            return self._create_error_result(document_path, str(e), timestamp)

if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    pipeline = WholeDocumentPipeline()
    
    print("æ•´ä½“æ–‡æ¡£å¤„ç†æµæ°´çº¿å·²åˆå§‹åŒ–")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("pipeline.process_whole_document('your_document.md')")
